\section{Analisi dei risultati}
Le performance prodotte dalle differenti topologie di reti, riportate in Tabella \ref{tab:topology}, non mostrano differenze sostanziali; per questo motivo, seguendo il principio del rasoio di Occam, è stata selezionata la topologia più semplice, ovvero una rete a tre layer nascosti con rispettivamente $512$, $256$ e $128$ neuroni. 
Il risultato ottenuto mostra che non è necessario un elevato livello di profondità della rete per risolvere il problema, e che aumentare il numero di layer non apporta benefici alla performance analizzata.

Fissata questa topologia, il processo di ottimizzazione degli iperparametri è stato iterato per le varie tecniche di feature reduction disponibili, oltre al dataset dopo la fase di \textit{data cleaning}. 
I risultati, riportati in Figura \ref{fig:HPO}, mostrano l'andamento del processo di ricerca degli iperparametri ottimi. 
In Figura \ref{subfig:best-seen} è possibile apprezzare come la tecnica di eliminazione delle feature correlate e l'utilizzo dell'intero dataset riescano a garantire performance leggermente migliori rispetto a tecniche di feature extraction, che trovano configurazioni collegate a performance leggenrmente minori. 
Considerando il fatto che gli approcci di feature extraction portano ad una drastica riduzione del numero di input della rete (da $\sim800$ a $100$ elementi), possiamo dedurre che entrambi gli approcci riescano a mantenere la quasi totalità dell'informazione utile contenuta nelle feature di partenza. 
In Figura \ref{subfig:auc-iteration} è invece riportata l'AUC media associata alle varie iterazioni dell'ottimizzazione per i quattro processi effettuati. 
Dal grafico emerge come il processo, per ogni input proposto, analizzi principalmente lo spazio nell'intorno dei valori ottimi già individuati, salvo esplorare sporadicamente configurazioni associate a performance notevolmente peggiori (depressioni nella curva associata al metodo).\\
Considerata la differenza di performance mostrata dalle differenti tecniche di feature reduction e la limitata complessità di addestramento delle rete, è stato deciso di utilizzare la correlazione come metodo di riduzione della dimensionalità dell'input. 
Questa, nel nostro caso, permette di dimezzare il numero di feature in input, garantendo comunque un valore di AUC medio ottimale, come riportato dal processo di ottimizzazione. 
Un approccio più drastico in termini di riduzione della dimensionalità, come PCA e autoencoder con numero di feature limitate a $100$, avrebbe senso nel caso in cui l'addestramento della rete richiedesse molte più risorse computazionali; in questo caso, il \textit{trade-off} tra la perdita di performance e l'aumento del costo in termini di tempo di addestramento della rete avrebbe potrebbe considerarsi vantaggioso.

Dopo aver selezionato la topologia, la tecnica di riduzione della dimensionalità e gli iperparametri ottimali per il nostro problema, il modello derivante è stato addestrato sull'intero dataset e le performance finali sono state valutate mediante il test set.
In Figura \ref{subfig:roc} sono riportate le curve ROC relative alle varie etichette del dataset; queste curve, oltre ad essere utilizzate per calcolare l'AUC utilizzata come misura di performance del problema, permettono di selezionare i \textit{threshold} di predizione associati alle varie classi. 
Modificando questi valori, è possibile ottenere un \%TP a piacere, a discapito di una maggiore \%FP \textit{percentuale di False Positive}. 
Allo stesso modo, fissando un \%TP maggiore, il valore della recall cresce, a discapito della precision, come mostrato in Figura \ref{subfig:pr}. 
A questo punto, visto il dominio di applicazione del problema, sarebbe necessaria una conoscenza di dominio per stabilire dei costi da associatre ai valori della matrice di confusione delle varie classi al fine di selezionare i differenti \textit{threshold}. 
Possiamo supporre che i falsi positivi abbiano un costo sensibilmente inferiore rispetto ai falsi negativi, in quanto una molecola tossica non rilevata appare essere un rischio ben più grave rispetto ad una molecola innocua classificata come dannosa.\\
L'approccio realizzato è stato poi confrontato con i metodi presenti nello stato dell'arte, producendo il grafico in Figura \ref{fig:comparison}. È possibile affermare che la tecnica proposta si posiziona circa a metà classifica come performance medie, con posizionamenti variegati a seconda delle classi. Si evidenzia come l'approccio proposto consente l'inferenza di tutte e dodici le classi e non è un metodo \textit{ensamble}, a differenza della maggioranza degli approcci presenti nel grafico. \todo{DeepTox è ensamble, gli altri lo sappiamo? DP}

