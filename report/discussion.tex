\section{Analisi dei risultati}
Le performance prodotte dalle differenti topologie di reti, riportate in Tabella \ref{tab:topology}, non mostrano differenze sostanziali; per questo motivo, seguendo il principio del rasoio di Occam, è stata selezionata la topologia più semplice, ovvero una rete a tre layer nascosti con rispettivamente $512$, $256$ e $128$ neuroni. Il risultato ottenuto mostra che non è necessario un elevato livello di profondità della rete per risolvere il problema, e che aumentare il numero di layer non apporta benefici alla performance analizzata.\\

Fissata questa topologia, il processo di ottimizzazione degli iperparametri è stato iterato per le varie tecniche di \textit{feature reduction} disponibili, oltre al dataset dopo la fase di \textit{data cleaning}. I risultati, riportati in Figura \ref{fig:HPO}, mostrano l'andamento del processo di ricerca degli iperparametri ottimi. In Figura \ref{subfig:best-seen} è possibile apprezzare come la tecnica di eliminazione delle feature correlate, unita al non utilizzo di alcuna \textit{feature reduction}, riescano a garantire performance leggermente migliori rispetto a tecniche di \textit{feature extraction} quali PCA e autoencoder, che trovano configurazioni collegate a valori di AUC identici nel grafico. Considerando il fatto che i due approcci appena nominati portano ad una drastica riduzione del numero di input della rete (da $\sim800$ a $100$ elementi), possiamo dedurre che entrambi gli approcci riescano a mantenere la quasi totalità dell'informazione utile contenuta nelle feature di partenza. In Figura \ref{subfig:auc-iteration} è invece riportata l'AUC associata alle varie iterazioni dell'ottimizzazione per i quattro test effettuati. Dal grafico emerge come il processo, per ogni input proposto, analizzi principalmente lo spazio nell'intorno dei valori ottimi già individuati, salvo esplorare sporadicamente configurazioni associate a performance notevolmente peggiori (depressioni nella curva associata al metodo).\\
Considerata la differenza di performance mostrata dalle differenti tecniche di \textit{feature reduction} e la limitatà complessità di addestramento delle rete, è stato deciso di utilizzare la correlazione come metodo di riduzione della dimensionalità dell'input. Questa, nel nostro caso, permette di dimezzare il numero di feature in input, garantendo comunque un valore di AUC ottimale, come riportato dal processo di ottimizzazione. Un approccio più drastico in termini di riduzione della dimensionalità, come PCA e autoencoder con numero di feature limitate a $100$, avrebbe senso nel caso in cui l'addestramento della rete richiedesse molte più risorse computazionali: in questo caso, il \textit{tradeoff} tra la perdita di performance e il guadagno in termini di tempo di addestramento e inferenza della rete avrebbe sicuramente senso.\\

Avendo selezionato la topologia, la tecnica di riduzione della dimensionalità e gli iperparametri ottimali per il nostro problema, il modello derivante è stato addestrato sull'intero dataset e le performance finali sono state valutate mediante il testset.
In Figura \ref{subfig:roc} sono riportate le curve ROC relative alle varie etichette del dataset; queste curve, oltre ad essere utilizzate per calcolare l'AUC utilizzata come misura di performance del problema, permettono di selezionare i \textit{threshold} di predizione associati alle varie classi. Modificando questi valori, è possivile ottenere una percentuale di Veri Positivi a piacere, a discapito ovviamente della percentuale di Falsi Positivi. Allo stesso modo, fissando una percentuale di Veri Positivi maggiore, il valore della \textit{Recall} per classe cresce, a discapito della \textit{Precision}, come mostrato in Figura \ref{subfig:pr}. A questo punto, visto il dominio di applicazione del problema, sarebbe necessaria una conoscenza esterna del reale costo associato ai valori della matrice di confusione delle varie classi al fine di selezionare i differenti \textit{threshold} (e quindi le percentuali di Veri Positivi): possiamo supporre che un Falso Positivo abbia un costo sensibilmente inferiori rispetto ad un Falso Negativo, in quanto una molecola tossica non rilevata appare essere un rischio ben più grave rispetto ad una molecola innocua classificata come dannosa.\\

L'approccio realizzato è stato poi confrontato con i metodi presenti nello stato dell'arte, producendo il grafico in Figura \ref{fig:comparison}. È possibile affermare che la tecnica proposta si posiziona circa a metà classifica come performance medie, con posizionamenti variegati a seconda delle classi. Si evidenzia come l'approccio proposto consente l'inferenza di tutte e dodici le classi e non è un metodo \textit{ensamble}, a differenza della maggioranza degli approcci presenti nel grafico.

