\section{Approccio e metodi utilizzati}
A seguito dell'analisi sulle feature, che ha evidenziato una correlazione tra un discreto numero di esse, sono state sviluppate e utilizzate tre differenti tecniche di feature reduction. 
La prima è una feature selection basata sulla correlazione; \textit{i.e.}, elimina quegli elementi che risultano correlati oltre una certa soglia con altri elementi. 
Ponendo questa soglia a $0.90$, la dimensione dell'input passa da $793$ a $414$ feature.
Le rimanenti due tecniche utilizzate, che afferiscono ai metodi di feature extraction, sono la Principal Component Analysis (PCA) e l'utilizzo di un autoencoder; in entrambi i casi, il numero di feature estratte è stato fissato a $100$.
Per quanto concerne la struttura dell'autoencoder, è stata utilizzata una topologia con 2 layer nascosti prima del \textit{bottleneck}, i layer sono composti da un numero di neuroni dimezzato progressivamente, i quali sfruttano la \texttt{relu} come funzione di attivazione. 
Al contrario, il layer di output utilizzata una funzione lineare, in quanto i valori attesi sono numeri reali.\\
Al fine di individuare la migliore topologia da utilizzare per il successivo processo di ottimizzazione, in particolare come la profondità della rete influisce le performance, sono state poste a confronto tre reti dalla profondità crescente; la struttura di base si compone di $3$ layer nascosti con attivazione \texttt{relu}, dropout $0.2$ e normalizzazione dopo l'attivazione, con rispettivamente $512$, $256$ e $128$ neuroni. 
Ad essa, sono stati aggiunti incrementalmente due layer con la medesima struttura e con numero di neuroni dimezzato rispetto al livello precedente. %(\textit{i.e.}, 64 e 32 neuroni rispettivamente). 
In coda alle reti così create, è stato posto un layer di output con $12$ neuroni attivati mediante una sigmoide, poiché le label assumono valori pari a 0 o 1; l'ottimizzatore utilizzato per il processo di training è stato \texttt{adam} (scelto per la maggiore efficienza e il \textit{learning rate} dinamico) e il parametro di peso della funzione di \textit{loss} è stato posto a $20$ (valore prossimo allo sbilanciamento complessivo tra le classi). 
Il valore appena citato influenza il peso che la funzione di loss associa ad un errore sulla classe positiva: questa funzione di \textit{loss} permette di gestire problemi fortemente sbilanciati come nel caso preso in analisi.
%Il fine di questa operazione è quello di determinare se una topologia più profonda della rete sia in grado di ottenere performance migliori a parità di valori degli iperparametri.

Una volta individuata la migliore tra le topologie proposte, è stato effettuato un processo di ottimizzazione bayesiana di alcuni iperparametri del modello. 
Questa tecnica, a differenza di approcci più \textit{na\"ive}, è \textit{sample efficient}, ovvero sfrutta al meglio il budget a disposizione per individuare la configurazione ottimale degli iperparametri. 
Nel processo sono stati inclusi: (i) dimensione del \textit{batch}; (ii) valore di dropout; (iii) peso della funzione di regolarizzazione all'interno della funzione obiettivo; (iv) funzione di attivazione dei layer nascosti; (v) peso della funzione di loss per la gestione dello sbilanciamento tra le classe.
Vista la presenza di variabili sia continue che categoriche, il modello surrogato selezionato è stato quello delle \textit{random forest}, maggiormente adatto alla gestione di valori non continui. 
Il budget messo a disposizione del processo è stato fissato a 120 valutazioni, di cui il 12,5\% è stato utilizzato come \textit{initial design} del modello surrogato, mediante un campionamento dello spazio Latin Hypercube Sampling (LHS). 
Il valore ottimizzato dal processo è stato il valore medio delle AUC sulle $12$ classi in \textit{3-fold cross validation}; la funzione di acquisizione utilizzata è stata la Lower Confidence Bound (LCB).
Il processo di AutoML (Automated Machine Learning) è stato effettuato considerando singolarmente come input della rete sia il dataset ottenuto dopo la fase di \textit{preprocessing} sia le tre tecniche di feature reduction (applicate al dataset dopo le operazioni di pulizia).
Così facendo, è stato possibile individuare la configurazione di iperparametri ottimi per i vari possibili input e confrontare le performance dei modelli.

Avendo stabilito la combinazione tra feature reduction e iperparametri in grado di massimizzare la AUC media, è stato analizzato il modello derivante più nel dettaglio.
Il classificatore così ottenuto è stato utilizzato come metodo di predizione per i dati di test e le performance ottenute sono state confrontate con quelle dei modelli proposti in letteratura.
Oltre a confrontare le performance ottenute con quelle presenti in letteratura in termini di AUC inerenti alle singole classi e medie, sono state analizzate le curve Receiver Operating Characteristic (ROC) riguardanti le singole classi e si è studiato come all'evolvere della percentuale di True Positive (\%TP) desiderata, evolvono le misure di precision e recall inerenti alle singole classi.