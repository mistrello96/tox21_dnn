\section{Approccio e metodi utilizzati}
A seguito dell'analisi sulle feature, che ha evidenziato una correlazione tra un discreto numero di esse, sono state sviluppate e utilizzate tre differenti tecniche di feature reduction. 
La prima, che ricade nella categoria delle tecniche di feature selection, si limita ad eliminare quegli elementi che risultano correlati oltre una certa soglia con altri elementi. 
Ponendo questa soglia a $0.90$, la dimensione dell'input si passa da $793$ a $414$ feature.
Le rimanenti due tecniche utilizzate, che afferiscono ai metodi di feature extraction, sono la Principal Component Analysis (PCA) e l'utilizzo di un autoencoder; in entrambi i casi, il numero di feature estratte è stato fissato a $100$.
Per quanto concerne la struttura dell'autoencoder, è stata utilizzata una topologia con 2 layer nascosti prima del \textit{bottleneck}, i layer sono composti da un numero di neuroni dimezzato progressivamente, i quali sfruttano la \texttt{relu} come funzione di attivazione. 
Al contrario, il layer di output utilizzata una funzione lineare, in quanto i valori attesi sono numeri reali.\\
Al fine di individuare la migliore topologia da utilizzare per il successivo processo di ottimizzazione, sono state poste a confronto tre reti dalla profondità crescente; la struttura di base si compone di $3$ layer nascosti con attivazione \texttt{relu}, dropout $0.2$ e normalizzazione prima dell'attivazione\todo[inline]{Eh? No? Cosa? Mica le funzioni di normalizzazione aggiungono una penalizzazione alla funzione obiettivo che la rete minizza nel processo di training? Sarebbe bene dire che utiliziamo la L2 e perché lo facciamo. DP Ti confondi con la regolarizzazione, dalla doc Normalize the activations of the previous layer at each batch, i.e. applies a transformation that maintains the mean activation close to 0 and the activation standard deviation close to 1. MM}, con rispettivamente $512$, $256$ e $128$ neuroni. 
Ad essa, sono stati aggiunti incrementalmente due layer con la medesima struttura e con numero di neuroni dimezzato rispetto al livello precedente (\textit{i.e.}, 64 e 32 neuroni rispettivamente). 
In coda alle reti così create, è stato posto un layer di output con $12$ neuroni attivati mediante una sigmoide; l'ottimizzatore utilizzato per il processo di training è stato \texttt{adam} (scelto per la maggiore efficienza e il \textit{learning rate} dinamico) e il parametro di peso della funzione di \textit{loss} è stato posto a $20$ (valore prossimo allo sbilanciamento complessivo tra le classi). Il valore appena citato influenza il peso che la funzione di loss associa ad un errore sulla classe positiva: questa funzione di \textit{loss}, preferita ad una semplice \textit{binary crossentropy}, permette di gestire problemi fortemente sbilanciati come nel caso preso in analisi.
Il fine di questa operazione è quello di determinare se una topologia più profonda della rete  sia in grado di ottenere performance migliori a parità di valori degli iperparametri.

Una volta individuata la migliore tra le topologie della rete proposte, è stato effettuato un processo di ottimizzazione bayesiana di alcuni iperparametri del modello. 
Questa tecnica, a differenza di approcci più \textit{na\"ive}, è \textit{sample efficient}, ovvero sfrutta al meglio il budget a disposizione per individuare la configurazione ottimale degli iperparametri. Nel processo sono stati inclusi: (I) dimensione del \textit{batch};(II) valore di dropout;(III) peso della funzione di normalizzazione all'interno della funzione obiettivo\todo{????? è la regolarizzaizone l2 che si applica ai pesi dei layer, che centra la FO? MM};(IV) funzione di attivazione dei layer nascosti;(V) peso della funzione di loss per la gestione dello sbilanciamento tra le classe.
Vista la presenza di variabili sia continue che categoriche, il modello surrogato selezionato è stato quello delle random forest, maggiormente adatto alla gestione di valori non continui. 
Il budget messo a disposizione del processo è stato fissato a 120 valutazioni, di cui il 25\% è stato utilizzato come \textit{initial design} del modello surrogato\todo{Ma mica era diminuito con i parametri che avevo inserito? o non è cambiato e mi hai ignorato quando ti avevo chiesto se funzionava o no? DP Hai ragione, mi ero scordato della cosa. Puoi corregere tu il valore? MM}, mediante un campionamento dello spazio Latin Hypercube Sampling (LHS). Il valore ottimizzato dal processo è stato il valore medio delle AUC sulle $12$ classi in \textit{3-fold cross validation}; la funzione di acquisizione utilizzata è stata la Lower Confidence Bound (LCB).
Il processo di AutoML (Automated Machine Learning) è stato effettuato considerando singolarmente come input della rete sia dataset ottenuto dopo la fase di \textit{preprocessing} sia le tre tecniche di feature reduction (applicate al dataset dopo le operazioni di pulizia).
Così facendo, è stato possibile individuare la configurazione di iperparametri ottimi per i vari possibli input e confrontare le performance dei modelli.\\ 
Avendo stabilito la combinazione tra feature reduction e iperparametri in grado di massimizzare la AUC media, è stato analizzato il modello derivante più nel dettaglio, monitorando i livelli di loss durante il nuovo processo di addestramento sull'intero dataset.
Il classificatore così ottenuto è stato utilizzato come metodo di predizione per i dati di test e le performance ottenute sono state confrontate con quelle dei modelli proposti in letteratura.
Oltre a confrontare le performance ottenute con quelle presenti in letteratura in termini di AUC inerenti alle singole classi e medie, sono state analizzate le curve Receiver Operating Characteristic (ROC) riguardanti le singole classi e si è studiato come all'evolvere della percentuale di True Positive (\%TP) desiderata, evolvono le misure di precision e recall inerenti alle singole classi.
