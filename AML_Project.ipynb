{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "AML_Project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "X71Bwbwowszz"
      },
      "source": [
        "# Toxicity - Matteo Mistri and Daniele Papetti\n",
        "**Tox21 dataset analysis and prediction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QkkAc7HDszms"
      },
      "source": [
        "This notebook is thought to run on colab and using google drive as storage system, so you may need to change the directories of input and output in order to make the notebook run on locally or on your drive. If you clone the repository, you will simply need to copy on your drive the notebook and the subdirectories \"dumps\" and \"images\". Moreover you need to download the datasets from source (http://bioinf.jku.at/research/DeepTox/tox21.html) and add them in the main directory of the repository; please, do note that the DENSE features and labels (of both training and test) have to be downloaded for this notebook.<br>\n",
        "If you have possible suggestions to fix this problem, you can write us. Any suggestion will be appreciated :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWNZYTCqPxUG",
        "colab_type": "text"
      },
      "source": [
        "In order to avoid modules version problems, we highlight that the notebook was delevoped with this versions:<br>\n",
        "- Keras = 2.2.5\n",
        "- matplotlib = 3.1.2\n",
        "- numpy = 1.17.4\n",
        "- pandas = 0.25.3\n",
        "- smac = 0.11.1\n",
        "- tensorflow = 1.15.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "15GuXYIXwsz5"
      },
      "source": [
        "### Dataset input and module load\n",
        "In this part, we load the dataset and modules which will be used in the notebook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pddFR07XwytH",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Nty7chPCwyYk",
        "colab": {}
      },
      "source": [
        "cd drive/'My Drive'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XBIqp-XGTc99",
        "colab": {}
      },
      "source": [
        "!apt-get install swig"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SbhlGoZmTdET",
        "colab": {}
      },
      "source": [
        "pip install smac"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZIUyTtoUs4jm",
        "colab": {}
      },
      "source": [
        "pip install pyDOE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TadpliB8W-8M",
        "colab": {}
      },
      "source": [
        "# suppress warnings\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5P_gvopKYHOH",
        "colab": {}
      },
      "source": [
        "# import of all required modules\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pyDOE\n",
        "import pickle\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "from operator import itemgetter\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import roc_auc_score, auc, roc_curve\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "from keras.callbacks import EarlyStopping, Callback\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, BatchNormalization, Dropout, Activation, Input\n",
        "from keras import regularizers\n",
        "from keras import optimizers\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "import keras.backend as kb\n",
        "\n",
        "# Import ConfigSpace and different types of parameters\n",
        "from smac.configspace import ConfigurationSpace, Configuration\n",
        "from ConfigSpace.hyperparameters import UniformFloatHyperparameter, UniformIntegerHyperparameter, CategoricalHyperparameter\n",
        "# Import SMAC-utilities\n",
        "from smac.scenario.scenario import Scenario\n",
        "from smac.facade.smac_hpo_facade import SMAC4HPO\n",
        "from smac.optimizer.acquisition import LCB\n",
        "from smac.initial_design import latin_hypercube_design\n",
        "\n",
        "import pickle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "e7xdczejJ_6k",
        "colab": {}
      },
      "source": [
        "# load the dataset\n",
        "X_train = pd.read_csv(\"./AML_project/tox21_dense_train.csv\")\n",
        "X_test = pd.read_csv(\"./AML_project/tox21_dense_test.csv\")\n",
        "Y_train = pd.read_csv(\"./AML_project/tox21_labels_train.csv\")\n",
        "Y_test = pd.read_csv(\"./AML_project/tox21_labels_test.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-HwhggOPxU-",
        "colab_type": "text"
      },
      "source": [
        "### Data Preprocessing\n",
        "In this section, we explore the dataset and we remove the features considered irrelevant and the instances considered outliers. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EBGbIq_Kws0I"
      },
      "source": [
        "The names of the samples are dropped becaues they are irrelevant for the classification task. \n",
        "In addition, we substitute the NaN values in the labels with the value 0 since we assume that the test was not performed by the lab researchers, because they thought that the molecule could not be involved in that biological pathway."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "s6V_MPfGNFqp",
        "colab": {}
      },
      "source": [
        "# drop first column which contains the names\n",
        "X_train = X_train.drop(X_train.columns[[0]], axis = 1)\n",
        "X_test = X_test.drop(X_test.columns[[0]], axis = 1)\n",
        "\n",
        "Y_train = Y_train.drop(Y_train.columns[[0]], axis = 1)\n",
        "Y_test = Y_test.drop(Y_test.columns[[0]], axis = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BPetLISCws02",
        "colab": {}
      },
      "source": [
        "# search for missing values in the features\n",
        "print(X_train.isnull().any().any())\n",
        "print(X_test.isnull().any().any())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zIb5irM2SGA_",
        "colab": {}
      },
      "source": [
        "# check if all features are numeric\n",
        "set(X_train.dtypes.append(X_test.dtypes))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ce0pqt0fNMP2",
        "colab": {}
      },
      "source": [
        "# transform NaN in 0 in the labels\n",
        "print(\"Missing values of train labels:\\n{}\".format(sum(Y_train.isnull().sum(axis = 1))))\n",
        "print(\"Missing values of test labels:\\n{}\".format(sum(Y_test.isnull().sum(axis = 1))))\n",
        "Y_train = Y_train.fillna(0)\n",
        "Y_test = Y_test.fillna(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9wO6SO_PxVS",
        "colab_type": "text"
      },
      "source": [
        "We study the distribution of the labels values and the correlation among them. Moreover, we remove the examples considered as outliers and the features with zero variance, then correlation between features and labels is explored. Finally, the values of all the features are normalized."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MNUixAt7ws0a",
        "colab": {}
      },
      "source": [
        "# labels distribution\n",
        "for l in Y_train.columns:\n",
        "  unique_values_count = Counter(Y_train[l])\n",
        "  print(\"{}: {}\".format(l, unique_values_count))\n",
        "\n",
        "  plt.figure(figsize = (8, 6))\n",
        "  sns.countplot(x = l, data = Y_train)\n",
        "  plt.xlabel(\"Values\", fontsize = 16)\n",
        "  plt.ylabel(\"Number of instances\", fontsize = 16)\n",
        "  plt.title(' '.join(l.split('.')), fontsize = 18)\n",
        "  plt.xticks(ticks = [0, 1], labels=[0, 1], fontsize = 12)\n",
        "  plt.yticks(fontsize = 12)\n",
        "  plt.savefig(\"./AML_project/images/png/hist-{}.png\".format(''.join(l.split('.'))), dpi = 300)\n",
        "  plt.savefig(\"./AML_project/images/pdf/hist-{}.pdf\".format(''.join(l.split('.'))))\n",
        "  plt.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "V1_QcGE8ws0i",
        "colab": {}
      },
      "source": [
        "# removal of outliers\n",
        "# we consider outliers what is out from 3 times the interquantile range\n",
        "outliers = list()\n",
        "# if we consider feature by feature, we will drop all the dataset,\n",
        "# so we count how many time a record is considered outlier\n",
        "for column in X_train.columns:\n",
        "  mean = X_train[column].mean()\n",
        "  q1 = X_train[column].quantile(1 / 4)\n",
        "  q3 = X_train[column].quantile(3 / 4)\n",
        "  threshold = 3 * (q3 - q1)\n",
        "\n",
        "  # Indexes of outliers\n",
        "  outliers.extend(X_train[X_train[column] > mean + threshold].index.values.tolist())      \n",
        "  outliers.extend(X_train[X_train[column] < mean - threshold].index.values.tolist())\n",
        "\n",
        "# a record is dropped if it is considered as outlier in more than a quarter of the features\n",
        "out_counter = Counter(outliers)\n",
        "toDrop = [k for k, v in zip(out_counter.keys(), out_counter.values()) if v > 200]\n",
        "print(\"Removed {} outliers\".format(len(toDrop)))\n",
        "# removal of the outlier rows, and reset of dataframe indexes \n",
        "X_train.drop(toDrop, inplace = True)\n",
        "Y_train.drop(toDrop, inplace = True)\n",
        "X_train.reset_index(drop = True, inplace = True)\n",
        "Y_train.reset_index(drop = True, inplace = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "f0xsjp3Hi2_8",
        "colab": {}
      },
      "source": [
        "# remove features with zero variance since they carry no information\n",
        "var_zero_columns = [c for c in X_train.columns if len(np.unique(X_train[c])) == 1]\n",
        "print(\"Removed {} features\".format(len(var_zero_columns)))\n",
        "X_train = X_train.drop(var_zero_columns, axis = 1)\n",
        "X_test = X_test.drop(var_zero_columns, axis = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fxjwvYhRws0q",
        "colab": {}
      },
      "source": [
        "# correlation matrix between labels\n",
        "corr_matrix = Y_train.corr().abs()\n",
        "\n",
        "plt.figure(figsize = (8, 6))\n",
        "sns.heatmap(corr_matrix, fmt = '.2f', annot = True, vmin = 0, vmax = 1,\n",
        "            xticklabels = range(12), yticklabels = range(12))\n",
        "plt.title(\"Labels correlation matrix heatmap\", fontsize = 18)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"./AML_project/images/png/labels_corr_matrix_heatmap.png\", dpi = 300)\n",
        "plt.savefig(\"./AML_project/images/pdf/labels_corr_matrix_heatmap.pdf\")\n",
        "plt.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "n0i6zTSNws0u",
        "colab": {}
      },
      "source": [
        "# correlation study between features and labels\n",
        "corr_matrix = X_train.merge(Y_train, right_index = True, left_index = True).corr().abs()\n",
        "# print the most correlated feature for each label\n",
        "s = set()\n",
        "for label in Y_train.columns:\n",
        "  # extract correlation column of the considered class\n",
        "  corr_col = corr_matrix[label]\n",
        "  # remove the elements whose index is a label\n",
        "  cleaned_col = corr_col.drop(Y_train.columns, axis = 0)\n",
        "  sorted_col = cleaned_col.sort_values(ascending = False)\n",
        "  # extract the most correlated features for the considered label\n",
        "  s.add(sorted_col.index[0])\n",
        "  print(\"Class {} is mosty correlated with feature {} with a value of {}\".format(label, sorted_col.index[0], round(sorted_col[0], 3)))\n",
        "print(\"Unique features: {}\".format(len(s)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QwkSRJQEszn6",
        "colab": {}
      },
      "source": [
        "# plot distribution of highly correlated features/labels\n",
        "high_corr_features_count = {k: (sum(1 for x in corr_matrix[k] if x > 0.90) - 1) for k in corr_matrix.columns}\n",
        "c = Counter(high_corr_features_count.values())\n",
        "\n",
        "plt.Figure(figsize = (8, 6))\n",
        "plt.xscale('log')\n",
        "plt.xlim(left = min(c.values()) - 0.2, right = max(c.values()) + 50)\n",
        "for p, v in c.items():\n",
        "  plt.scatter(v, p)\n",
        "plt.xlabel(\"# features/labels\", fontsize = 16)\n",
        "plt.ylabel(\"# highly correlated f/l\", fontsize = 16)\n",
        "plt.xticks(fontsize = 14)\n",
        "plt.yticks(fontsize = 14)\n",
        "plt.title(\"Distribution of correlated features/labels\", fontsize = 18)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"./AML_project/images/png/distribution_high_corr.png\", dpi = 300)\n",
        "plt.savefig(\"./AML_project/images/pdf/distribution_high_corr.pdf\")\n",
        "plt.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "905LrB1NNboE",
        "colab": {}
      },
      "source": [
        "# normalize features in 0 mean and 1 std\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train.append(X_test).values)\n",
        "X_train = pd.DataFrame(scaler.transform(X_train.values), columns = X_train.columns)\n",
        "X_test = pd.DataFrame(scaler.transform(X_test.values), columns = X_test.columns)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "E5nXCDuISkyF"
      },
      "source": [
        "### Feature reduction\n",
        "In this notebook, we offer 3 possible feature reduction strategies (which are all analyzed in the associated report), in addition to the “no reduction” strategy; i.e., keeping all the remaining features of the dataset after the prepocessing step (~800).<br>\n",
        "If you are interested in the use one feature reduction strategy, run the related cell; if you want to keep all the features, just skip this step.<br>\n",
        "Because the SMAC module (which is used for the SMBO process) causes some problems with Keras if a different network is trained before running SMAC, after the autoencoder step the runtime must be restarted and the results produced by the encoder must be loaded. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvgD4lNwPxVw",
        "colab_type": "text"
      },
      "source": [
        "#### Correlation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XdLtq27eMeun",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "# feature reduction using the correlation between the features\n",
        "# if two features are highly correlated (i.e., the correlation is greater than 0.9),\n",
        "# one of them is dropped\n",
        "corr_matrix = X_train.corr().abs()\n",
        "\n",
        "# select upper triangle of correlation matrix\n",
        "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k = 1).astype(np.bool))\n",
        "\n",
        "# find index of feature columns with correlation greater than 0.90\n",
        "to_drop = [column for column in upper.columns if any(upper[column] > 0.90)]\n",
        "\n",
        "# drop the features\n",
        "X_train = X_train.drop(X_train[to_drop], axis = 1)\n",
        "X_test = X_test.drop(X_test[to_drop], axis = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWkPB_evPxV1",
        "colab_type": "text"
      },
      "source": [
        "#### PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nsX5l7J3P-12",
        "colab": {}
      },
      "source": [
        "features_limit = 100\n",
        "columns = ['col' + str(x) for x in range(features_limit)]\n",
        "PCA_transformer = PCA(n_components = features_limit)\n",
        "PCA_transformer.fit(X_train.values)\n",
        "X_train = pd.DataFrame(PCA_transformer.transform(X_train.values), columns = columns)\n",
        "X_test = pd.DataFrame(PCA_transformer.transform(X_test.values), columns = columns)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHq2azcEPxV6",
        "colab_type": "text"
      },
      "source": [
        "#### Autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vfe4cnNtTABp",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "# Define early stopping\n",
        "es = EarlyStopping(monitor = 'val_loss', mode = 'min', \n",
        "                   verbose = 1, patience = 15, min_delta = 0.001,\n",
        "                   restore_best_weights = True)\n",
        "\n",
        "encoding_dim1 = int(X_train.shape[1] / 2)\n",
        "encoding_dim2 = int(encoding_dim1 / 2)\n",
        "encoding_dim3 = 100\n",
        "columns = ['col' + str(x) for x in range(encoding_dim3)]\n",
        "\n",
        "input_layer = Input(shape = (X_train.shape[1],))\n",
        "# \"encoded\" is the encoded representation of the input\n",
        "encoded1 = Dense(encoding_dim1, activation = 'relu')(input_layer)\n",
        "encoded2 = Dense(encoding_dim2, activation = 'relu')(encoded1)\n",
        "bottleneck = Dense(encoding_dim3, activation = 'relu')(encoded2)\n",
        "# \"decoded\" is the lossy reconstruction of the input\n",
        "decoded2 = Dense(encoding_dim2, activation = 'relu')(bottleneck)\n",
        "decoded1 = Dense(encoding_dim1, activation = 'relu')(decoded2)\n",
        "output_layer = Dense(X_train.shape[1], activation = 'linear')(decoded1)\n",
        "# this model maps an input to its reconstruction\n",
        "autoencoder = Model(input_layer, output_layer)\n",
        "# this model maps an input to its encoded representation\n",
        "encoder = Model(input_layer, bottleneck)\n",
        "\n",
        "# compile the model\n",
        "autoencoder.compile(optimizer = 'adam', loss = 'mse')\n",
        "autoencoder.summary()\n",
        "# fit the autoencoder\n",
        "autoencoder.fit(X_train.values, X_train.values, \n",
        "                validation_split = 0.1, epochs = 300, batch_size = 256, \n",
        "                verbose = True, callbacks = [es], use_multiprocessing = True)\n",
        "# extract representation\n",
        "X_train = pd.DataFrame(encoder.predict(X_train.values), columns = columns)\n",
        "X_test = pd.DataFrame(encoder.predict(X_test.values), columns = columns)\n",
        "\n",
        "# dumping the results in order to make the loadable due to issues with smac\n",
        "f = open(\"./AML_project/dumps/X_train_autoencoder.pkl\",\"wb\")\n",
        "pickle.dump(X_train,f)\n",
        "f.close()\n",
        "f = open(\"./AML_project/dumps/X_test_autoencoder.pkl\",\"wb\")\n",
        "pickle.dump(X_test,f)\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZaXxLn8V1vFm"
      },
      "source": [
        "If you used the autoencoder FE approach, you need to restart the runtime and run the following cell, since SMAC get stuck if we train any other model before it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NeFNc1w81vQ-",
        "colab": {}
      },
      "source": [
        "infile = open(\"./AML_project/dumps/X_train_autoencoder.pkl\",'rb')\n",
        "X_train = pickle.load(infile)\n",
        "infile.close()\n",
        "infile = open(\"./AML_project/dumps/X_test_autoencoder.pkl\",'rb')\n",
        "X_test = pickle.load(infile)\n",
        "infile.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "G4_TLzUNws1m"
      },
      "source": [
        "### Testing of some topologies\n",
        "In this part, we define some functions which will be used in the remaining part of the notebook. We also show the performance of three different topologies with different depth. All the NN have hidden layers with depth starting from 512, the depth of a layer is the half of the previous one. So, we test netwrks with 3,4 and 5 hidden layers. Additional tests with different netowrks were performend in a more embrional state of this work, so they results are not present anymore, but they did not fluctuate significantly from the ones presented.<br>\n",
        "Finally, all the networks have 12 output neurons with a sigmoid activation function: each of them represent a predicted value for a signle label, remember that is not a one-hot encoding, because a molecule can be positive to different toxicological experiments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yts-GtRjws15",
        "colab": {}
      },
      "source": [
        "# evaluate auc for a given model\n",
        "# it returns a dictionary with the auc for each label and the mean of the aucs\n",
        "def evaluate_performance(model, test_features, test_label):\n",
        "  test_predictions = model.predict(test_features)\n",
        "  test_pred_df = pd.DataFrame(data = test_predictions, columns = test_label.columns)\n",
        "  auc = dict()\n",
        "  for c_pred, c_true in zip(test_pred_df, test_label):\n",
        "    auc[c_true] = roc_auc_score(y_true = test_label[c_true], y_score = test_pred_df[c_pred])\n",
        "\n",
        "  return((auc, np.mean(list(auc.values()))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gx-dJhLgPxWI",
        "colab_type": "text"
      },
      "source": [
        "Technically speaking, this is a multi-label task, and keras do not offer any loss for these particular kind of tasks. So, we defined a custom binary crossentropy; to do that, we searched and adapted a solution found online."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-XSUeaV0WS_l",
        "colab": {}
      },
      "source": [
        "# def custom loss: weighted_binary_crossentropy\n",
        "def weighted_binary_crossentropy(POS_WEIGHT):\n",
        "  def loss(target, output):\n",
        "    # transform back to logits\n",
        "    _epsilon = kb.tensorflow_backend._to_tensor(kb.tensorflow_backend.epsilon(), \n",
        "                                                output.dtype.base_dtype)\n",
        "    output = tf.clip_by_value(output, _epsilon, 1 - _epsilon)\n",
        "    output = tf.log(output / (1 - output))\n",
        "    # compute weighted loss\n",
        "    loss = tf.nn.weighted_cross_entropy_with_logits(targets = target,\n",
        "                                                    logits = output,\n",
        "                                                    pos_weight = POS_WEIGHT)\n",
        "    return tf.reduce_mean(loss, axis = -1)\n",
        "  return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gJnEuTnDruM6"
      },
      "source": [
        "The output of the following cell is the performance of the different netowrks presented previously. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dfgt0GtmrqdR",
        "colab": {}
      },
      "source": [
        "es = EarlyStopping(monitor = 'val_loss', mode = 'min', verbose = 1, \n",
        "                    patience = 10, min_delta = 0.001, restore_best_weights = True)\n",
        "\n",
        "kfold = KFold(n_splits = 3, shuffle = True)\n",
        "POS_WEIGHT = 20\n",
        "\n",
        "for t, v in kfold.split(X_train, Y_train):\n",
        "  aucs = list()\n",
        "  for i in range(3):\n",
        "    # NN\n",
        "    NN = Sequential()\n",
        "    NN.add(Dense(512, input_shape = (X_train.shape[1],)))\n",
        "    NN.add(BatchNormalization())\n",
        "    NN.add(Activation('relu'))\n",
        "    NN.add(Dropout(rate = 0.2))\n",
        "    NN.add(Dense(256))\n",
        "    NN.add(BatchNormalization())\n",
        "    NN.add(Activation('relu'))\n",
        "    NN.add(Dropout(rate = 0.2))\n",
        "    NN.add(Dense(128))\n",
        "    NN.add(BatchNormalization())\n",
        "    NN.add(Activation('relu'))\n",
        "    NN.add(Dropout(rate = 0.2))\n",
        "    if i >= 1:\n",
        "      NN.add(Dense(64))\n",
        "      NN.add(BatchNormalization())\n",
        "      NN.add(Activation('relu'))\n",
        "      NN.add(Dropout(rate = 0.2))\n",
        "    if i >= 2:\n",
        "      NN.add(Dense(32))\n",
        "      NN.add(BatchNormalization())\n",
        "      NN.add(Activation('relu'))\n",
        "      NN.add(Dropout(rate = 0.2))\n",
        "    NN.add(Dense(12, activation = 'sigmoid'))\n",
        "\n",
        "    # Compile model\n",
        "    NN.compile(optimizer = 'adam', \n",
        "              loss = weighted_binary_crossentropy(POS_WEIGHT = POS_WEIGHT))\n",
        "    # Fit tne network\n",
        "    learning_process_NN = NN.fit(X_train.iloc[t], Y_train.iloc[t], \n",
        "                                validation_data = (X_train.iloc[v], Y_train.iloc[v]), \n",
        "                                callbacks = [es], epochs = 300, batch_size = 128, \n",
        "                                verbose = False, use_multiprocessing = True)\n",
        "    aucs.append(evaluate_performance(NN, X_train.iloc[v], Y_train.iloc[v])[1])\n",
        "\n",
        "  print(\"The average AUC in the 3 fold CV is {}\".format(np.mean(aucs)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "x_wgXbSCszHG"
      },
      "source": [
        "Since the deeper topologies do not bring any advantage, we decide to use a simpler topology (Occam's razor): three hidden layers with a depth of 512, 256 and 128 respectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcSnYAe8PxWR",
        "colab_type": "text"
      },
      "source": [
        "### Hyperparameter optimization\n",
        "In the following, we apply the task of hyperparameter optimization. We decided to do that after the preliminary exploration of possible topologies, mainly because it is time consuming. The budget used is of 120 possible points in the search space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iJTcK3bqws1u",
        "colab": {}
      },
      "source": [
        "# define early stopping\n",
        "es = EarlyStopping(monitor = 'val_loss', mode = 'min', verbose = 1, \n",
        "                    patience = 10, min_delta = 0.001, restore_best_weights = True)\n",
        "\n",
        "kfold = KFold(n_splits = 3, shuffle = True)\n",
        "# function that returns 1 - mean of average auc over 3 folds\n",
        "def create_compute_model(config):\n",
        "  POS_WEIGHT = config['pos_weight']\n",
        "  BATCH_SIZE = config['batch_size']\n",
        "  DROPOUT_RATE = config['dropout_rate']\n",
        "  ACTIVATION = config['activation']\n",
        "  L2_REG = config['l2_reg']\n",
        "\n",
        "  aucs = list()\n",
        "\n",
        "  for t, v in kfold.split(X_train, Y_train):\n",
        "    # NN\n",
        "    NN = Sequential()\n",
        "    NN.add(Dense(512, input_shape = (X_train.shape[1],), \n",
        "                 kernel_regularizer = regularizers.l2(L2_REG)))\n",
        "    NN.add(BatchNormalization())\n",
        "    NN.add(Activation(ACTIVATION))\n",
        "    NN.add(Dropout(rate = DROPOUT_RATE))\n",
        "    NN.add(Dense(256, kernel_regularizer = regularizers.l2(L2_REG)))\n",
        "    NN.add(BatchNormalization())\n",
        "    NN.add(Activation(ACTIVATION))\n",
        "    NN.add(Dropout(rate = DROPOUT_RATE))\n",
        "    NN.add(Dense(128, kernel_regularizer = regularizers.l2(L2_REG)))\n",
        "    NN.add(BatchNormalization())\n",
        "    NN.add(Activation(ACTIVATION))\n",
        "    NN.add(Dropout(rate = DROPOUT_RATE))\n",
        "    NN.add(Dense(12, activation = 'sigmoid'))\n",
        "\n",
        "    # compile model\n",
        "    NN.compile(optimizer = 'adam', \n",
        "               loss = weighted_binary_crossentropy(POS_WEIGHT = POS_WEIGHT))\n",
        "    # fit tne network\n",
        "    learning_process_NN = NN.fit(X_train.iloc[t], Y_train.iloc[t], \n",
        "                                 validation_data = (X_train.iloc[v], Y_train.iloc[v]), \n",
        "                                 callbacks = [es], epochs = 300, batch_size = BATCH_SIZE, \n",
        "                                 verbose = False, use_multiprocessing = True)\n",
        "    aucs.append(evaluate_performance(NN, X_train.iloc[v], Y_train.iloc[v])[1])\n",
        "    \n",
        "    # clear model representation\n",
        "    NN = None\n",
        "    learning_process_NN = None\n",
        "  print(\"The average AUC in the 3 fold CV is {}\".format(np.mean(aucs)))\n",
        "  return 1 - np.mean(aucs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xniZQHL0PxWU",
        "colab_type": "text"
      },
      "source": [
        "The search space is defined with the usage of these hyperparameter:\n",
        "- pos_weight is a parameter for the loss function, we treat it as a continuous hyperparameter which can vary from 0.1 to 50;\n",
        "- batch_size is the dimension of the batch during the trainig of the model, it is categorical and it can assume the following values: 64, 128, 256, 512 and 1024; they are all potence of 2 because of efficency with GPUs;\n",
        "- dropout_rate is the probability of the neurons to be turned off during the evaluation of a single batch; it is treated as continuous and it can vary from 0 (i.e., no dropout) from 0.7;\n",
        "- l2_reg is the regularization hyperparameter, it is treated as continuou and it spans form 0 to 0.2;\n",
        "- activation is the possible activation function used by the hidden layers;"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UnePoKfYTWSk",
        "colab": {}
      },
      "source": [
        "# define search spaces\n",
        "cs = ConfigurationSpace()\n",
        "pos_weight = UniformFloatHyperparameter('pos_weight', 0.1, 50, default_value = 10)\n",
        "batch_size = CategoricalHyperparameter('batch_size', [2 ** i for i in range(6, 11)], \n",
        "                                       default_value = 128)\n",
        "dropout_rate = UniformFloatHyperparameter('dropout_rate', 0.0, 0.7, default_value = 0.2)\n",
        "l2_reg = UniformFloatHyperparameter('l2_reg', 0.0, 0.2, default_value = 0.001)\n",
        "activation = CategoricalHyperparameter('activation', ['relu', 'tanh', 'exponential', 'linear', 'selu'], \n",
        "                                       default_value = 'relu')\n",
        "\n",
        "cs.add_hyperparameters([pos_weight, batch_size, dropout_rate, l2_reg, activation])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RMpwy4DmTWgg",
        "colab": {}
      },
      "source": [
        "# define scenario\n",
        "scenario = Scenario({\"run_obj\": \"quality\",\n",
        "                     \"runcount-limit\": 120,\n",
        "                     \"cs\": cs,\n",
        "                     \"deterministic\": \"True\"\n",
        "                     })"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rt41qylHTWnu",
        "colab": {}
      },
      "source": [
        "s_smac = SMAC4HPO(scenario = scenario, tae_runner = create_compute_model, \n",
        "                     acquisition_function = LCB, \n",
        "                     initial_design = latin_hypercube_design.LHDesign,\n",
        "                     initial_design_kwargs = {'max_config_fracs': 1 / 6,\n",
        "                                              'n_configs_x_params': 3})\n",
        "\n",
        "s_incumbent = s_smac.optimize()\n",
        "\n",
        "s_inc_value = create_compute_model(s_incumbent)\n",
        "\n",
        "print(\"Optimized Value: %.4f\" % (1 - s_inc_value))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EvOe1jz5nHti",
        "colab": {}
      },
      "source": [
        "# collect the results of the optimization and dump them in order to make them available \n",
        "# for further analysis without running the process again \n",
        "cfg_acc = dict()\n",
        "bs = dict()\n",
        "\n",
        "cfg_acc['no_fs'] = list()\n",
        "bs['no_fs'] = list()\n",
        "best_seen = 1 - s_smac.get_runhistory().get_cost(s_smac.get_runhistory().get_all_configs()[0])\n",
        "for config in s_smac.get_runhistory().get_all_configs():\n",
        "  loss = s_smac.get_runhistory().get_cost(config)\n",
        "  auc = 1 - loss\n",
        "  cfg_acc['no_fs'].append((config, auc))\n",
        "  if auc > best_seen:\n",
        "    best_seen = auc\n",
        "  bs['no_fs'].append(best_seen)\n",
        "\n",
        "print(bs['no_fs'])\n",
        " \n",
        "f = open(\"./AML_project/dumps/no_fs_cfg_acc.pkl\",\"wb\")\n",
        "pickle.dump(cfg_acc,f)\n",
        "f.close()\n",
        "f = open(\"./AML_project/dumps/no_fs_bs.pkl\",\"wb\")\n",
        "pickle.dump(bs,f)\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VN6QvH0H3G2q"
      },
      "source": [
        "To collect data for each feature reduction approach (and no feature reduction) we executed the hyperparameter optimization (the code above) 4 different times and dumped the results each time in a different file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yoCt_aqvPxWn",
        "colab_type": "text"
      },
      "source": [
        "### Analisys of hyperparameter optimization process results\n",
        "In this section, we analyze the results obtained by the optimization process: the best-seen configuration and, in some sense, if and how the search space was explored.\n",
        "First of all, we load the results of each optimization task (one optimization for each possible input strategy). Then, we plot the data to analyze what we said below, and finally we print the performance obtained by the network with the optimal hyperparameter for each input strategy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "y-7SnjttN3uP",
        "colab": {}
      },
      "source": [
        "# Gathering the results together\n",
        "bs = dict()\n",
        "cfg_auc = dict()\n",
        "possible_inputs = [\"no_fs\", \"corr\", \"pca\", \"autoencoder\"]\n",
        "for x in possible_inputs:\n",
        "  infile = open(\"./AML_project/dumps/{}_cfg_auc.pkl\".format(x),'rb')\n",
        "  cfg_auc.update(pickle.load(infile))\n",
        "  infile.close()\n",
        "  infile = open(\"./AML_project/dumps/{}_bs.pkl\".format(x),'rb')\n",
        "  bs.update(pickle.load(infile))\n",
        "  infile.close()\n",
        "\n",
        "labels = {\"no_fs\": \"No reduction\", \"corr\": \"Correlation\", \"pca\": \"PCA\", \"autoencoder\": \"Autoencoder\"}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9v82XxOFTTni",
        "colab": {}
      },
      "source": [
        "x = list(range(120))\n",
        "plt.figure(figsize = (8, 6))\n",
        "colors = [\"red\", \"orange\", \"blue\", \"black\"]\n",
        "for k, c in zip(bs.keys(), colors):\n",
        "  plt.plot(x, bs[k], '-', color = c, label = labels[k], linewidth = 1.5)\n",
        "\n",
        "plt.xticks(fontsize = 14)\n",
        "plt.yticks(fontsize = 14)\n",
        "plt.title(\"Best seen\", fontsize = 18)\n",
        "plt.xlabel(\"Iteration\", fontsize = 16)\n",
        "plt.ylabel(\"Mean of AUC\", fontsize = 16)\n",
        "plt.legend(loc = 'lower right')\n",
        "plt.savefig(\"./AML_project/images/pdf/best-seen.pdf\")\n",
        "plt.savefig(\"./AML_project/images/png/best-seen.png\", dpi = 300)\n",
        "plt.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vsDl-lCQzDTL",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize = (8, 6))\n",
        "for k, c in zip(cfg_auc.keys(), colors):\n",
        "  Y = [e[1] for e in cfg_auc[k]]\n",
        "  plt.plot(x, Y, color = c, label = labels[k], linewidth = 1.5)\n",
        "\n",
        "plt.xticks(fontsize = 14)\n",
        "plt.yticks(fontsize = 14)\n",
        "plt.title(\"AUC of configuration evaluation\", fontsize = 18)\n",
        "plt.xlabel(\"Configuration\", fontsize = 16)\n",
        "plt.ylabel(\"Mean of AUC\", fontsize = 16)\n",
        "plt.legend(loc = 'lower right')\n",
        "plt.savefig(\"./AML_project/images/pdf/AUC-iteration.pdf\")\n",
        "plt.savefig(\"./AML_project/images/png/AUC-iteration.png\", dpi = 300)\n",
        "plt.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XuyUvXis1hNB",
        "colab": {}
      },
      "source": [
        "# print best config for each method\n",
        "from operator import itemgetter\n",
        "for method in [\"no_fs\", \"corr\", \"pca\", \"autoencoder\"]:\n",
        "  tmp = max(cfg_auc[method], key = itemgetter(1))\n",
        "  print(\"Best configuration for {} method was \\n{}\\n with a mean AUC value of {}\\n\\n\".format(method, tmp[0], tmp[1]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2t5CJDYe4lDJ"
      },
      "source": [
        "### Analysis of the best model\n",
        "In this final part, we study deeply the behaviour of the neural network with the optimal configuration of the hyperparameters. First of all, we define some functions which will be used. Then, we train the network monitoring the loss and the mean of the auc. Finally, we plot how our model perform in comparison of what presented during the challenge and we study how the threshold of the %TP (Ture Positive percentage) affects the recall and precision of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fQRzL_VLD6r_",
        "colab": {}
      },
      "source": [
        "# plot the evolution of loss function during the epochs of the training of the network, for\n",
        "# both train and validation set\n",
        "def plot_history(network_history):\n",
        "  plt.figure()\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.axvline(x = len(network_history.history['loss']) - 10, linestyle= '--', color = 'red', linewidth = 1.5)\n",
        "  plt.plot(network_history.history['loss'])\n",
        "  plt.plot(network_history.history['val_loss'])\n",
        "  plt.legend(['Training', 'Validation'])\n",
        "\n",
        "  plt.savefig(\"./AML_project/images/pdf/learning_process.pdf\")\n",
        "  plt.savefig(\"./AML_project/images/png/learning_process.png\", dpi = 300)\n",
        "  plt.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ikk2BH8DySFG",
        "colab": {}
      },
      "source": [
        "# class to evaluate auc and roc at each step of the network training\n",
        "class roc_callback(Callback):\n",
        "  def __init__(self,training_data,validation_data):\n",
        "    self.x = training_data[0]\n",
        "    self.y = training_data[1]\n",
        "    self.x_val = validation_data[0]\n",
        "    self.y_val = validation_data[1]\n",
        "\n",
        "  def on_train_begin(self, logs = dict()):\n",
        "    return\n",
        "\n",
        "  def on_train_end(self, logs = dict()):\n",
        "    return\n",
        "\n",
        "  def on_epoch_begin(self, epoch, logs = dict()):\n",
        "    return\n",
        "\n",
        "  def on_epoch_end(self, epoch, logs = dict()):\n",
        "    y_pred = pd.DataFrame(data = self.model.predict(self.x), columns = self.y.columns)\n",
        "    y_pred_val = pd.DataFrame(data = self.model.predict(self.x_val), columns = self.y.columns)\n",
        "    average_roc = 0.0\n",
        "    average_roc_val = 0.0\n",
        "    for i in self.y.columns:\n",
        "      average_roc += roc_auc_score(self.y[i], y_pred[i])\n",
        "      average_roc_val += roc_auc_score(self.y_val[i], y_pred_val[i])\n",
        "\n",
        "    average_roc = average_roc / 12\n",
        "    average_roc_val = average_roc_val / 12\n",
        "    print('Average-roc-auc: {}   Average-roc-auc_val: {}'.format(round(average_roc, 4), \n",
        "                                                                  round(average_roc_val,4)))\n",
        "    return\n",
        "\n",
        "    def on_batch_begin(self, batch, logs = dict()):\n",
        "      return\n",
        "\n",
        "    def on_batch_end(self, batch, logs = dict()):\n",
        "      return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UUqjc8BHws10",
        "colab": {}
      },
      "source": [
        "# confusion matrix as a dictionary, very naive implementation\n",
        "# predict and evaluate performances gathering them in the confusion matrix\n",
        "def get_confusion_matrix(model, tpr_threshold, test_features, test_labels):\n",
        "  thresholds = get_thresholds(tpr_threshold, model, test_features, test_labels)\n",
        "  predictions = model.predict(test_features)\n",
        "  d = {k: {'t1': 0, 't0': 0, 'f1': 0, 'f0': 0} for k in test_labels.columns}\n",
        "  for preds, trues in zip(predictions, test_labels.itertuples()):\n",
        "    for p, t, k, i in zip(preds, trues[1:], test_labels.columns, range(12)):\n",
        "      p = 1 if p > thresholds[i] else 0\n",
        "      if p == t and p == 0:\n",
        "          d[k]['t0'] = d[k]['t0'] + 1\n",
        "      if p == t and p == 1:\n",
        "          d[k]['t1'] = d[k]['t1'] + 1\n",
        "      if p != t and p == 0:\n",
        "          d[k]['f0'] = d[k]['f0'] + 1\n",
        "      if p != t and p == 1:\n",
        "          d[k]['f1'] = d[k]['f1'] + 1\n",
        "  return d, thresholds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MX61sRCx0bpv",
        "colab": {}
      },
      "source": [
        "# function which prints the confusion matrix\n",
        "def print_confusion_matrix(confusion_matrix, label):\n",
        "  predicted_header = ''.join([' '] * 12) + \"PREDICTED\" + ' '\n",
        "  class_header = ''.join([' '] * 14) + \"CLASS\" + ''.join([' '] * 3)\n",
        "  class_values = ''.join([' '] * 13) + \"1\" + ''.join([' '] * 5) + \"0\" + ''.join([' '] * 2)\n",
        "  header = '\\n'.join([predicted_header, class_header, class_values])\n",
        "  first_row = \"ACTUAL  1\" + ''.join([' '] * 2) + '{0:^5d}'.format(confusion_matrix[label]['t1']) + ' ' + '{0:^5d}'.format(confusion_matrix[label]['f0'])\n",
        "  second_row = \" CLASS  0\" + ''.join([' '] * 2) + '{0:^5d}'.format(confusion_matrix[label]['f1']) + ' ' + '{0:^5d}'.format(confusion_matrix[label]['t0'])\n",
        "  to_print = '\\n'.join([header, first_row, second_row]) + '\\n'\n",
        "  print(to_print)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ug1Dv4D_Kjb9",
        "colab": {}
      },
      "source": [
        "# given the roc curve, calculate the threshold to obtain the %tp (tpr_threshold)\n",
        "def get_thresholds(tpr_threshold, model, X, Y):\n",
        "  test_predictions = model.predict(X)\n",
        "  test_pred_df = pd.DataFrame(data = test_predictions, columns = Y.columns)\n",
        "  prediction_thresholds = []\n",
        "  for i in range (12):\n",
        "    # note that tpr are already sorted ascending\n",
        "    _, tpr, thresholds = roc_curve(Y.iloc[:, i], test_pred_df.iloc[:, i])\n",
        "    for tpr_it, thresh_it in zip (tpr, thresholds):\n",
        "      if tpr_it >= tpr_threshold:\n",
        "        prediction_thresholds.append(thresh_it)\n",
        "        break\n",
        "  return prediction_thresholds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-oz2d6tbT3W_",
        "colab": {}
      },
      "source": [
        "# train with optimal hyperparameters\n",
        "POS_WEIGHT = 0.8226761093444144\n",
        "BATCH_SIZE = 256\n",
        "DROPOUT_RATE = 0.3573514213523089\n",
        "ACTIVATION = 'relu'\n",
        "L2_REG = 0.0001080355932884863\n",
        "\n",
        "X_t, X_v, Y_t, Y_v = train_test_split(X_train, Y_train, test_size = 0.20)\n",
        "\n",
        "es = EarlyStopping(monitor = 'val_loss', mode = 'min', verbose = 1, \n",
        "                    patience = 10, min_delta = 0.001, restore_best_weights = True)\n",
        "\n",
        "NN = Sequential()\n",
        "NN.add(Dense(512, input_shape = (X_train.shape[1],), \n",
        "              kernel_regularizer = regularizers.l2(L2_REG)))\n",
        "NN.add(BatchNormalization())\n",
        "NN.add(Activation(ACTIVATION))\n",
        "NN.add(Dropout(rate = DROPOUT_RATE))\n",
        "NN.add(Dense(256, kernel_regularizer = regularizers.l2(L2_REG)))\n",
        "NN.add(BatchNormalization())\n",
        "NN.add(Activation(ACTIVATION))\n",
        "NN.add(Dropout(rate = DROPOUT_RATE))\n",
        "NN.add(Dense(128, kernel_regularizer = regularizers.l2(L2_REG)))\n",
        "NN.add(BatchNormalization())\n",
        "NN.add(Activation(ACTIVATION))\n",
        "NN.add(Dropout(rate = DROPOUT_RATE))\n",
        "NN.add(Dense(12, activation = 'sigmoid'))\n",
        "\n",
        "# Compile model\n",
        "NN.compile(optimizer = 'adam', \n",
        "            loss = weighted_binary_crossentropy(POS_WEIGHT = POS_WEIGHT))\n",
        "# Fit tne network\n",
        "learning_process_NN = NN.fit(X_t, Y_t, \n",
        "                              validation_data = (X_v, Y_v), \n",
        "                              callbacks = [roc_callback(training_data = (X_t, Y_t), \n",
        "                              validation_data = (X_v, Y_v)), es],\n",
        "                              epochs = 300, batch_size = BATCH_SIZE, \n",
        "                              verbose = True, use_multiprocessing = True)\n",
        "plot_history(learning_process_NN)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SupYFKeGws1o",
        "colab": {}
      },
      "source": [
        "# print ROC curves for all classes\n",
        "test_predictions = NN.predict(X_test)\n",
        "\n",
        "test_pred_df = pd.DataFrame(data = test_predictions, columns = Y_test.columns)\n",
        "plt.figure(figsize = (8, 6))\n",
        "for i in range(12):\n",
        "  fpr, tpr, threshold = roc_curve(Y_test.iloc[:, i], test_pred_df.iloc[:, i])\n",
        "  roc_auc = auc(fpr, tpr)\n",
        "  plt.plot(fpr, tpr, label = ' '.join(Y_test.columns[i].split('.')))\n",
        "\n",
        "plt.xticks(fontsize = 14)\n",
        "plt.yticks(fontsize = 14)\n",
        "plt.title('Receiver Operating Characteristic', fontsize = 18)\n",
        "plt.legend(loc = 'lower right')\n",
        "plt.plot([0, 1], [0, 1],'r--')\n",
        "plt.xlim([0, 1])\n",
        "plt.ylim([0, 1])\n",
        "plt.ylabel('True Positive Rate', fontsize = 16)\n",
        "plt.xlabel('False Positive Rate', fontsize = 16)\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.savefig(\"./AML_project/images/pdf/ROC.pdf\")\n",
        "plt.savefig(\"./AML_project/images/png/ROC.png\", dpi = 300)\n",
        "plt.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2INomTZ5FQLQ",
        "colab": {}
      },
      "source": [
        "# plot how our model performs in comparison with all other models that partecipated to the\n",
        "# challenge and reported in the DeepTox paper. Their performance are stored in a csv, where 0\n",
        "# stands for a NaN value\n",
        "our_results, auc_mean = evaluate_performance(NN, X_test, Y_test)\n",
        "our_results = {k.replace('.', '_'): v for k, v in our_results.items()}\n",
        "perf_df = pd.read_csv(\"./AML_project/paper_perf.csv\")\n",
        "our_results['AVG'] = auc_mean\n",
        "nr_keys = [\"NR_AhR\", \"NR_AR\", \"NR_AR_LBD\", \"NR_Aromatase\", \"NR_ER\", \"NR_ER_LBD\", \"NR_PPAR_gamma\"]\n",
        "sr_keys = [\"SR_ARE\", \"SR_ATAD5\", \"SR_HSE\", \"SR_MMP\", \"SR_p53\"]\n",
        "our_results['NR'] = np.mean([our_results[k] for k in nr_keys])\n",
        "our_results['SR'] = np.mean([our_results[k] for k in sr_keys])\n",
        "our_results['model'] = \"our\"\n",
        "perf_df = perf_df.append(our_results, ignore_index = True)\n",
        "\n",
        "colors = 'red-orangered-darkorange-orange-gold-forestgreen-darkgreen-green-lightseagreen-cyan-deepskyblue-navy-blue-indigo-darkviolet-purple-crimson-saddlebrown-black'.split('-')\n",
        "label_map = {l: i for l, i in zip(perf_df.columns, range(len(perf_df.columns))) if l != 'model'}\n",
        "colors_map = {m: x for m, x in zip(perf_df['model'], colors)}\n",
        "legend_plot_best2 = dict() # the bold and gray background in the table of the paper\n",
        "\n",
        "plt.figure(figsize = (8, 6), dpi = 300)\n",
        "for r in perf_df.itertuples(index = False):\n",
        "  r = r._asdict()\n",
        "  model = r.pop('model') # r is  also updated\n",
        "  artist = None\n",
        "  for k in r:\n",
        "    if r[k] == 0: # in the csv, 0 means empty value\n",
        "      continue\n",
        "    artist, = plt.plot(r[k], label_map[k], color = colors_map[model], \n",
        "                       alpha = 1 if model == 'our' else 0.9,\n",
        "                       marker = 'x' if model == 'our' else '|',\n",
        "                       markersize = 12 if model == 'our' else 10,\n",
        "                       markeredgewidth = 2 if model == 'our' else 2,\n",
        "                       linestyle = 'None')\n",
        "  if model in ['DeepTox21', 'AMAZIZ', 'dmlab', 'microsomes', 'NCI', 'our']:\n",
        "    legend_plot_best2[model] = artist\n",
        "\n",
        "plt.xlim(left = 0.26, right = 1)\n",
        "xticks = [0.3, 0.4, 0.5, 0.6, 0.7]\n",
        "xticks.extend([round(0.7 + x * 0.05, 2) for x in range(1, 7)])\n",
        "plt.xticks(ticks = xticks, labels = xticks, fontsize = 14, rotation = 45)\n",
        "\n",
        "yticks = list()\n",
        "for l in label_map.keys():\n",
        "  if l == \"NR_PPAR_gamma\":\n",
        "    yticks.append(\"NR PPAR \\u03B3\")\n",
        "    continue\n",
        "  if l == \"NR_Aromatase\":\n",
        "    yticks.append(\"NR Arom.\")\n",
        "    continue\n",
        "  yticks.append(l.replace('_', ' '))\n",
        "\n",
        "plt.yticks(ticks = [v for v in label_map.values()], \n",
        "           labels = yticks, \n",
        "           fontsize = 14)\n",
        "plt.axhline(y = 3.5, color = 'r', linestyle = '-', linewidth = 1)\n",
        "plt.title(\"Comparison between different classifiers\", fontsize = 18)\n",
        "plt.xlabel(\"AUC\", fontsize = 16)\n",
        "plt.ylabel(\"Model\", fontsize = 16)\n",
        "plt.legend([a for a in legend_plot_best2.values()], \n",
        "           [x for x in legend_plot_best2.keys()],\n",
        "           numpoints = 1)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"./AML_project/images/pdf/comparison.pdf\")\n",
        "plt.savefig(\"./AML_project/images/png/comparison.png\")\n",
        "plt.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-NfLxFr_S1gs",
        "colab": {}
      },
      "source": [
        "# function that returns a dictionary label: perf, where perf is a dictionary with precision\n",
        "# and recall of the label. Those metrics are computed on the confusion matrix.\n",
        "def precision_recall_from_cm(cm):\n",
        "  each_class_pr = {k: dict() for k in cm.keys()}\n",
        "  for k in cm:\n",
        "    each_class_pr[k]['precision'] = cm[k]['t1'] / (cm[k]['t1'] + cm[k]['f1'])\n",
        "    each_class_pr[k]['recall'] = cm[k]['t1'] / (cm[k]['t1'] + cm[k]['f0'])\n",
        "  return each_class_pr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rQGIsKsCTyLR",
        "colab": {}
      },
      "source": [
        "# plot the evolution of precision and recall varying the threshold of TruePositive rate\n",
        "ts = np.linspace(0.1, 1, 10, endpoint = True)\n",
        "precisions_m = list()\n",
        "recalls_m = list()\n",
        "precisions_std = list()\n",
        "recalls_std = list()\n",
        "for t in ts:\n",
        "  cm, _ = get_confusion_matrix(NN, t, X_test, Y_test)\n",
        "  p_r = precision_recall_from_cm(cm)\n",
        "  ps = [p_r[k]['precision'] for k in p_r]\n",
        "  precisions_m.append(np.mean(ps))\n",
        "  precisions_std.append(np.std(ps))\n",
        "  rs = [p_r[k]['recall'] for k in p_r]\n",
        "  recalls_m.append(np.mean(rs))\n",
        "  recalls_std.append(np.std(rs))\n",
        "\n",
        "plt.figure(figsize = (8, 6), dpi = 300)\n",
        "\n",
        "plt.plot(ts, [m - s for m, s in zip(precisions_m, precisions_std)],\n",
        "\t\t\t color = 'green', linestyle = '-', linewidth = 0.2, antialiased = True,\n",
        "\t\t\t label = \"Average precision std\")\n",
        "plt.plot(ts, [m + s for m, s in zip(precisions_m, precisions_std)],\n",
        "\t\t\t color = 'green', linestyle = '-', linewidth = 0.2, antialiased = True)\n",
        "\n",
        "plt.fill_between(ts, [m - s for m, s in zip(precisions_m, precisions_std)],\n",
        "                 [m + s for m, s in zip(precisions_m, precisions_std)],\n",
        "                 color = 'green', alpha = 0.5)\n",
        "\n",
        "plt.plot(ts, [m - s for m, s in zip(recalls_m, recalls_std)],\n",
        "\t\t\t color = 'darkturquoise', linestyle = '-', linewidth = 0.2, antialiased = True,\n",
        "\t\t\t label = \"Average precision std\")\n",
        "plt.plot(ts, [m + s for m, s in zip(recalls_m, recalls_std)],\n",
        "\t\t\t color = 'darkturquoise', linestyle = '-', linewidth = 0.2, antialiased = True)\n",
        "\n",
        "plt.fill_between(ts, [m - s for m, s in zip(recalls_m, recalls_std)],\n",
        "                 [m + s for m, s in zip(recalls_m, recalls_std)],\n",
        "                 color = 'darkturquoise', alpha = 0.5)\n",
        "\n",
        "plt.plot(ts, precisions_m, '-', color = 'green', linewidth = 2.5,\n",
        "         label = \"Precision\", antialiased = True)\n",
        "plt.plot(ts, recalls_m, '-', color = 'darkturquoise', linewidth = 2.5, \n",
        "         label = \"Recall\", antialiased = True)\n",
        "\n",
        "plt.xlim(left = 0.0995, right = 1.0005)\n",
        "plt.ylim(bottom = 0.0, top = 1.0)\n",
        "plt.xticks(fontsize = 14)\n",
        "plt.yticks(fontsize = 14)\n",
        "plt.xlabel(\"%TP\", fontsize = 16)\n",
        "plt.ylabel(\"Value\", fontsize = 16)\n",
        "plt.title(\"Evolution of Precision and Recall\", fontsize = 18)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"./AML_project/images/pdf/pr_evolution.pdf\")\n",
        "plt.savefig(\"./AML_project/images/png/pr_evolution.png\")\n",
        "plt.close()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}