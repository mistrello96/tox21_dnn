{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toxicity - Matteo Mistri and Daniele Papetti\n",
    "**Tox21 dataset analysis and prediction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset input and preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e7xdczejJ_6k"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "X_train = pd.read_csv(\"./dataset/tox21_dense_train.csv\")\n",
    "X_test = pd.read_csv(\"./dataset/tox21_dense_test.csv\")\n",
    "Y_train = pd.read_csv(\"./dataset/tox21_labels_train.csv\")\n",
    "Y_test = pd.read_csv(\"./dataset/tox21_labels_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decide to drop the names of the samples because we are sure that it is an irrelevant feature for the further operations.\n",
    "We substitute the NaN values in the labels with 0 value since we assume that if the test was not performed, the doctors would have thought that the molecule would have not been involved in that biological pathway. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s6V_MPfGNFqp"
   },
   "outputs": [],
   "source": [
    "# drop first column that contains names\n",
    "X_train = X_train.drop(X_train.columns[[0]], axis = 1)\n",
    "X_test = X_test.drop(X_test.columns[[0]], axis = 1)\n",
    "\n",
    "Y_train = Y_train.drop(Y_train.columns[[0]], axis = 1)\n",
    "Y_test = Y_test.drop(Y_test.columns[[0]], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ce0pqt0fNMP2"
   },
   "outputs": [],
   "source": [
    "# transform NaN in 0 in the labels\n",
    "Y_train = Y_train.fillna(0)\n",
    "Y_test = Y_test.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO DATA EXPLORATION\n",
    "# distribuzione etichette\n",
    "# ricerca e gestione outliers (3 * Q3/4-q1/4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "print(X_train.shape)\n",
    "# remove outliers\n",
    "outliers = list()\n",
    "# if we consider feature by feature, we drop all the dataset\n",
    "# we count how many time a record is considered outlier\n",
    "for column in X_train.columns:\n",
    "    mean = X_train[column].mean()\n",
    "    q1 = X_train[column].quantile(1 / 4)\n",
    "    q3 = X_train[column].quantile(3 / 4)\n",
    "    threshold = 3 * (q3 - q1)\n",
    "\n",
    "    # Indexes of outliers\n",
    "    outliers.extend(X_train[X_train[column] > mean + threshold].index.values.tolist())      \n",
    "    outliers.extend(X_train[X_train[column] < mean - threshold].index.values.tolist())\n",
    "\n",
    "# drop a record if it is considered to be an outlier in more that 1/4 of the features\n",
    "out_counter = Counter(outliers)\n",
    "toDrop = [k for k, v in zip(out_counter.keys(), out_counter.values()) if v > 200]\n",
    "\n",
    "# Delete these row indexes from dataFrame\n",
    "X_train.drop(toDrop, inplace = True)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation between labels\n",
    "import numpy as np\n",
    "\n",
    "# Create correlation matrix\n",
    "corr_matrix = Y_train.append(Y_test).corr().abs()\n",
    "# Show matrix\n",
    "corr_matrix.style.background_gradient(cmap = 'coolwarm').set_precision(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation between features and labels\n",
    "\n",
    "# Create correlation matrix\n",
    "corr_matrix = X_train.append(X_test).merge(Y_train.append(Y_test),\n",
    "                                           right_index = True, left_index = True).corr().abs()\n",
    "# Print most correlated feature for each class\n",
    "for label in Y_train.columns:\n",
    "    # extract correlation column of the considered class\n",
    "    corr_col = corr_matrix[label]\n",
    "    # remove the elements whose indes is a label\n",
    "    cleaned_col = corr_col.drop(Y_train.columns, axis = 0)\n",
    "    # sort the result\n",
    "    sorted_col = cleaned_col.sort_values(ascending = False)\n",
    "    # extract the most correlated features for the considered class\n",
    "    print(\"Class {} is mosty correlated with feature {} with a value of {}\".format(label, sorted_col.index[0], round(sorted_col[0], 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search for missing values in features\n",
    "print(X_train.isnull().any().any())\n",
    "print(X_test.isnull().any().any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zIb5irM2SGA_"
   },
   "outputs": [],
   "source": [
    "# check if all features are numeric\n",
    "set(X_train.dtypes.append(X_test.dtypes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "905LrB1NNboE"
   },
   "outputs": [],
   "source": [
    "# normalize features in 0 mean and 1 std\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train.append(X_test).values)\n",
    "X_train = pd.DataFrame(scaler.transform(X_train.values), columns = X_train.columns)\n",
    "X_test = pd.DataFrame(scaler.transform(X_test.values), columns = X_test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E5nXCDuISkyF"
   },
   "source": [
    "**Use only one of the following method to reduce number of features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XdLtq27eMeun",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# correlation analysis and drop correlated features\n",
    "# create correlation matrix\n",
    "corr_matrix = X_train.corr().abs()\n",
    "\n",
    "# select upper triangle of correlation matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k = 1).astype(np.bool))\n",
    "\n",
    "# find index of feature columns with correlation greater than 0.90\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.90)]\n",
    "print(len(to_drop))\n",
    "\n",
    "# drop the features\n",
    "X_train.drop(X_train[to_drop], axis = 1)\n",
    "X_test.drop(X_test[to_drop], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nsX5l7J3P-12"
   },
   "outputs": [],
   "source": [
    "# PCA features reduction\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "features_limit = 100\n",
    "columns = ['col' + str(x) for x in range(features_limit)]\n",
    "PCA_transformer = PCA(n_components = features_limit)\n",
    "PCA_transformer.fit(X_train.append(X_test).values)\n",
    "X_train = pd.DataFrame(PCA_transformer.transform(X_train.values), columns = columns)\n",
    "X_test = pd.DataFrame(PCA_transformer.transform(X_test.values), columns = columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vfe4cnNtTABp",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "import keras.optimizers\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "# AUTOENCODER\n",
    "# Define early stopping\n",
    "es = EarlyStopping(monitor = 'val_loss', mode = 'min', \n",
    "                   verbose = 1, patience = 15, min_delta = 0.001,\n",
    "                   restore_best_weights = True)\n",
    "\n",
    "encoding_dim1 = int(X_train.shape[1] / 2)\n",
    "encoding_dim2 = int(encoding_dim1 / 2)\n",
    "encoding_dim3 = 100\n",
    "columns = ['col' + str(x) for x in range(encoding_dim3)]\n",
    "\n",
    "input_layer = Input(shape = (X_train.shape[1],))\n",
    "# \"encoded\" is the encoded representation of the input\n",
    "encoded1 = Dense(encoding_dim1, activation = 'relu')(input_layer)\n",
    "encoded2 = Dense(encoding_dim2, activation = 'relu')(encoded1)\n",
    "bottleneck = Dense(encoding_dim3, activation = 'relu')(encoded2)\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "decoded2 = Dense(encoding_dim2, activation = 'relu')(bottleneck)\n",
    "decoded1 = Dense(encoding_dim1, activation = 'relu')(decoded2)\n",
    "output_layer = Dense(X_train.shape[1], activation = 'linear')(decoded1)\n",
    "# this model maps an input to its reconstruction\n",
    "autoencoder = Model(input_layer, output_layer)\n",
    "# this model maps an input to its encoded representation\n",
    "encoder = Model(input_layer, bottleneck)\n",
    "\n",
    "# compile the model\n",
    "autoencoder.compile(optimizer = 'adam', loss = 'mse', metrics = ['accuracy'])\n",
    "autoencoder.summary()\n",
    "# fit the autoencoder\n",
    "autoencoder.fit(X_train.append(X_test).values, X_train.append(X_test).values, \n",
    "                validation_split = 0.1, epochs = 300, batch_size = 256, \n",
    "                verbose = True, callbacks = [es], use_multiprocessing = True)\n",
    "# extract representation\n",
    "X_train = pd.DataFrame(encoder.predict(X_train.values), columns = columns)\n",
    "X_test = pd.DataFrame(encoder.predict(X_test.values), columns = columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# controllare funzione di attivazione autoencoder e relativa loss\n",
    "# data visualization in 2/3D per vedere se clusterizza (a occhio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GGQsJNk_Mw4E"
   },
   "outputs": [],
   "source": [
    "# Test print\n",
    "print(X_train)\n",
    "print(X_test)\n",
    "\n",
    "print(Y_train)\n",
    "print(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classification of the records**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# creazione della DNN cin keras\n",
    "    # esplorazione preliminare dei parametri (con tutte le features)\n",
    "# confronto con la medesima DNN delle 3 feature extraction\n",
    "# AutoML (maybe AutoKeras) su DNN o RandomForest\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "AML Project.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
