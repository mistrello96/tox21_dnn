{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"AML_Project.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"X71Bwbwowszz"},"source":["# Toxicity - Matteo Mistri and Daniele Papetti\n","**Tox21 dataset analysis and prediction**"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"QkkAc7HDszms"},"source":["Please, do note that this notebook is thought to run on colab and using google drive as storage system, so you may need to change the directories of input and output in order to make the notebook run locally. If you have possible suggestions to fix this problem, you can write us. Any suggestion will be appreciated :)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"15GuXYIXwsz5"},"source":["**Dataset input and preprocessing**"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"pddFR07XwytH","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Nty7chPCwyYk","colab":{}},"source":["cd drive/'My Drive'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"XBIqp-XGTc99","colab":{}},"source":["!apt-get install swig"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"SbhlGoZmTdET","colab":{}},"source":["pip install smac"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ZIUyTtoUs4jm","colab":{}},"source":["pip install pyDOE"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TadpliB8W-8M","colab_type":"code","colab":{}},"source":["import os\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n","import warnings\n","warnings.filterwarnings(\"ignore\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"5P_gvopKYHOH","colab":{}},"source":["import pandas as pd\n","import numpy as np\n","import pyDOE\n","import pickle\n","from collections import Counter\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","import seaborn as sns\n","from operator import itemgetter\n","\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.decomposition import PCA\n","from sklearn.metrics import roc_auc_score, auc, roc_curve\n","from sklearn.model_selection import KFold, train_test_split\n","from sklearn.utils.class_weight import compute_class_weight\n","\n","from keras.callbacks import EarlyStopping, Callback\n","from keras.models import Sequential, Model\n","from keras.layers import Dense, BatchNormalization, Dropout, Activation, Input\n","from keras import regularizers\n","from keras import optimizers\n","\n","import tensorflow as tf\n","tf.logging.set_verbosity(tf.logging.ERROR)\n","import keras.backend as kb\n","\n","# Import ConfigSpace and different types of parameters\n","from smac.configspace import ConfigurationSpace, Configuration\n","from ConfigSpace.hyperparameters import UniformFloatHyperparameter, UniformIntegerHyperparameter, CategoricalHyperparameter\n","# Import SMAC-utilities\n","from smac.scenario.scenario import Scenario\n","from smac.facade.smac_hpo_facade import SMAC4HPO\n","from smac.optimizer.acquisition import LCB\n","from smac.initial_design import latin_hypercube_design\n","\n","import pickle"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"e7xdczejJ_6k","colab":{}},"source":["X_train = pd.read_csv(\"./AML_project/tox21_dense_train.csv\")\n","X_test = pd.read_csv(\"./AML_project/tox21_dense_test.csv\")\n","Y_train = pd.read_csv(\"./AML_project/tox21_labels_train.csv\")\n","Y_test = pd.read_csv(\"./AML_project/tox21_labels_test.csv\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"EBGbIq_Kws0I"},"source":["We decide to drop the names of the samples because we are sure that it is an irrelevant feature for the further operations.\n","We substitute the NaN values in the labels with 0 value since we assume that if the test was not performed, the doctors would have thought that the molecule would have not been involved in that biological pathway. "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"s6V_MPfGNFqp","colab":{}},"source":["# drop first column that contains names\n","X_train = X_train.drop(X_train.columns[[0]], axis = 1)\n","X_test = X_test.drop(X_test.columns[[0]], axis = 1)\n","\n","Y_train = Y_train.drop(Y_train.columns[[0]], axis = 1)\n","Y_test = Y_test.drop(Y_test.columns[[0]], axis = 1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Ce0pqt0fNMP2","colab":{}},"source":["# transform NaN in 0 in the labels\n","Y_train = Y_train.fillna(0)\n","Y_test = Y_test.fillna(0)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"MNUixAt7ws0a","colab":{}},"source":["# distribuzione etichette\n","for l in Y_train.columns:\n","  unique_values_count = Counter(Y_train[l])\n","  print(\"{}: {}\".format(l, unique_values_count))\n","\n","  plt.figure(figsize = (8, 6))\n","  sns.countplot(x = l, data = Y_train)\n","  plt.xlabel(\"Values\", fontsize = 16)\n","  plt.ylabel(\"Number of instances\", fontsize = 16)\n","  plt.title(' '.join(l.split('.')), fontsize = 18)\n","  plt.xticks(ticks = [0, 1], labels=[0, 1], fontsize = 12)\n","  plt.yticks(fontsize = 12)\n","  plt.savefig(\"./AML_project/images/png/hist-{}.png\".format(''.join(l.split('.'))), dpi=300)\n","  plt.savefig(\"./AML_project/images/pdf/hist-{}.pdf\".format(''.join(l.split('.'))))\n","  plt.close()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"f0xsjp3Hi2_8","colab":{}},"source":["# remove features with zero variance, since they carry no information\n","var_zero_columns = [c for c in X_train.columns if len(np.unique(X_train[c])) == 1]\n","X_train = X_train.drop(var_zero_columns, axis = 1)\n","X_test = X_test.drop(var_zero_columns, axis = 1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"V1_QcGE8ws0i","colab":{}},"source":["# remove outliers\n","outliers = list()\n","# if we consider feature by feature, we drop all the dataset\n","# we count how many time a record is considered outlier\n","for column in X_train.columns:\n","  mean = X_train[column].mean()\n","  q1 = X_train[column].quantile(1 / 4)\n","  q3 = X_train[column].quantile(3 / 4)\n","  threshold = 3 * (q3 - q1)\n","\n","  # Indexes of outliers\n","  outliers.extend(X_train[X_train[column] > mean + threshold].index.values.tolist())      \n","  outliers.extend(X_train[X_train[column] < mean - threshold].index.values.tolist())\n","\n","# drop a record if it is considered to be an outlier in more that 1/4 of the features\n","out_counter = Counter(outliers)\n","toDrop = [k for k, v in zip(out_counter.keys(), out_counter.values()) if v > 200]\n","\n","# Delete these row indexes from dataFrame\n","X_train.drop(toDrop, inplace = True)\n","Y_train.drop(toDrop, inplace = True)\n","X_train.reset_index(drop = True, inplace = True)\n","Y_train.reset_index(drop = True, inplace = True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"fxjwvYhRws0q","colab":{}},"source":["# correlation between labels\n","# Create correlation matrix\n","corr_matrix = Y_train.corr().abs()\n","\n","plt.figure(figsize = (8, 6))\n","'''\n","issue with the labels cut off, know issue with matplotlib\n","sns.heatmap(corr_matrix, fmt = '.2f', annot = True, vmin=0, vmax=1,\n","            xticklabels = ['\\n'.join(x.split('.')) for x in corr_matrix],\n","            yticklabels = ['\\n'.join(x.split('.')) for x in corr_matrix])\n","'''\n","sns.heatmap(corr_matrix, fmt = '.2f', annot = True, vmin = 0, vmax = 1,\n","            xticklabels = range(12), yticklabels = range(12))\n","plt.title(\"Labels correlation matrix heatmap\", fontsize = 18)\n","plt.tight_layout()\n","plt.savefig(\"./AML_project/images/png/labels_corr_matrix_heatmap.png\", dpi=300)\n","plt.savefig(\"./AML_project/images/pdf/labels_corr_matrix_heatmap.pdf\")\n","plt.close()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"n0i6zTSNws0u","colab":{}},"source":["# correlation between features and labels\n","corr_matrix = X_train.merge(Y_train, right_index = True, left_index = True).corr().abs()\n","# Print most correlated feature for each class\n","s = set()\n","for label in Y_train.columns:\n","  # extract correlation column of the considered class\n","  corr_col = corr_matrix[label]\n","  # remove the elements whose indes is a label\n","  cleaned_col = corr_col.drop(Y_train.columns, axis = 0)\n","  # sort the result\n","  sorted_col = cleaned_col.sort_values(ascending = False)\n","  # extract the most correlated features for the considered class\n","  s.add(sorted_col.index[0])\n","  print(\"Class {} is mosty correlated with feature {} with a value of {}\".format(label, sorted_col.index[0], round(sorted_col[0], 3)))\n","print(\"Unique features: {}\".format(len(s)))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"QwkSRJQEszn6","colab":{}},"source":["# correlation between labels and features\n","high_corr_features_count = {k: (sum(1 for x in corr_matrix[k] if x > 0.90) - 1) for k in corr_matrix.columns}\n","c = Counter(high_corr_features_count.values())\n","print(c)\n","\n","plt.Figure(figsize = (8, 6))\n","plt.yscale('log')\n","plt.ylim(bottom = min(c.values()) - 0.2, top = max(c.values()) + 50)\n","for p, v in c.items():\n","  plt.scatter(p, v)\n","plt.xlabel(\"Number of highly correlated features/labels\", fontsize = 16)\n","plt.ylabel(\"Number of features/labels\", fontsize = 16)\n","plt.xticks(fontsize = 14)\n","plt.yticks(fontsize = 14)\n","plt.title(\"Distribution of correlated features/labels\", fontsize = 18)\n","plt.tight_layout()\n","plt.savefig(\"./AML_project/images/png/distribution_high_corr.png\", dpi=300)\n","plt.savefig(\"./AML_project/images/pdf/distribution_high_corr.pdf\")\n","plt.close()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"BPetLISCws02","colab":{}},"source":["# search for missing values in features\n","print(X_train.isnull().any().any())\n","print(X_test.isnull().any().any())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"zIb5irM2SGA_","colab":{}},"source":["# check if all features are numeric\n","set(X_train.dtypes.append(X_test.dtypes))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"905LrB1NNboE","colab":{}},"source":["# normalize features in 0 mean and 1 std\n","scaler = StandardScaler()\n","scaler.fit(X_train.append(X_test).values)\n","X_train = pd.DataFrame(scaler.transform(X_train.values), columns = X_train.columns)\n","X_test = pd.DataFrame(scaler.transform(X_test.values), columns = X_test.columns)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"E5nXCDuISkyF"},"source":["**Use only one of the following method to reduce number of features**"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"XdLtq27eMeun","scrolled":true,"colab":{}},"source":["# correlation analysis and drop correlated features\n","# create correlation matrix\n","corr_matrix = X_train.corr().abs()\n","\n","# select upper triangle of correlation matrix\n","upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k = 1).astype(np.bool))\n","\n","# find index of feature columns with correlation greater than 0.90\n","to_drop = [column for column in upper.columns if any(upper[column] > 0.90)]\n","\n","# drop the features\n","X_train = X_train.drop(X_train[to_drop], axis = 1)\n","X_test = X_test.drop(X_test[to_drop], axis = 1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"nsX5l7J3P-12","colab":{}},"source":["# PCA features reduction\n","features_limit = 100\n","columns = ['col' + str(x) for x in range(features_limit)]\n","PCA_transformer = PCA(n_components = features_limit)\n","PCA_transformer.fit(X_train.values)\n","X_train = pd.DataFrame(PCA_transformer.transform(X_train.values), columns = columns)\n","X_test = pd.DataFrame(PCA_transformer.transform(X_test.values), columns = columns)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"vfe4cnNtTABp","scrolled":true,"colab":{}},"source":["# AUTOENCODER\n","# Define early stopping\n","es = EarlyStopping(monitor = 'val_loss', mode = 'min', \n","                   verbose = 1, patience = 15, min_delta = 0.001,\n","                   restore_best_weights = True)\n","\n","encoding_dim1 = int(X_train.shape[1] / 2)\n","encoding_dim2 = int(encoding_dim1 / 2)\n","encoding_dim3 = 100\n","columns = ['col' + str(x) for x in range(encoding_dim3)]\n","\n","input_layer = Input(shape = (X_train.shape[1],))\n","# \"encoded\" is the encoded representation of the input\n","encoded1 = Dense(encoding_dim1, activation = 'relu')(input_layer)\n","encoded2 = Dense(encoding_dim2, activation = 'relu')(encoded1)\n","bottleneck = Dense(encoding_dim3, activation = 'relu')(encoded2)\n","# \"decoded\" is the lossy reconstruction of the input\n","decoded2 = Dense(encoding_dim2, activation = 'relu')(bottleneck)\n","decoded1 = Dense(encoding_dim1, activation = 'relu')(decoded2)\n","output_layer = Dense(X_train.shape[1], activation = 'linear')(decoded1)\n","# this model maps an input to its reconstruction\n","autoencoder = Model(input_layer, output_layer)\n","# this model maps an input to its encoded representation\n","encoder = Model(input_layer, bottleneck)\n","\n","# compile the model\n","autoencoder.compile(optimizer = 'adam', loss = 'mse')\n","autoencoder.summary()\n","# fit the autoencoder\n","autoencoder.fit(X_train.values, X_train.values, \n","                validation_split = 0.1, epochs = 300, batch_size = 256, \n","                verbose = True, callbacks = [es], use_multiprocessing = True)\n","# extract representation\n","X_train = pd.DataFrame(encoder.predict(X_train.values), columns = columns)\n","X_test = pd.DataFrame(encoder.predict(X_test.values), columns = columns)\n","\n","f = open(\"./AML_project/dumps/X_train_autoencoder.pkl\",\"wb\")\n","pickle.dump(X_train,f)\n","f.close()\n","f = open(\"./AML_project/dumps/X_test_autoencoder.pkl\",\"wb\")\n","pickle.dump(X_test,f)\n","f.close()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZaXxLn8V1vFm","colab_type":"text"},"source":["If we used the autoencoder FE approach, we need to restart the runtime and run the cell below, since SMAC get stuck if we train any other model before him"]},{"cell_type":"code","metadata":{"id":"NeFNc1w81vQ-","colab_type":"code","colab":{}},"source":["infile = open(\"./AML_project/dumps/X_train_autoencoder.pkl\",'rb')\n","X_train = pickle.load(infile)\n","infile.close()\n","infile = open(\"./AML_project/dumps/X_test_autoencoder.pkl\",'rb')\n","X_test = pickle.load(infile)\n","infile.close()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"G4_TLzUNws1m"},"source":["**Classification of the records**"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"yts-GtRjws15","colab":{}},"source":["# evaluate auc for a given model\n","def evaluate_performance(model, test_features, test_label):\n","  test_predictions = model.predict(test_features)\n","  test_pred_df = pd.DataFrame(data = test_predictions, columns = test_label.columns)\n","  auc = dict()\n","  for c_pred, c_true in zip(test_pred_df, test_label):\n","    auc[c_true] = roc_auc_score(y_true = test_label[c_true], y_score = test_pred_df[c_pred])\n","\n","  #for k in auc:\n","  #  print(\"{}: {}\".format(k, auc[k]))\n","\n","  #print(\"\\nmean: {}\".format(np.mean(list(auc.values()))))\n","  return((auc, np.mean(list(auc.values()))))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"-XSUeaV0WS_l","colab":{}},"source":["# def custom loss(weighted_binary_crossentropy not defined in keras)\n","def weighted_binary_crossentropy(POS_WEIGHT):\n","  def loss(target, output):\n","    # transform back to logits\n","    _epsilon = kb.tensorflow_backend._to_tensor(kb.tensorflow_backend.epsilon(), \n","                                                output.dtype.base_dtype)\n","    output = tf.clip_by_value(output, _epsilon, 1 - _epsilon)\n","    output = tf.log(output / (1 - output))\n","    # compute weighted loss\n","    loss = tf.nn.weighted_cross_entropy_with_logits(targets = target,\n","                                                    logits = output,\n","                                                    pos_weight = POS_WEIGHT)\n","    return tf.reduce_mean(loss, axis = -1)\n","  return loss"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"iJTcK3bqws1u","colab":{}},"source":["### CLASSIC NN\n","# Define early stopping\n","es = EarlyStopping(monitor = 'val_loss', mode = 'min', verbose = 1, \n","                    patience = 10, min_delta = 0.001, restore_best_weights = True)\n","\n","kfold = KFold(n_splits = 3, shuffle = True)\n","# function that returns 1 - mean(average auc)\n","def create_compute_model(config):\n","  POS_WEIGHT = config['pos_weight']\n","  BATCH_SIZE = config['batch_size']\n","  DROPOUT_RATE = config['dropout_rate']\n","  ACTIVATION = config['activation']\n","  L2_REG = config['l2_reg']\n","\n","  aucs = list()\n","\n","  for t, v in kfold.split(X_train, Y_train):\n","    # NN\n","    NN = Sequential()\n","    NN.add(Dense(512, input_shape = (X_train.shape[1],), \n","                 kernel_regularizer = regularizers.l2(L2_REG)))\n","    NN.add(BatchNormalization())\n","    NN.add(Activation(ACTIVATION))\n","    NN.add(Dropout(rate = DROPOUT_RATE))\n","    NN.add(Dense(256, kernel_regularizer = regularizers.l2(L2_REG)))\n","    NN.add(BatchNormalization())\n","    NN.add(Activation(ACTIVATION))\n","    NN.add(Dropout(rate = DROPOUT_RATE))\n","    NN.add(Dense(128, kernel_regularizer = regularizers.l2(L2_REG)))\n","    NN.add(BatchNormalization())\n","    NN.add(Activation(ACTIVATION))\n","    NN.add(Dropout(rate = DROPOUT_RATE))\n","    NN.add(Dense(12, activation = 'sigmoid'))\n","\n","    # Compile model\n","    NN.compile(optimizer = 'adam', \n","               loss = weighted_binary_crossentropy(POS_WEIGHT = POS_WEIGHT))\n","    # Fit tne network\n","\n","    learning_process_NN = NN.fit(X_train.iloc[t], Y_train.iloc[t], \n","                                 validation_data = (X_train.iloc[v], Y_train.iloc[v]), \n","                                 callbacks = [es], epochs = 300, batch_size = BATCH_SIZE, \n","                                 verbose = False, use_multiprocessing = True)\n","    aucs.append(evaluate_performance(NN, X_train.iloc[v], Y_train.iloc[v])[1])\n","    \n","    # clear model representation\n","    NN = None\n","    learning_process_NN = None\n","    #kb.clear_session()\n","  print(\"The average AUC in the 3 fold CV is {}\".format(np.mean(aucs)))\n","  return 1 - np.mean(aucs)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"UnePoKfYTWSk","colab":{}},"source":["# define search spaces\n","cs = ConfigurationSpace()\n","pos_weight = UniformFloatHyperparameter('pos_weight', 0.1, 50, default_value = 10)\n","batch_size = CategoricalHyperparameter('batch_size', [2 ** i for i in range(6, 11)], \n","                                       default_value = 128)\n","dropout_rate = UniformFloatHyperparameter('dropout_rate', 0.0, 0.7, default_value = 0.2)\n","l2_reg = UniformFloatHyperparameter('l2_reg', 0.0, 0.2, default_value = 0.001)\n","activation = CategoricalHyperparameter('activation', ['relu', 'tanh', 'exponential', 'linear', 'selu'], \n","                                       default_value = 'relu')\n","\n","cs.add_hyperparameters([pos_weight, batch_size, dropout_rate, l2_reg, activation])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"RMpwy4DmTWgg","colab":{}},"source":["# define scenario\n","scenario = Scenario({\"run_obj\": \"quality\",\n","                     \"runcount-limit\": 120,\n","                     \"cs\": cs,\n","                     \"deterministic\": \"True\"\n","                     })"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"rt41qylHTWnu","colab":{}},"source":["s_smac = SMAC4HPO(scenario = scenario, tae_runner = create_compute_model, \n","                     acquisition_function = LCB, \n","                     initial_design = latin_hypercube_design.LHDesign,\n","                     initial_design_kwargs = {'max_config_fracs': 1 / 6,\n","                                              'n_configs_x_params': 3})\n","\n","s_incumbent = s_smac.optimize()\n","\n","s_inc_value = create_compute_model(s_incumbent)\n","\n","print(\"Optimized Value: %.4f\" % (1 - s_inc_value))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EvOe1jz5nHti","colab_type":"code","colab":{}},"source":["cfg_acc = dict()\n","bs = dict()\n","\n","cfg_acc['no_fs'] = list()\n","bs['no_fs'] = list()\n","best_seen = 1 - s_smac.get_runhistory().get_cost(s_smac.get_runhistory().get_all_configs()[0])\n","for config in s_smac.get_runhistory().get_all_configs():\n","  loss = s_smac.get_runhistory().get_cost(config)\n","  auc = 1 - loss\n","  cfg_acc['no_fs'].append((config, auc))\n","  if auc > best_seen:\n","    best_seen = auc\n","  bs['no_fs'].append(best_seen)\n","\n","print(bs['no_fs'])\n"," \n","f = open(\"./AML_project/dumps/no_fs_cfg_acc.pkl\",\"wb\")\n","pickle.dump(cfg_acc,f)\n","f.close()\n","f = open(\"./AML_project/dumps/no_fs_bs.pkl\",\"wb\")\n","pickle.dump(bs,f)\n","f.close()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VN6QvH0H3G2q","colab_type":"text"},"source":["Since now we need to perform the AutoML task for each FE approach, we have executed the notebook 4 time, one for each approach, dumping the result to a different pickle file. We will now gather all the data and do some analysis"]},{"cell_type":"code","metadata":{"id":"y-7SnjttN3uP","colab_type":"code","colab":{}},"source":["# Gathering the results together\n","bs = dict()\n","cfg_auc = dict()\n","possible_inputs = [\"no_fs\", \"corr\", \"pca\", \"autoencoder\"]\n","for x in possible_inputs:\n","  infile = open(\"./AML_project/dumps/{}_cfg_auc.pkl\".format(x),'rb')\n","  cfg_auc.update(pickle.load(infile))\n","  infile.close()\n","  infile = open(\"./AML_project/dumps/{}_bs.pkl\".format(x),'rb')\n","  bs.update(pickle.load(infile))\n","  infile.close()\n","\n","labels = {\"no_fs\": \"No reduction\", \"corr\": \"Selection by correlation\", \"pca\": \"Selection using PCA\", \"autoencoder\": \"Selection using encoded representation of an autoencoder\"}"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9v82XxOFTTni","colab_type":"code","colab":{}},"source":["x = list(range(120))\n","plt.figure(figsize = (8, 6))\n","colors = [\"red\", \"orange\", \"blue\", \"black\"]\n","for k, c in zip(bs.keys(), colors):\n","  plt.plot(x, bs[k], '-', color = c, label = labels[k], linewidth = 1.5)\n","\n","plt.xticks(fontsize = 14)\n","plt.yticks(fontsize = 14)\n","plt.title(\"Best seen\", fontsize = 18)\n","plt.xlabel(\"Iteration\", fontsize = 16)\n","plt.ylabel(\"Mean of AUC\", fontsize = 16)\n","plt.legend(loc = 'lower right')\n","plt.savefig(\"./AML_project/images/pdf/best-seen.pdf\")\n","plt.savefig(\"./AML_project/images/png/best-seen.png\", dpi=300)\n","plt.close()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vsDl-lCQzDTL","colab_type":"code","colab":{}},"source":["plt.figure(figsize = (8, 6))\n","for k, c in zip(cfg_auc.keys(), colors):\n","  Y = [e[1] for e in cfg_auc[k]]\n","  plt.plot(x, Y, color = c, label = labels[k], linewidth = 1.5)\n","\n","plt.xticks(fontsize = 14)\n","plt.yticks(fontsize = 14)\n","plt.title(\"AUC of configuration evaluation\", fontsize = 18)\n","plt.xlabel(\"Configuration\", fontsize = 16)\n","plt.ylabel(\"Mean of AUC\", fontsize = 16)\n","plt.legend(loc = 'lower right')\n","plt.savefig(\"./AML_project/images/pdf/AUC-iteration.pdf\")\n","plt.savefig(\"./AML_project/images/png/AUC-iteration.png\", dpi=300)\n","plt.close()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XuyUvXis1hNB","colab_type":"code","colab":{}},"source":["# print best config for each method\n","from operator import itemgetter\n","for method in [\"no_fs\", \"corr\", \"pca\", \"autoencoder\"]:\n","  tmp = max(cfg_auc[method], key = itemgetter(1))\n","  print(\"Best configuration for {} method was \\n{}\\n with a mean AUC value of {}\\n\\n\".format(method, tmp[0], tmp[1]))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2t5CJDYe4lDJ","colab_type":"text"},"source":["***Now we will train the model with the optimal configuration and we will evaluate the test set***"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"fQRzL_VLD6r_","colab":{}},"source":["def plot_history(network_history):\n","  plt.figure()\n","  plt.xlabel('Epochs')\n","  plt.ylabel('Loss')\n","  plt.plot(network_history.history['loss'])\n","  plt.plot(network_history.history['val_loss'])\n","  plt.axvline(x = len(network_history.history['loss']) - 10, linestyle= '--', color = 'red', linewidth = 1.5)\n","  plt.legend(['Training', 'Validation'])\n","\n","  plt.savefig(\"./AML_project/images/pdf/learning_process.pdf\")\n","  plt.savefig(\"./AML_project/images/png/learning_process.png\", dpi=300)\n","  plt.close()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Ikk2BH8DySFG","colab":{}},"source":["class roc_callback(Callback):\n","  def __init__(self,training_data,validation_data):\n","    self.x = training_data[0]\n","    self.y = training_data[1]\n","    self.x_val = validation_data[0]\n","    self.y_val = validation_data[1]\n","\n","  def on_train_begin(self, logs = dict()):\n","    return\n","\n","  def on_train_end(self, logs = dict()):\n","    return\n","\n","  def on_epoch_begin(self, epoch, logs = dict()):\n","    return\n","\n","  def on_epoch_end(self, epoch, logs = dict()):\n","    y_pred = pd.DataFrame(data = self.model.predict(self.x), columns = self.y.columns)\n","    y_pred_val = pd.DataFrame(data = self.model.predict(self.x_val), columns = self.y.columns)\n","    average_roc = 0.0\n","    average_roc_val = 0.0\n","    for i in self.y.columns:\n","      average_roc += roc_auc_score(self.y[i], y_pred[i])\n","      average_roc_val += roc_auc_score(self.y_val[i], y_pred_val[i])\n","\n","    average_roc = average_roc / 12\n","    average_roc_val = average_roc_val / 12\n","    print('Average-roc-auc: {}   Average-roc-auc_val: {}'.format(round(average_roc, 4), \n","                                                                  round(average_roc_val,4)))\n","    return\n","\n","    def on_batch_begin(self, batch, logs = dict()):\n","      return\n","\n","    def on_batch_end(self, batch, logs = dict()):\n","      return"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"UUqjc8BHws10","colab":{}},"source":["# confusion matrix as a dictionary\n","# Predict and evaluate performances\n","def get_confusion_matrix(model, test_features, test_label):\n","  predictions = model.predict(test_features).round()\n","  #predictions = [[round(x,1) for x in l] for l in predictions]\n","  #print(predictions[:10])\n","  #print(test_label.head)\n","  d = {k: {'t1': 0, 't0': 0, 'f1': 0, 'f0': 0} for k in range (1, 13)}\n","  for preds, trues in zip(predictions, test_label.itertuples()):\n","    for p, t, k in zip(preds, trues[1:], range(1, 13)):\n","      #p = 1 if p > 0.5 else 0\n","      if p == t and p == 0:\n","          d[k]['t0'] = d[k]['t0'] + 1\n","      if p == t and p == 1:\n","          d[k]['t1'] = d[k]['t1'] + 1\n","      if p != t and p == 0:\n","          d[k]['f0'] = d[k]['f0'] + 1\n","      if p != t and p == 1:\n","          d[k]['f1'] = d[k]['f1'] + 1\n","  return d\n","#tmp = [get_class(x) for x in y_val]\n","#print(classification_report(tmp, predictions))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"MX61sRCx0bpv","colab":{}},"source":["def print_confusion_matrix(confusion_matrix, label):\n","  predicted_header = ''.join([' '] * 12) + \"PREDICTED\" + ' '\n","  class_header = ''.join([' '] * 14) + \"CLASS\" + ''.join([' '] * 3)\n","  class_values = ''.join([' '] * 13) + \"1\" + ''.join([' '] * 5) + \"0\" + ''.join([' '] * 2)\n","  header = '\\n'.join([predicted_header, class_header, class_values])\n","  first_row = \"ACTUAL  1\" + ''.join([' '] * 2) + '{0:^5d}'.format(confusion_matrix[label]['t1']) + ' ' + '{0:5d}'.format(confusion_matrix[label]['f0'])\n","  second_row = \" CLASS  0\" + ''.join([' '] * 2) + '{0:^5d}'.format(confusion_matrix[label]['f1']) + ' ' + '{0:5d}'.format(confusion_matrix[label]['t0'])\n","  to_print = '\\n'.join([header, first_row, second_row]) + '\\n'\n","  print(to_print)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"-oz2d6tbT3W_","colab":{}},"source":["POS_WEIGHT = 1.0581504242078517\n","BATCH_SIZE = 128\n","DROPOUT_RATE = 0.2560668734997164\n","ACTIVATION = 'relu'\n","L2_REG = 3.787332973990032e-05\n","\n","X_t, X_v, Y_t, Y_v = train_test_split(X_train, Y_train, test_size=0.20)\n","\n","es = EarlyStopping(monitor = 'val_loss', mode = 'min', verbose = 1, \n","                    patience = 10, min_delta = 0.001, restore_best_weights = True)\n","\n","NN = Sequential()\n","NN.add(Dense(512, input_shape = (X_train.shape[1],), \n","              kernel_regularizer = regularizers.l2(L2_REG)))\n","NN.add(BatchNormalization())\n","NN.add(Activation(ACTIVATION))\n","NN.add(Dropout(rate = DROPOUT_RATE))\n","NN.add(Dense(256, kernel_regularizer = regularizers.l2(L2_REG)))\n","NN.add(BatchNormalization())\n","NN.add(Activation(ACTIVATION))\n","NN.add(Dropout(rate = DROPOUT_RATE))\n","NN.add(Dense(128, kernel_regularizer = regularizers.l2(L2_REG)))\n","NN.add(BatchNormalization())\n","NN.add(Activation(ACTIVATION))\n","NN.add(Dropout(rate = DROPOUT_RATE))\n","NN.add(Dense(12, activation = 'sigmoid'))\n","\n","# Compile model\n","NN.compile(optimizer = 'adam', \n","            loss = weighted_binary_crossentropy(POS_WEIGHT = POS_WEIGHT))\n","# Fit tne network\n","learning_process_NN = NN.fit(X_t, Y_t, \n","                              validation_data = (X_v, Y_v), \n","                              callbacks = [roc_callback(training_data = (X_t, Y_t), \n","                              validation_data = (X_v, Y_v)), es],\n","                              epochs = 300, batch_size = BATCH_SIZE, \n","                              verbose = True, use_multiprocessing = True)\n","plot_history(learning_process_NN)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"SupYFKeGws1o","colab":{}},"source":["print(evaluate_performance(NN, X_test, Y_test))\n","cm = get_confusion_matrix(NN, X_test, Y_test)\n","\n","f = open(\"./AML_project/dumps/confusion_matrix.pkl\",\"wb\")\n","pickle.dump(cm,f)\n","f.close()\n","\n","# calculate the fpr and tpr for all thresholds of the classification\n","test_predictions = NN.predict(X_v)\n","print(test_predictions)\n","test_pred_df = pd.DataFrame(data = test_predictions, columns = Y_v.columns)\n","fpr, tpr, threshold = roc_curve(Y_v.iloc[:, 0], test_pred_df.iloc[:, 0])\n","roc_auc = auc(fpr, tpr)\n","\n","plt.title('Receiver Operating Characteristic')\n","plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n","plt.legend(loc = 'lower right')\n","plt.plot([0, 1], [0, 1],'r--')\n","plt.xlim([0, 1])\n","plt.ylim([0, 1])\n","plt.ylabel('True Positive Rate')\n","plt.xlabel('False Positive Rate')\n","plt.savefig(\"./AML_project/images/pdf/ROC_1.pdf\")\n","plt.savefig(\"./AML_project/images/png/ROC_1.png\", dpi=300)"],"execution_count":0,"outputs":[]}]}