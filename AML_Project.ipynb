{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toxicity - Matteo Mistri and Daniele Papetti\n",
    "**Tox21 dataset analysis and prediction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset input and preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e7xdczejJ_6k"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "X_train = pd.read_csv(\"./dataset/tox21_dense_train.csv\")\n",
    "X_test = pd.read_csv(\"./dataset/tox21_dense_test.csv\")\n",
    "Y_train = pd.read_csv(\"./dataset/tox21_labels_train.csv\")\n",
    "Y_test = pd.read_csv(\"./dataset/tox21_labels_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decide to drop the names of the samples because we are sure that it is an irrelevant feature for the further operations.\n",
    "We substitute the NaN values in the labels with 0 value since we assume that if the test was not performed, the doctors would have thought that the molecule would have not been involved in that biological pathway. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s6V_MPfGNFqp"
   },
   "outputs": [],
   "source": [
    "# drop first column that contains names\n",
    "X_train = X_train.drop(X_train.columns[[0]], axis=1)\n",
    "X_test = X_test.drop(X_test.columns[[0]], axis=1)\n",
    "\n",
    "Y_train = Y_train.drop(Y_train.columns[[0]], axis=1)\n",
    "Y_test = Y_test.drop(Y_test.columns[[0]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ce0pqt0fNMP2"
   },
   "outputs": [],
   "source": [
    "# transform NaN in 0 in the labels\n",
    "Y_train = Y_train.fillna(0)\n",
    "Y_test = Y_test.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO DATA EXPLORATION\n",
    "# distribuzione etichette\n",
    "# correlazione tra le etichette\n",
    "# analisi missing values nelle feature (X_train.isnull().any().any())\n",
    "    # drop o imputazione dei sample con missing\n",
    "# ricerca e gestione outliers (3 * Q3/4-q1/4)\n",
    "# Cercare feature fortemente correlate a labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zIb5irM2SGA_"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{dtype('int64'), dtype('float64')}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if all features are numeric\n",
    "set(X_train.dtypes.append(X_test.dtypes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "905LrB1NNboE"
   },
   "outputs": [],
   "source": [
    "# normalize features in 0 mean and 1 std\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train.append(X_test).values)\n",
    "X_train = pd.DataFrame(scaler.transform(X_train.values), columns=X_train.columns)\n",
    "X_test = pd.DataFrame(scaler.transform(X_test.values), columns=X_test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E5nXCDuISkyF"
   },
   "source": [
    "**Use only one of the following method to reduce number of features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XdLtq27eMeun",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# correlation analysis and drop correlated features\n",
    "import numpy as np\n",
    "# Create correlation matrix\n",
    "corr_matrix = X_train.corr().abs()\n",
    "\n",
    "# Select upper triangle of correlation matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "\n",
    "# Find index of feature columns with correlation greater than 0.90\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.90)]\n",
    "print(len(to_drop))\n",
    "\n",
    "# Drop the features\n",
    "X_train.drop(X_train[to_drop], axis=1)\n",
    "X_test.drop(X_test[to_drop], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nsX5l7J3P-12"
   },
   "outputs": [],
   "source": [
    "# PCA features reduction\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "features_limit = 100\n",
    "columns = ['col' + str(x) for x in range(features_limit)]\n",
    "PCA_transformer = PCA(n_components = features_limit)\n",
    "PCA_transformer.fit(X_train.append(X_test).values)\n",
    "X_train = pd.DataFrame(PCA_transformer.transform(X_train.values), columns = columns)\n",
    "X_test = pd.DataFrame(PCA_transformer.transform(X_test.values), columns = columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vfe4cnNtTABp",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 801)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 400)               320800    \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 200)               80200     \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 200)               20200     \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 400)               80400     \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 801)               321201    \n",
      "=================================================================\n",
      "Total params: 842,901\n",
      "Trainable params: 842,901\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 11436 samples, validate on 1271 samples\n",
      "Epoch 1/300\n",
      "11436/11436 [==============================] - 4s 340us/step - loss: 0.6195 - accuracy: 0.0048 - val_loss: 0.6453 - val_accuracy: 0.0126\n",
      "Epoch 2/300\n",
      "11436/11436 [==============================] - 3s 277us/step - loss: 0.4155 - accuracy: 0.0445 - val_loss: 0.5798 - val_accuracy: 0.0488\n",
      "Epoch 3/300\n",
      "11436/11436 [==============================] - 3s 247us/step - loss: 0.3561 - accuracy: 0.0899 - val_loss: 0.4566 - val_accuracy: 0.0724\n",
      "Epoch 4/300\n",
      "11436/11436 [==============================] - 3s 248us/step - loss: 0.3121 - accuracy: 0.1236 - val_loss: 0.4131 - val_accuracy: 0.0850\n",
      "Epoch 5/300\n",
      "11436/11436 [==============================] - 3s 251us/step - loss: 0.2827 - accuracy: 0.1453 - val_loss: 0.4204 - val_accuracy: 0.1054\n",
      "Epoch 6/300\n",
      "11436/11436 [==============================] - 3s 250us/step - loss: 0.2720 - accuracy: 0.1625 - val_loss: 0.3885 - val_accuracy: 0.1322\n",
      "Epoch 7/300\n",
      "11436/11436 [==============================] - 3s 247us/step - loss: 0.2730 - accuracy: 0.1691 - val_loss: 0.3782 - val_accuracy: 0.1204\n",
      "Epoch 8/300\n",
      "11436/11436 [==============================] - 3s 247us/step - loss: 0.2546 - accuracy: 0.1828 - val_loss: 0.3612 - val_accuracy: 0.1330\n",
      "Epoch 9/300\n",
      "11436/11436 [==============================] - 3s 253us/step - loss: 0.2301 - accuracy: 0.2011 - val_loss: 0.3520 - val_accuracy: 0.1613\n",
      "Epoch 10/300\n",
      "11436/11436 [==============================] - 3s 255us/step - loss: 0.2748 - accuracy: 0.2041 - val_loss: 0.3455 - val_accuracy: 0.1566\n",
      "Epoch 11/300\n",
      "11436/11436 [==============================] - 3s 251us/step - loss: 0.2382 - accuracy: 0.2117 - val_loss: 0.3658 - val_accuracy: 0.1841\n",
      "Epoch 12/300\n",
      "11436/11436 [==============================] - 3s 250us/step - loss: 0.2234 - accuracy: 0.2329 - val_loss: 0.3209 - val_accuracy: 0.1747\n",
      "Epoch 13/300\n",
      "11436/11436 [==============================] - 3s 249us/step - loss: 0.2053 - accuracy: 0.2378 - val_loss: 0.4210 - val_accuracy: 0.1794\n",
      "Epoch 14/300\n",
      "11436/11436 [==============================] - 3s 251us/step - loss: 0.2224 - accuracy: 0.2516 - val_loss: 0.3432 - val_accuracy: 0.2006\n",
      "Epoch 15/300\n",
      "11436/11436 [==============================] - 3s 250us/step - loss: 0.2014 - accuracy: 0.2681 - val_loss: 0.3763 - val_accuracy: 0.1943\n",
      "Epoch 16/300\n",
      "11436/11436 [==============================] - 3s 252us/step - loss: 0.1993 - accuracy: 0.2688 - val_loss: 0.3272 - val_accuracy: 0.2179\n",
      "Epoch 17/300\n",
      "11436/11436 [==============================] - 3s 256us/step - loss: 0.2021 - accuracy: 0.2823 - val_loss: 0.3297 - val_accuracy: 0.2172\n",
      "Epoch 18/300\n",
      "11436/11436 [==============================] - 3s 252us/step - loss: 0.1870 - accuracy: 0.2913 - val_loss: 0.3220 - val_accuracy: 0.2116\n",
      "Epoch 19/300\n",
      "11436/11436 [==============================] - 3s 267us/step - loss: 0.1774 - accuracy: 0.2983 - val_loss: 0.3500 - val_accuracy: 0.2400\n",
      "Epoch 20/300\n",
      "11436/11436 [==============================] - 3s 268us/step - loss: 0.1798 - accuracy: 0.3043 - val_loss: 0.3368 - val_accuracy: 0.2297\n",
      "Epoch 21/300\n",
      "11436/11436 [==============================] - 3s 263us/step - loss: 0.1984 - accuracy: 0.3127 - val_loss: 0.2964 - val_accuracy: 0.2463\n",
      "Epoch 22/300\n",
      "11436/11436 [==============================] - 3s 253us/step - loss: 0.1742 - accuracy: 0.3250 - val_loss: 0.3048 - val_accuracy: 0.2447\n",
      "Epoch 23/300\n",
      "11436/11436 [==============================] - 3s 260us/step - loss: 0.1712 - accuracy: 0.3222 - val_loss: 0.2929 - val_accuracy: 0.2486\n",
      "Epoch 24/300\n",
      "11436/11436 [==============================] - 3s 260us/step - loss: 0.1826 - accuracy: 0.3361 - val_loss: 0.3011 - val_accuracy: 0.2596\n",
      "Epoch 25/300\n",
      "11436/11436 [==============================] - 3s 260us/step - loss: 0.1703 - accuracy: 0.3492 - val_loss: 0.3112 - val_accuracy: 0.2533\n",
      "Epoch 26/300\n",
      "11436/11436 [==============================] - 3s 258us/step - loss: 0.1586 - accuracy: 0.3485 - val_loss: 0.3172 - val_accuracy: 0.2777\n",
      "Epoch 27/300\n",
      "11436/11436 [==============================] - 3s 254us/step - loss: 0.1808 - accuracy: 0.3517 - val_loss: 0.3388 - val_accuracy: 0.2675\n",
      "Epoch 28/300\n",
      "11436/11436 [==============================] - 3s 257us/step - loss: 0.1775 - accuracy: 0.3557 - val_loss: 0.3087 - val_accuracy: 0.2785\n",
      "Epoch 29/300\n",
      "11436/11436 [==============================] - 3s 249us/step - loss: 0.1632 - accuracy: 0.3590 - val_loss: 0.2901 - val_accuracy: 0.2840\n",
      "Epoch 30/300\n",
      "11436/11436 [==============================] - 3s 268us/step - loss: 0.1592 - accuracy: 0.3662 - val_loss: 0.2764 - val_accuracy: 0.2935\n",
      "Epoch 31/300\n",
      "11436/11436 [==============================] - 3s 276us/step - loss: 0.1488 - accuracy: 0.3652 - val_loss: 0.3009 - val_accuracy: 0.2950\n",
      "Epoch 32/300\n",
      "11436/11436 [==============================] - 3s 262us/step - loss: 0.1762 - accuracy: 0.3701 - val_loss: 0.2910 - val_accuracy: 0.3139\n",
      "Epoch 33/300\n",
      "11436/11436 [==============================] - 3s 263us/step - loss: 0.1516 - accuracy: 0.3778 - val_loss: 0.3150 - val_accuracy: 0.3092\n",
      "Epoch 34/300\n",
      "11436/11436 [==============================] - 3s 255us/step - loss: 0.1475 - accuracy: 0.3779 - val_loss: 0.2799 - val_accuracy: 0.3021\n",
      "Epoch 35/300\n",
      "11436/11436 [==============================] - 3s 275us/step - loss: 0.2206 - accuracy: 0.3633 - val_loss: 0.2669 - val_accuracy: 0.3013\n",
      "Epoch 36/300\n",
      "11436/11436 [==============================] - 3s 277us/step - loss: 0.1561 - accuracy: 0.3812 - val_loss: 0.2514 - val_accuracy: 0.3116\n",
      "Epoch 37/300\n",
      "11436/11436 [==============================] - 3s 269us/step - loss: 0.1579 - accuracy: 0.3847 - val_loss: 0.2448 - val_accuracy: 0.3092\n",
      "Epoch 38/300\n",
      "10496/11436 [==========================>...] - ETA: 0s - loss: 0.1530 - accuracy: 0.3916"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-9772793eb83a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mautoencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# fit the autoencoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mautoencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_multiprocessing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;31m# extract representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3738\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3739\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3740\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3742\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1079\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m     \"\"\"\n\u001b[0;32m-> 1081\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1082\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1119\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m   1120\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m-> 1121\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "import keras.optimizers\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "# AUTOENCODER\n",
    "# Define early stopping\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=15, min_delta=0.001 ,restore_best_weights=True)\n",
    "\n",
    "encoding_dim1 = int(X_train.shape[1] / 2)\n",
    "encoding_dim2 = int(encoding_dim1 / 2)\n",
    "encoding_dim3 = 100\n",
    "columns = ['col' + str(x) for x in range(encoding_dim3)]\n",
    "\n",
    "input_layer = Input(shape=(X_train.shape[1],))\n",
    "# \"encoded\" is the encoded representation of the input\n",
    "encoded1 = Dense(encoding_dim1, activation='relu')(input_layer)\n",
    "encoded2 = Dense(encoding_dim2, activation='relu')(encoded1)\n",
    "bottleneck = Dense(encoding_dim3, activation='relu')(encoded2)\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "decoded2 = Dense(encoding_dim2, activation='relu')(bottleneck)\n",
    "decoded1 = Dense(encoding_dim1, activation='relu')(decoded2)\n",
    "output_layer = Dense(X_train.shape[1], activation='linear')(decoded1)\n",
    "# this model maps an input to its reconstruction\n",
    "autoencoder = Model(input_layer, output_layer)\n",
    "# this model maps an input to its encoded representation\n",
    "encoder = Model(input_layer, bottleneck)\n",
    "\n",
    "# compile the model\n",
    "autoencoder.compile(optimizer='adam', loss='mse', metrics = ['accuracy'])\n",
    "autoencoder.summary()\n",
    "# fit the autoencoder\n",
    "autoencoder.fit(X_train.append(X_test).values, X_train.append(X_test).values, validation_split = 0.1, epochs=300, batch_size=256, verbose=True, callbacks = [es], use_multiprocessing = True)\n",
    "# extract representation\n",
    "X_train = pd.DataFrame(encoder.predict(X_train.values), columns = columns)\n",
    "X_test = pd.DataFrame(encoder.predict(X_test.values), columns = columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# controllare funzione di attivazione autoencoder e relativa loss\n",
    "# data visualization in 2/3D per vedere se clusterizza (a occhio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GGQsJNk_Mw4E"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             AW   AWeight      Arto   BertzCT      Chi0      Chi1     Chi10  \\\n",
      "0      4.261302 -0.308802  0.836605  1.490057  1.142090  1.303726  1.189859   \n",
      "1      0.717300  1.605286  0.272748  1.359206  0.883107  0.896950  1.727203   \n",
      "2     -0.099952 -0.302049  0.724850  1.519900  4.104884  3.969533  4.004521   \n",
      "3      5.733907 -0.355441  0.089875  1.917044  4.626837  4.364390  1.567584   \n",
      "4      0.317784 -0.333493  0.572456  2.360100  7.019719  7.080712  3.758391   \n",
      "5     -0.361586 -0.289809 -0.057439  0.142524  0.390553  0.390465 -0.284488   \n",
      "6     -0.361586  3.033148  0.790887  0.036925 -0.651358 -0.620422 -0.587887   \n",
      "7     -0.361586  0.582188 -0.692413 -0.555347 -0.173379 -0.260756 -0.632970   \n",
      "8      0.224833 -0.299939 -0.057439  0.723317  0.778343  0.897896  0.573314   \n",
      "9      0.521380 -0.129844  0.211790  1.189330  1.465600  1.576172  1.015618   \n",
      "10     0.521380 -0.157490 -0.860047  0.477685  1.628913  1.584686 -0.099281   \n",
      "11     0.205286 -0.139341  0.618174  0.822029  0.801762  0.948601  0.651296   \n",
      "12    -0.361586 -0.081306  0.364184  0.727909  0.520356  0.378357  0.120043   \n",
      "13    -0.361586 -0.394693  0.404823  0.610832  0.229483  0.333706  0.068868   \n",
      "14    -0.361586  0.778239 -0.057439 -0.126065 -0.598664 -0.630071 -0.632970   \n",
      "15    -0.361586 -0.282423  1.182032  1.125052  1.962514  1.937920  3.210080   \n",
      "16     0.533475  0.247486 -1.129276 -0.761954 -0.031368 -0.078747 -0.414864   \n",
      "17     0.318661  0.094908 -0.463823  0.117272  0.444493  0.503606 -0.166297   \n",
      "18     0.377813  0.112635 -0.499381 -0.073265  0.221137  0.369843 -0.200414   \n",
      "19     0.224833 -0.314500  0.293067  0.923037  0.786689  0.829784  0.647640   \n",
      "20    -0.361586 -0.372113  0.887403  1.473988  2.083971  2.205446  1.916066   \n",
      "21     1.671335  0.299190  0.282908  1.069957  1.015526  0.998928  0.350334   \n",
      "22    -0.361586 -0.444498  1.568096  0.824325  0.436645  0.555257  2.131735   \n",
      "23    -0.361586 -0.135964  0.765488  1.063070  1.621065  1.576172  1.367755   \n",
      "24    -0.361586 -0.377177  1.974480  1.239834  0.323783  0.627342  2.838447   \n",
      "25     2.621952 -0.351431 -0.057439  0.213688  0.038267  0.022474 -0.392932   \n",
      "26    -0.361586 -0.348266  0.521658  1.203104  1.443925  1.465302  0.704908   \n",
      "27     0.354215 -0.174795  0.374344  1.494648  2.239809  2.421889  1.740607   \n",
      "28     0.377812 -0.111273  0.826446  0.578693  0.124221  0.346761  1.044861   \n",
      "29     0.268272  0.044470  0.318466  0.824325  0.643931  0.588367  0.603775   \n",
      "...         ...       ...       ...       ...       ...       ...       ...   \n",
      "12030  0.533475 -0.113173  0.475940  0.222871 -0.175497 -0.035042  0.407602   \n",
      "12031 -0.361586 -0.484383  1.547777  0.973541 -0.119689  0.055584  0.826755   \n",
      "12032  0.411421  0.228282  0.404823  0.766934  0.115625  0.213186  0.112733   \n",
      "12033 -0.361586 -0.170785  1.298867  0.608536 -0.460017 -0.305974  0.085926   \n",
      "12034 -0.361586 -0.201385  0.506419  0.477685 -0.061888 -0.106559 -0.562299   \n",
      "12035  0.533475  0.291804 -0.057439  0.101202 -0.122803 -0.041475 -0.083441   \n",
      "12036  1.764184  0.300245 -1.327388 -1.000699 -1.100062 -1.049146 -0.632970   \n",
      "12037 -0.361586 -0.229453  0.790887  0.842690  0.379840  0.493957  0.353989   \n",
      "12038 -0.361586  0.018513  0.755329  1.067661  0.522350  0.549203  0.421005   \n",
      "12039 -0.361586 -0.184291 -0.057439 -0.546165 -0.666431 -0.745482 -0.632970   \n",
      "12040 -0.361586 -0.368314  0.425142  0.587875  0.141412  0.221511 -0.189448   \n",
      "12041  5.070510 -0.369158  0.465780  1.540561  3.518778  3.547242  1.002215   \n",
      "12042 -0.361586 -0.438378  1.395383  1.136530  0.620387  0.911140  1.595609   \n",
      "12043  0.752611  0.524786 -0.057439  0.975836  0.746079  0.860434  0.479492   \n",
      "12044 -0.361586 -0.076031 -0.057439  0.307809 -0.416043 -0.457333 -0.586668   \n",
      "12045  0.347004  0.191140  0.790887  0.810551  0.273208  0.401060  1.615104   \n",
      "12046 -0.361586  0.281674 -0.057439  0.268783 -0.192688 -0.323570 -0.601290   \n",
      "12047  0.318660  0.162228  0.348945  0.473094  0.420575  0.470307  0.212647   \n",
      "12048  0.245777 -0.231774  0.303227  0.847281  0.617273  0.814081  0.453904   \n",
      "12049 -0.361586  0.264369 -0.057439  0.032334 -0.307542 -0.389032 -0.597635   \n",
      "12050 -0.361586 -0.349321  1.070276  1.131939  0.605314  0.782674  1.392125   \n",
      "12051 -0.361586 -0.272293  0.303227  0.844986  0.818953  0.871219  0.355208   \n",
      "12052  0.772158  0.271122 -0.733052 -0.109995 -0.422521 -0.444089 -0.575702   \n",
      "12053 -0.361586 -0.269761 -0.057439 -0.093926 -0.510593 -0.538688 -0.632970   \n",
      "12054 -0.361586 -0.429936  0.577536  1.007975  1.098365  1.259643  1.083852   \n",
      "12055  0.853140 -0.055138 -0.057439  0.190732 -0.583591 -0.504822 -0.441671   \n",
      "12056  0.652986 -0.177749  0.252429  0.861055  0.896685  1.211776  0.830411   \n",
      "12057 -0.361586 -0.452095  0.171152  1.338546  2.403993  2.171958  0.349115   \n",
      "12058 -0.361586 -0.416430  1.298867  0.356017 -0.460017 -0.302001  0.082271   \n",
      "12059  1.280388  0.102927 -0.733052  1.010271  0.778094  0.689021  0.266260   \n",
      "\n",
      "           Chi2      Chi3     Chi3c  ...        W3D       W3DH      WNSA1  \\\n",
      "0      1.358616  1.447572  0.771777  ...   0.466100   0.320456   0.410718   \n",
      "1      0.979827  1.300525  0.683727  ...   0.304809  -0.157816   1.506111   \n",
      "2      4.283102  4.424530  4.685133  ...   4.029958   5.682312   2.103210   \n",
      "3      4.084921  3.514719  2.730713  ...   4.031183   3.605409   1.941845   \n",
      "4      6.631399  6.030577  4.070285  ...  13.464632  12.615454  10.561530   \n",
      "5      0.192961  0.072991 -0.290850  ...   0.113788   0.477174  -0.056373   \n",
      "6     -0.546849 -0.414260 -0.463940  ...  -0.322864  -0.375441  -0.345040   \n",
      "7     -0.484954 -0.343604 -0.593382  ...  -0.225223  -0.164154  -0.161000   \n",
      "8      0.498140  0.590294 -0.496301  ...   0.224725   0.581748  -0.305293   \n",
      "9      1.495488  1.431514  0.867353  ...   0.891463   0.985699   1.110263   \n",
      "10     1.056171  0.732524 -0.022935  ...   1.441291   2.689183   2.096737   \n",
      "11     0.925547  1.027306  0.342813  ...   0.393832   0.277025   1.319339   \n",
      "12     0.721704  0.317993  1.697436  ...   0.049252   0.098229   0.769923   \n",
      "13     0.110370  0.180810 -0.475229  ...  -0.033350   0.010541  -0.122122   \n",
      "14    -0.629441 -0.552590 -0.524898  ...  -0.326470  -0.376854   0.147983   \n",
      "15     2.204253  2.743238  2.412376  ...  -0.218917  -0.237269  -0.215909   \n",
      "16    -0.189147 -0.573924 -0.024440  ...  -0.048847   0.402739  -0.170841   \n",
      "17     0.258371  0.026652 -0.137326  ...   0.143523   0.599473  -0.036720   \n",
      "18    -0.054618 -0.135536 -1.045676  ...   0.173474   0.612642  -0.101600   \n",
      "19     0.813277  0.832543  0.389472  ...   0.335256   0.474964   0.699427   \n",
      "20     2.142944  2.058700  1.183432  ...   1.282702   1.040458   1.708479   \n",
      "21     0.948001  0.617364  0.531708  ...   0.426141   0.033024   2.742802   \n",
      "22     0.739667  1.249138  0.981744  ...   0.049637   0.083888  -0.066048   \n",
      "23     1.894974  1.530845  2.425170  ...   0.840266   0.413207   2.952568   \n",
      "24     0.677382  1.082592  0.075651  ...   0.082454  -0.092867   0.299217   \n",
      "25    -0.135453 -0.115807 -0.431580  ...  -0.166911  -0.085881  -0.274543   \n",
      "26     1.332257  1.303507  0.634057  ...   0.594081   0.495631   1.278211   \n",
      "27     2.173599  2.285121  0.876384  ...   1.051310   0.940117   0.565789   \n",
      "28     0.297031  0.461140 -0.270530  ...  -0.022581   0.032442  -0.175188   \n",
      "29     0.882201  0.666915  1.631963  ...   0.110285  -0.064577   1.182582   \n",
      "...         ...       ...       ...  ...        ...        ...        ...   \n",
      "12030 -0.086249  0.086297 -0.424054  ...  -0.177327  -0.146824  -0.323621   \n",
      "12031  0.067805  0.403331 -0.290850  ...  -0.172352  -0.216464  -0.366736   \n",
      "12032  0.217563  0.373509  0.065868  ...  -0.073148  -0.113466   0.313764   \n",
      "12033 -0.321528 -0.095390 -0.588867  ...  -0.275567  -0.333787  -0.148124   \n",
      "12034  0.005324 -0.096078  0.088445  ...  -0.204276  -0.304962   0.323328   \n",
      "12035 -0.180751 -0.055474 -0.542960  ...  -0.152063  -0.025685  -0.493542   \n",
      "12036 -1.095312 -0.970332 -1.045676  ...  -0.371596  -0.387317  -0.742890   \n",
      "12037  0.462019  0.463893  0.322494  ...   0.045322   0.054763   0.164627   \n",
      "12038  0.599671  0.580430  0.339050  ...   0.024049  -0.180695   1.206872   \n",
      "12039 -0.636079 -0.666832 -0.348045  ...  -0.337857  -0.363748  -0.414240   \n",
      "12040  0.113689  0.120936 -0.320200  ...  -0.059835  -0.131860   0.294980   \n",
      "12041  3.232451  3.204796  1.722271  ...   3.146452   3.240185   3.223708   \n",
      "12042  0.802343  1.043594 -0.072605  ...   0.306162   0.211863   0.425292   \n",
      "12043  0.753335  0.732983  0.117795  ...  -0.218917  -0.237269  -0.215909   \n",
      "12044 -0.413882 -0.455552 -0.083141  ...  -0.277658  -0.327417   0.044666   \n",
      "12045  0.426873  0.668750  0.088445  ...  -0.008030  -0.017134   0.410883   \n",
      "12046 -0.091131 -0.353009  0.904982  ...  -0.252874  -0.298767   0.161080   \n",
      "12047  0.455771  0.706601  0.237454  ...   0.056544   0.187436   0.365229   \n",
      "12048  0.536214  0.672191 -0.333746  ...   0.193532   0.221007   0.662329   \n",
      "12049 -0.237374 -0.494551  0.443657  ...  -0.267733  -0.294137   0.013876   \n",
      "12050  0.727757  0.863742  0.101239  ...   0.244577  -0.055953   1.322264   \n",
      "12051  0.544806  0.640763 -0.221613  ...   0.205766   0.325847   0.048931   \n",
      "12052 -0.477339 -0.418848 -0.473724  ...  -0.273069  -0.294774   0.004022   \n",
      "12053 -0.530057 -0.569336 -0.435343  ...  -0.306854  -0.335690  -0.342170   \n",
      "12054  0.936091  0.993813 -0.120769  ...   0.462052   0.695089   0.617626   \n",
      "12055 -0.527324 -0.412425 -0.628753  ...  -0.297293  -0.301024  -0.126870   \n",
      "12056  0.887278  0.985096 -0.384921  ...   0.415180   0.563509   0.127888   \n",
      "12057  2.253652  1.953863  2.362707  ...   2.544846   3.093594   2.508912   \n",
      "12058 -0.334610 -0.097455 -0.589619  ...  -0.269905  -0.270781  -0.635323   \n",
      "12059  0.800781  0.596488  1.157845  ...   0.332650  -0.057659   0.977785   \n",
      "\n",
      "           WNSA2     WNSA3     WPSA1      WPSA2     WPSA3      grav      rygr  \n",
      "0      -0.194471 -0.271099  0.642228   0.341363  0.753540 -0.013710  0.499887  \n",
      "1      -0.255191 -0.110357  0.307454   0.007155  1.446990 -0.003007  0.927168  \n",
      "2      -2.026731 -2.122607  4.182235   4.641351  2.999775  0.015786  1.880723  \n",
      "3      -1.189530 -0.881462  3.537400   2.582734  2.708759  0.020694  1.441320  \n",
      "4     -10.954174 -9.371987  9.872230  13.733265  9.717868  0.038945  4.678761  \n",
      "5       0.052250  0.260562  1.268433   0.301165  0.762454 -0.023632  1.933250  \n",
      "6       0.235715  0.293528 -0.674644  -0.344289 -0.669138 -0.024495 -0.802161  \n",
      "7       0.143801  0.066603  0.017827  -0.161251 -0.105765 -0.024455  0.032200  \n",
      "8      -0.038251  0.360712  1.021682   0.703721  0.641114  5.049660  0.457461  \n",
      "9      -0.421412 -0.358592  1.297947   0.628453  0.938705 -0.008522  1.247377  \n",
      "10     -1.210671 -2.087749  3.789872   2.637877  3.163438 -0.014332  3.307016  \n",
      "11     -0.269041 -0.354558  0.615607   0.139950  0.520869 -0.017288  1.250407  \n",
      "12     -0.061971  0.144276  0.296356  -0.038171  0.069656 -0.021515  0.991816  \n",
      "13      0.087746  0.020172  0.532046   0.045262  0.442592 -0.023838  0.602919  \n",
      "14      0.214499  0.472325 -0.855479  -0.364233 -0.767407 -0.027533 -0.552661  \n",
      "15      0.153553  0.224758 -0.237918  -0.208700 -0.211394 -0.025588 -0.071843  \n",
      "16      0.063247 -0.763535  0.969904   0.253256  0.501500 -0.025619  2.520131  \n",
      "17      0.003943 -0.721750  1.231449   0.412628  0.724337 -0.020450  1.401926  \n",
      "18      0.058678  0.158102  1.379826   0.343143  0.712668 -0.022603  2.565586  \n",
      "19     -0.101176 -0.208255  1.400616   0.324515  0.849943 -0.019317  1.327177  \n",
      "20     -0.719427 -1.002199  1.664816   0.926272  1.365277 -0.007524  1.268589  \n",
      "21     -0.271432 -0.530044  0.094108  -0.149848  0.098917 -0.015080  1.542333  \n",
      "22      0.067231  0.331251  0.297482   0.001101  0.275227 -0.021272  0.393824  \n",
      "23     -0.728265 -0.889947  0.163665   0.042773  0.039831 -0.010652  1.385764  \n",
      "24      0.110388  0.138823  0.047653  -0.191434 -0.147969 -0.021159  0.818075  \n",
      "25      0.110714 -0.023727  0.129789  -0.046798  0.059793 -0.025106 -0.018306  \n",
      "26     -0.407653 -0.784761  0.983651   0.421091  0.728128 -0.013586  0.758478  \n",
      "27     -0.843043 -0.403103  0.306286   0.866785  0.399381  0.799036 -0.064772  \n",
      "28      0.126854  0.495332  0.470399  -0.032267  0.059408 -0.020689  0.276650  \n",
      "29     -0.108186 -0.094472 -0.239888  -0.183288 -0.286310 -0.016637  0.257457  \n",
      "...          ...       ...       ...        ...       ...       ...       ...  \n",
      "12030   0.163803  0.338262 -0.022663  -0.162725 -0.088588 -0.023526 -0.160733  \n",
      "12031   0.194946  0.427424 -0.171457  -0.231643 -0.272597 -0.025356 -0.027397  \n",
      "12032  -0.009566  0.000810 -0.039268  -0.090042 -0.174832 -0.019819  0.131192  \n",
      "12033   0.226944  0.564380 -0.620713  -0.342447 -0.635375 -0.027186 -0.337505  \n",
      "12034   0.062460 -0.331579 -0.620040  -0.281033 -0.448078 -0.024843 -0.192047  \n",
      "12035   0.138351  0.129364  0.451017   0.089772  0.526645 -0.022058  0.071595  \n",
      "12036   0.272935  0.710127 -0.661093  -0.357648 -0.706870 -0.030502 -1.139542  \n",
      "12037  -0.063034 -0.741724  0.596109   0.225795  0.416914 -0.021287  0.739285  \n",
      "12038  -0.076133 -0.411199 -0.309097  -0.215735 -0.089772 -0.019526  0.252407  \n",
      "12039   0.191327  0.098596 -0.739760  -0.316565 -0.494102 -0.029401 -0.705189  \n",
      "12040   0.071304 -0.103681  0.006778  -0.157249  0.028192 -0.024521  0.738275  \n",
      "12041  -1.880514 -2.217723  2.531239   2.035246  2.328064  0.003821  1.883754  \n",
      "12042  -0.043269 -0.100899  0.917606   0.188147  0.780017 -0.020313  1.425158  \n",
      "12043   0.153553  0.224758 -0.237918  -0.208700 -0.211394 -0.025588 -0.071843  \n",
      "12044   0.107549 -0.294913 -0.554826  -0.269224 -0.553899 -0.027361 -0.212249  \n",
      "12045   0.001756  0.006346  0.159462  -0.069851  0.197364 -0.019803  0.262508  \n",
      "12046   0.010747 -0.499860 -0.569455  -0.227666 -0.373532 -0.024749 -0.224371  \n",
      "12047  -0.073142 -0.934431  0.392821   0.097188  0.224315 -0.018960  0.503927  \n",
      "12048  -0.032982 -0.056304  0.796093   0.070290  0.244010 -0.018782  0.632213  \n",
      "12049   0.158407  0.216023 -0.477866  -0.287560 -0.345218 -0.025948 -0.161743  \n",
      "12050  -0.072442 -0.450369  0.066803  -0.151779  0.002099 -0.020862  1.395865  \n",
      "12051  -0.008337  0.129058  1.095589   0.344136  0.640759 -0.018637  0.693830  \n",
      "12052   0.056965  0.156600 -0.553639  -0.231887 -0.076267  0.603052 -0.219320  \n",
      "12053   0.196262  0.187813 -0.598834  -0.303944 -0.487053 -0.028439 -0.447608  \n",
      "12054  -0.168323 -0.011904  1.499663   0.517451  0.932870 -0.017348  0.984745  \n",
      "12055   0.191463  0.458499 -0.516882  -0.306926 -0.437564 -0.027232 -0.638521  \n",
      "12056  -0.024196  0.221531  0.850569   0.256832  0.456008 -0.012452  0.457461  \n",
      "12057  -1.120896 -2.053114  4.218907   2.359950  3.265113 -0.012903  4.889877  \n",
      "12058   0.236669  0.563935 -0.235807  -0.262897 -0.301207 -0.027449 -0.261745  \n",
      "12059  -0.211091 -1.056670  0.341685   0.074682  2.090625 -0.013806  0.847369  \n",
      "\n",
      "[12060 rows x 801 columns]\n",
      "           AW   AWeight      Arto   BertzCT      Chi0      Chi1     Chi10  \\\n",
      "0    1.859829 -0.385830  1.090595  1.563517  2.913115  3.114734  3.830280   \n",
      "1    0.347004  0.629671  0.790887  0.867942  0.300115  0.358491  0.820663   \n",
      "2   -0.361586 -0.433102  1.791607  1.143417  0.091832  0.349788  1.985519   \n",
      "3    0.318660 -0.145461  0.348945  0.688883  0.353058  0.518931  0.313780   \n",
      "4   -0.361586 -0.001535  0.384504  0.769230  0.364767  0.367572 -0.129743   \n",
      "5    0.583201 -0.207083  0.506419  0.633788 -0.283873 -0.123208  0.194370   \n",
      "6   -0.361586  0.048691  1.014398  0.583284 -0.040213 -0.009500  0.246764   \n",
      "7   -0.361586  0.079291  0.577536  0.275670 -0.298947 -0.244296 -0.235749   \n",
      "8   -0.361586 -0.412631  0.643573  0.874829  0.813721  0.979062  0.406383   \n",
      "9   -0.361586 -0.156013  0.541977  0.316991 -0.190570 -0.156318 -0.216254   \n",
      "10  -0.361586 -0.410732  1.161712  0.911559  0.408741  0.610503  1.085071   \n",
      "11  -0.361586  0.298346  0.668972  0.434068 -0.454785 -0.444089 -0.514779   \n",
      "12   0.053199  0.357013  0.684212  1.487761  1.779021  1.945866  1.953839   \n",
      "13  -0.361586 -0.169941  0.867084  0.851873  0.197219  0.327273  0.534323   \n",
      "14  -0.361586 -0.388362  1.395383  0.867942  0.056454  0.248756  0.693942   \n",
      "15  -0.361586 -0.393427 -0.057439  0.489163  0.878124  0.834703 -0.173608   \n",
      "16  -0.361586 -0.031924  1.161712  1.136530  0.469781  0.566042  0.774361   \n",
      "17   0.224834 -0.227554 -0.758451  0.156297  0.910512  0.806891 -0.235749   \n",
      "18   0.347004 -0.199908  0.790887  0.923037  0.212293  0.444387  0.954694   \n",
      "19  -0.361586 -0.375700  0.633414  1.187034  2.311686  2.223042  2.416857   \n",
      "20  -0.361586 -0.309224  0.384504  1.278859  2.574655  2.367779  2.543577   \n",
      "21  -0.361586 -0.413898  1.329346  0.530485  0.191738  0.296055  1.396998   \n",
      "22  -0.361586 -0.463913  0.724850  0.902376  0.643931  0.590637  0.402728   \n",
      "23  -0.361586 -0.455471  0.384504  0.530485  0.317555  0.421116  0.276007   \n",
      "24   0.853140  1.556536  0.668972  0.183845 -0.656590 -0.478334 -0.110247   \n",
      "25   2.385564  0.067262 -0.057439  0.936811  0.742591  0.566042 -0.246716   \n",
      "26  -0.361586 -0.366837  1.507139  1.196217  0.511761  0.671236  2.465595   \n",
      "27   0.448231 -0.112751 -0.057439  0.583284  0.100552  0.070342 -0.094407   \n",
      "28   0.583201 -0.066745  0.506419  0.284853 -0.277396 -0.120749  0.004289   \n",
      "29   0.853140  0.035185 -1.510261 -0.624216 -0.457899 -0.556284 -0.632970   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "617 -0.361586  0.059243  0.303227  1.106687  0.872643  0.784566 -0.074912   \n",
      "618 -0.361586 -0.125835 -0.057439  0.578693  0.369999  0.232863 -0.425831   \n",
      "619 -0.361586 -0.106631 -0.057439  0.530485  0.261623  0.155102 -0.429486   \n",
      "620 -0.361586 -0.332227 -0.057439 -0.183455 -0.354754 -0.329435 -0.477006   \n",
      "621 -0.361586  0.071272  0.958521  0.532780  0.034032  0.103451 -0.229657   \n",
      "622 -0.361586 -0.348266  1.029638  0.787595  0.713690  0.864408  1.562710   \n",
      "623 -0.361586 -0.265751 -0.057439 -0.429088 -0.991685 -0.978575 -0.632970   \n",
      "624 -0.361586 -0.196320 -0.057439  0.356017  0.810731  0.890706  0.113951   \n",
      "625 -0.361586 -0.463913  0.724850  0.112680 -0.603896 -0.495173 -0.406335   \n",
      "626 -0.361586 -0.388362  0.790887  0.080542 -0.671663 -0.606611 -0.489191   \n",
      "627 -0.361586 -0.375700 -0.057439 -0.275281 -0.707041 -0.700642 -0.632970   \n",
      "628 -0.361586 -0.458637 -0.057439 -0.497956 -0.639274 -0.596205 -0.567173   \n",
      "629 -0.361586  0.005007  0.668972  0.461616 -0.454785 -0.451279 -0.433141   \n",
      "630 -0.361586 -0.444498  0.958521 -0.325784 -0.868111 -0.778970 -0.615912   \n",
      "631 -0.361586 -0.341724 -0.057439 -0.580599 -0.903614 -0.880002 -0.632970   \n",
      "632 -0.361586  1.367449 -0.057439 -0.752771 -0.991685 -0.978575 -0.632970   \n",
      "633 -0.361586 -0.212148  0.790887  0.957471  0.379840  0.490741  0.228487   \n",
      "634 -0.361586 -0.348266 -0.057439 -0.929534 -1.100062 -1.056335 -0.632970   \n",
      "635 -0.361586 -0.370846 -0.057439 -0.865257 -0.991685 -0.981791 -0.632970   \n",
      "636 -0.361586 -0.381398  0.450541  0.613127  0.073645  0.106857  0.001852   \n",
      "637 -0.361586 -0.470666  0.994079  1.083731  0.808364  0.943492  0.771924   \n",
      "638 -0.361586 -0.476153  1.212510  0.874829  0.999581  1.266454  1.588298   \n",
      "639 -0.361586 -0.388362  0.790887  0.080542 -0.671663 -0.603394 -0.483099   \n",
      "640 -0.361586 -0.168042 -1.510261 -2.217381 -1.027064 -1.072606 -0.632970   \n",
      "641 -0.361586 -0.402502 -0.057439 -0.699972 -0.835847 -0.765348 -0.607382   \n",
      "642 -0.361586  0.486167 -2.089358 -3.186136 -1.223636 -1.241750 -0.632970   \n",
      "643 -0.361586 -0.276092 -0.057439 -0.943308 -0.354754 -0.318083 -0.545240   \n",
      "644 -0.361586  0.316706 -0.057439 -0.573712 -1.079757 -1.073174 -0.632970   \n",
      "645 -0.361586  0.244321  0.907723  0.702657  0.142533  0.181212 -0.187011   \n",
      "646 -0.361586  0.196205 -0.057439 -1.046611 -1.100062 -1.049146 -0.632970   \n",
      "\n",
      "         Chi2      Chi3     Chi3c  ...       W3D      W3DH     WNSA1  \\\n",
      "0    3.130139  3.197914  2.236275  ...  2.710282  2.956792  1.502026   \n",
      "1    0.575460  0.687561  0.827467  ...  0.007296 -0.206569  0.253935   \n",
      "2    0.353654  0.731147 -0.189253  ... -0.051688 -0.164309  0.108677   \n",
      "3    0.384699  0.441641 -0.257737  ...  0.055954 -0.049355  0.574180   \n",
      "4    0.364393  0.329693  0.373668  ... -0.005315 -0.069495  0.678890   \n",
      "5   -0.150487  0.005776 -0.480497  ... -0.168659 -0.216624  0.318939   \n",
      "6    0.224983  0.118413  0.634057  ... -0.141670 -0.313122  0.594316   \n",
      "7   -0.258462 -0.187610 -0.372127  ... -0.227463 -0.281818 -0.091124   \n",
      "8    0.747477  0.775422 -0.125285  ...  0.257628  0.253530  0.503034   \n",
      "9   -0.187390 -0.124066 -0.282571  ... -0.211002 -0.216739 -0.368751   \n",
      "10   0.550663  0.690773 -0.081636  ...  0.049759 -0.108513  0.355457   \n",
      "11  -0.397091 -0.220185 -0.336756  ... -0.292583 -0.345037  0.060358   \n",
      "12   1.889116  1.906606  0.991527  ...  1.177950  0.556973  3.729152   \n",
      "13   0.220101  0.379932 -0.223871  ... -0.075185 -0.171450  0.301950   \n",
      "14   0.197257  0.486375 -0.278056  ... -0.102486 -0.217014  0.160004   \n",
      "15   0.571555  0.564830  0.251752  ...  0.303454  0.708376 -0.009559   \n",
      "16   0.629545  0.804327  0.304432  ...  0.036256 -0.152625  1.019368   \n",
      "17   0.676405 -0.009135  0.653624  ...  0.466232  1.124053  0.481284   \n",
      "18   0.343305  0.560931 -0.338261  ... -0.218917 -0.237269 -0.215909   \n",
      "19   2.240765  2.339260  2.121885  ...  1.408778  2.129737  1.289293   \n",
      "20   2.400286  2.370229  2.386037  ...  1.614435  1.861532  1.892837   \n",
      "21   0.372203  0.754546  0.299917  ... -0.044851  0.057715 -0.321137   \n",
      "22   0.845884  0.774046  1.391894  ...  0.219730  0.282204  0.382646   \n",
      "23   0.212291  0.229673 -0.418786  ...  0.091933  0.171858  0.342111   \n",
      "24  -0.491397 -0.273177 -0.743896  ... -0.294840 -0.340161 -0.337339   \n",
      "25   0.772469  0.177369  1.488975  ...  0.056731 -0.095856  0.786637   \n",
      "26   0.741424  1.217252  0.495585  ...  0.132172 -0.033720  0.518064   \n",
      "27   0.254661 -0.028405  0.762746  ... -0.120967 -0.092801 -0.212921   \n",
      "28  -0.180361  0.089967 -0.381158  ... -0.208741 -0.224825  0.029361   \n",
      "29  -0.569303 -0.734735 -0.463940  ... -0.266331 -0.234252 -0.312235   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "617  0.844713  0.726789  1.237617  ...  0.111153 -0.036895  0.345410   \n",
      "618  0.306598  0.026423  0.800375  ... -0.081278 -0.049431  0.203064   \n",
      "619  0.201748 -0.062127  0.641583  ... -0.122327 -0.106364  0.013876   \n",
      "620 -0.458595 -0.406919 -0.679175  ... -0.276892 -0.284418 -0.509634   \n",
      "621  0.201748  0.290465  0.354102  ... -0.152058 -0.209312  0.039049   \n",
      "622  0.810153  1.001613  0.244979  ...  0.150101  0.168163 -0.010815   \n",
      "623 -0.962541 -0.858383 -0.844740  ... -0.373479 -0.397060 -0.762322   \n",
      "624  0.447570  0.365938 -0.597898  ...  0.270542  0.801609  0.050546   \n",
      "625 -0.534743 -0.456241 -0.731102  ... -0.306175 -0.331643 -0.424494   \n",
      "626 -0.573599 -0.434677 -0.593382  ... -0.328487 -0.358362 -0.520662   \n",
      "627 -0.768460 -0.621410 -0.743143  ... -0.339220 -0.349201 -0.585210   \n",
      "628 -0.692507 -0.672338 -0.828183  ... -0.316938 -0.289807 -0.684635   \n",
      "629 -0.370146 -0.206880 -0.287087  ... -0.294625 -0.350704 -0.145722   \n",
      "630 -0.741906 -0.626228 -0.731102  ... -0.354053 -0.362069 -0.798771   \n",
      "631 -0.902598 -0.839572 -0.828183  ... -0.363322 -0.378315 -0.699554   \n",
      "632 -0.962541 -0.858383 -0.844740  ... -0.372261 -0.396451 -0.504666   \n",
      "633  0.453037  0.548772  0.168217  ...  0.058587 -0.070347  0.751278   \n",
      "634 -1.059386 -1.006577 -0.981707  ... -0.379999 -0.400397 -0.872346   \n",
      "635 -0.937939 -0.912293 -0.764968  ... -0.372409 -0.388080 -0.764061   \n",
      "636  0.113298  0.013576 -0.064326  ... -0.069756 -0.121204  0.227547   \n",
      "637  0.927695  1.112415  0.513646  ...  0.233720  0.174928  0.395936   \n",
      "638  1.117480  1.412014  0.223155  ...  0.380372  0.689624 -0.476608   \n",
      "639 -0.590585 -0.430318 -0.639289  ... -0.331391 -0.359237 -0.514285   \n",
      "640 -1.145492 -1.102009 -0.981707  ... -0.377218 -0.383579 -0.810764   \n",
      "641 -0.883073 -0.823744 -1.045676  ... -0.350137 -0.356446 -0.767125   \n",
      "642 -1.330591 -1.278878 -1.199200  ... -0.387448 -0.398841 -0.831301   \n",
      "643 -0.487688 -0.502580 -0.738628  ... -0.265109 -0.205764 -0.469404   \n",
      "644 -1.031660 -0.915734 -0.844740  ... -0.379606 -0.403658 -0.703708   \n",
      "645  0.302888  0.413884  0.503110  ... -0.106063 -0.187986  0.371619   \n",
      "646 -1.095312 -0.970332 -1.045676  ... -0.378539 -0.396499 -0.666500   \n",
      "\n",
      "        WNSA2     WNSA3     WPSA1     WPSA2     WPSA3      grav      rygr  \n",
      "0   -1.047605 -1.470877  3.591013  2.774299  2.890133 -0.000459  2.309015  \n",
      "1    0.039476 -0.089521  0.209876 -0.068863  1.263276 -0.016484  0.520089  \n",
      "2    0.110884  0.247904 -0.063116 -0.184205 -0.034477 -0.023385  0.448370  \n",
      "3    0.016079  0.069803  0.085371 -0.129042  0.049250 -0.020570  0.450391  \n",
      "4   -0.141807 -0.702665  0.047420 -0.016919 -0.025444 -0.020834  0.426148  \n",
      "5    0.123533  0.156043 -0.109939 -0.231656 -0.005482 -0.025812  0.688779  \n",
      "6    0.076184 -0.147775 -0.604995 -0.299903 -0.431788 -0.023805  0.462512  \n",
      "7    0.171566  0.241784 -0.228043 -0.251474 -0.234658 -0.026334  0.450391  \n",
      "8   -0.070440 -0.128997  0.576506  0.107414  0.402372 -0.018882  0.606959  \n",
      "9    0.168282  0.264874 -0.037530 -0.161566 -0.062377 -0.025725  0.070585  \n",
      "10   0.023946 -0.110858 -0.028108 -0.127372  0.124062 -0.021415  0.410996  \n",
      "11   0.176352  0.421137 -0.667994 -0.327770 -0.535359 -0.026757 -0.291039  \n",
      "12  -0.636299 -0.247091  0.609544  0.055455  0.403794 -0.003317  1.757488  \n",
      "13   0.061785  0.111170 -0.118424 -0.174615 -0.094600 -0.022498  0.143313  \n",
      "14   0.107373  0.216468 -0.240133 -0.220931 -0.124128 -0.023846  0.165536  \n",
      "15  -0.043812  0.025013  1.474804  0.616695  0.871593 -0.019660  1.442330  \n",
      "16  -0.014783 -0.069017 -0.150930 -0.202356 -0.077333 -0.019889  0.386753  \n",
      "17  -0.285143 -1.033413  1.725551  0.944403  1.177595 -0.019034  2.012039  \n",
      "18   0.153553  0.224758 -0.237918 -0.208700 -0.211394 -0.025588 -0.071843  \n",
      "19  -0.815423 -1.290689  2.074231  1.540559  1.438520 -0.006069  1.437280  \n",
      "20  -1.268749 -1.755861  2.154746  1.850507  2.000175 -0.005370  1.418087  \n",
      "21   0.133488  0.231351  0.488772  0.015735  0.302859 -0.023635  0.541302  \n",
      "22  -0.035241 -0.251653  0.809969  0.159475  0.561444 -0.021948  1.644355  \n",
      "23   0.018555 -0.120317  0.690059  0.053230  0.397485 -0.024246  1.717084  \n",
      "24   0.215786  0.170231 -0.493675 -0.308123 -0.039275 -0.023483 -0.823373  \n",
      "25  -0.361926 -1.318981 -0.091896  0.077601  0.332595 -0.016832  0.222103  \n",
      "26  -0.015360 -0.250012  0.155853 -0.073256  0.161527 -0.020463  0.811004  \n",
      "27   0.077183  0.002145  0.008919 -0.045883  0.040601 -0.022018 -0.181946  \n",
      "28   0.138872  0.305936 -0.209004 -0.228908 -0.335385 -0.024476 -0.188007  \n",
      "29   0.135887 -0.078726 -0.176480 -0.155083 -0.048842 -0.027678  0.324125  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "617 -0.288917 -0.962362  0.130089  0.258189  0.336830 -0.013079 -0.112247  \n",
      "618 -0.088926 -0.777584  0.027084  0.042080  0.000234 -0.021284  0.065534  \n",
      "619 -0.027855 -0.654398 -0.045680  0.004330 -0.113495 -0.021663 -0.134470  \n",
      "620  0.214975  0.360212 -0.409520 -0.274070 -0.442983 -0.027226 -0.574883  \n",
      "621  0.135373  0.376597 -0.268032 -0.236814 -0.277780 -0.022374 -0.193057  \n",
      "622  0.053408  0.020284  0.446783  0.046343  0.241908 -0.017816  0.256447  \n",
      "623  0.263154  0.490241 -0.855479 -0.359928 -0.711935 -0.030948 -1.413285  \n",
      "624 -0.030710  0.395014  1.533441  0.551299  0.888504 -0.019342  1.078686  \n",
      "625  0.231466  0.471741 -0.528940 -0.320689 -0.492325 -0.028965 -0.427405  \n",
      "626  0.240904  0.439192 -0.680316 -0.338605 -0.587188 -0.029152 -0.756705  \n",
      "627  0.246529  0.539259 -0.613487 -0.332258 -0.582568 -0.029677 -0.742564  \n",
      "628  0.239769  0.517560 -0.343101 -0.273483 -0.388459 -0.029538 -0.276897  \n",
      "629  0.175768  0.283680 -0.695232 -0.319941 -0.530590 -0.027250 -0.418314  \n",
      "630  0.262314  0.592283 -0.610948 -0.330650 -0.574453 -0.030230 -0.994084  \n",
      "631  0.253413  0.523263 -0.738763 -0.343099 -0.644793 -0.030613 -1.094086  \n",
      "632  0.258989  0.620715 -0.894317 -0.370177 -0.782601 -0.029834 -1.223382  \n",
      "633 -0.139725 -0.808575  0.126002 -0.008878  0.035478 -0.021170  0.732215  \n",
      "634  0.273439  0.581183 -0.850927 -0.362599 -0.742559 -0.031569 -1.524398  \n",
      "635  0.265587  0.587331 -0.783731 -0.355750 -0.684776 -0.031136 -1.258736  \n",
      "636  0.076426 -0.005450  0.135687 -0.125328  0.174796 -0.025277  1.140304  \n",
      "637  0.005800 -0.017023  0.648407  0.047808  0.448900 -0.019422  0.483725  \n",
      "638  0.116745  0.361937  1.251382  0.410903  0.744240 -0.016509  0.538271  \n",
      "639  0.236940  0.470155 -0.693984 -0.336991 -0.561510 -0.029083 -0.894082  \n",
      "640  0.263020  0.443865 -0.780378 -0.348339 -0.683147 -0.031559 -1.239544  \n",
      "641  0.262263  0.531469 -0.594001 -0.333134 -0.611296 -0.030341 -0.817313  \n",
      "642  0.274173  0.614900 -0.852297 -0.366235 -0.780231 -0.032332 -1.358738  \n",
      "643  0.193426  0.132174 -0.205614 -0.213021 -0.271620 -0.027194 -0.239523  \n",
      "644  0.271504  0.669038 -0.895565 -0.371997 -0.797528 -0.030976 -1.448639  \n",
      "645  0.056773  0.047658 -0.258812 -0.208290 -0.176017 -0.021113  0.069575  \n",
      "646  0.269844  0.677272 -0.841725 -0.368794 -0.762580 -0.031310 -1.234493  \n",
      "\n",
      "[647 rows x 801 columns]\n",
      "       NR.AhR  NR.AR  NR.AR.LBD  NR.Aromatase  NR.ER  NR.ER.LBD  \\\n",
      "0         0.0    0.0        0.0           0.0    0.0        0.0   \n",
      "1         0.0    0.0        0.0           0.0    0.0        0.0   \n",
      "2         0.0    0.0        0.0           0.0    0.0        0.0   \n",
      "3         0.0    0.0        0.0           0.0    0.0        0.0   \n",
      "4         0.0    0.0        0.0           0.0    0.0        0.0   \n",
      "5         0.0    0.0        0.0           0.0    0.0        0.0   \n",
      "6         0.0    0.0        0.0           0.0    0.0        0.0   \n",
      "7         0.0    0.0        0.0           0.0    0.0        0.0   \n",
      "8         0.0    0.0        0.0           0.0    0.0        0.0   \n",
      "9         0.0    0.0        0.0           0.0    0.0        0.0   \n",
      "10        0.0    0.0        0.0           0.0    0.0        0.0   \n",
      "11        0.0    0.0        0.0           0.0    0.0        0.0   \n",
      "12        0.0    0.0        0.0           0.0    0.0        0.0   \n",
      "13        0.0    0.0        0.0           0.0    0.0        0.0   \n",
      "14        0.0    0.0        0.0           0.0    0.0        0.0   \n",
      "15        0.0    0.0        0.0           0.0    0.0        0.0   \n",
      "16        0.0    0.0        0.0           0.0    0.0        0.0   \n",
      "17        0.0    0.0        0.0           0.0    0.0        0.0   \n",
      "18        0.0    0.0        0.0           0.0    0.0        0.0   \n",
      "19        0.0    0.0        0.0           0.0    0.0        0.0   \n",
      "20        0.0    0.0        0.0           0.0    0.0        0.0   \n",
      "21        0.0    0.0        0.0           0.0    0.0        0.0   \n",
      "22        0.0    0.0        0.0           0.0    0.0        0.0   \n",
      "23        0.0    0.0        0.0           0.0    0.0        0.0   \n",
      "24        0.0    0.0        0.0           0.0    0.0        0.0   \n",
      "25        0.0    0.0        0.0           0.0    0.0        0.0   \n",
      "26        0.0    0.0        0.0           0.0    0.0        0.0   \n",
      "27        0.0    0.0        0.0           0.0    0.0        0.0   \n",
      "28        0.0    0.0        0.0           0.0    0.0        0.0   \n",
      "29        0.0    0.0        0.0           0.0    0.0        0.0   \n",
      "...       ...    ...        ...           ...    ...        ...   \n",
      "12030     0.0    0.0        0.0           0.0    1.0        0.0   \n",
      "12031     1.0    0.0        1.0           1.0    1.0        0.0   \n",
      "12032     0.0    0.0        0.0           1.0    0.0        0.0   \n",
      "12033     0.0    0.0        0.0           0.0    0.0        0.0   \n",
      "12034     0.0    0.0        0.0           0.0    0.0        0.0   \n",
      "12035     0.0    0.0        0.0           0.0    0.0        0.0   \n",
      "12036     0.0    0.0        0.0           0.0    0.0        0.0   \n",
      "12037     1.0    0.0        0.0           1.0    0.0        0.0   \n",
      "12038     0.0    0.0        0.0           0.0    0.0        0.0   \n",
      "12039     0.0    0.0        0.0           0.0    0.0        0.0   \n",
      "12040     0.0    0.0        0.0           0.0    0.0        1.0   \n",
      "12041     0.0    0.0        0.0           0.0    0.0        0.0   \n",
      "12042     0.0    0.0        0.0           0.0    0.0        0.0   \n",
      "12043     0.0    0.0        0.0           0.0    0.0        0.0   \n",
      "12044     0.0    0.0        0.0           0.0    0.0        0.0   \n",
      "12045     1.0    0.0        0.0           0.0    0.0        0.0   \n",
      "12046     0.0    0.0        0.0           0.0    0.0        0.0   \n",
      "12047     0.0    0.0        0.0           0.0    0.0        0.0   \n",
      "12048     1.0    0.0        0.0           1.0    0.0        0.0   \n",
      "12049     0.0    0.0        0.0           0.0    0.0        0.0   \n",
      "12050     0.0    0.0        0.0           0.0    0.0        0.0   \n",
      "12051     0.0    0.0        0.0           0.0    0.0        0.0   \n",
      "12052     0.0    0.0        0.0           0.0    0.0        0.0   \n",
      "12053     0.0    0.0        0.0           0.0    0.0        0.0   \n",
      "12054     0.0    1.0        0.0           1.0    0.0        0.0   \n",
      "12055     0.0    0.0        0.0           0.0    0.0        0.0   \n",
      "12056     0.0    0.0        0.0           0.0    0.0        0.0   \n",
      "12057     0.0    0.0        0.0           0.0    0.0        0.0   \n",
      "12058     0.0    0.0        0.0           0.0    0.0        0.0   \n",
      "12059     0.0    0.0        0.0           0.0    0.0        0.0   \n",
      "\n",
      "       NR.PPAR.gamma  SR.ARE  SR.ATAD5  SR.HSE  SR.MMP  SR.p53  \n",
      "0                0.0     0.0       0.0     0.0     0.0     0.0  \n",
      "1                0.0     0.0       0.0     0.0     0.0     0.0  \n",
      "2                0.0     0.0       0.0     0.0     0.0     0.0  \n",
      "3                0.0     0.0       0.0     1.0     0.0     0.0  \n",
      "4                0.0     0.0       0.0     0.0     0.0     0.0  \n",
      "5                0.0     0.0       0.0     1.0     0.0     0.0  \n",
      "6                0.0     0.0       0.0     1.0     0.0     0.0  \n",
      "7                0.0     0.0       0.0     1.0     0.0     0.0  \n",
      "8                0.0     0.0       0.0     0.0     0.0     0.0  \n",
      "9                0.0     0.0       0.0     0.0     0.0     0.0  \n",
      "10               0.0     0.0       0.0     0.0     0.0     0.0  \n",
      "11               0.0     0.0       0.0     0.0     0.0     0.0  \n",
      "12               0.0     0.0       0.0     0.0     0.0     0.0  \n",
      "13               0.0     0.0       0.0     0.0     0.0     0.0  \n",
      "14               0.0     1.0       0.0     0.0     0.0     0.0  \n",
      "15               0.0     0.0       0.0     0.0     0.0     0.0  \n",
      "16               0.0     1.0       0.0     0.0     0.0     0.0  \n",
      "17               0.0     0.0       0.0     0.0     0.0     0.0  \n",
      "18               0.0     1.0       0.0     0.0     0.0     0.0  \n",
      "19               0.0     0.0       0.0     1.0     0.0     0.0  \n",
      "20               0.0     1.0       0.0     0.0     0.0     0.0  \n",
      "21               0.0     0.0       0.0     1.0     0.0     0.0  \n",
      "22               0.0     0.0       0.0     0.0     0.0     0.0  \n",
      "23               0.0     0.0       0.0     0.0     0.0     0.0  \n",
      "24               0.0     0.0       0.0     1.0     0.0     0.0  \n",
      "25               0.0     0.0       0.0     1.0     0.0     0.0  \n",
      "26               0.0     0.0       0.0     1.0     0.0     0.0  \n",
      "27               0.0     0.0       0.0     0.0     0.0     0.0  \n",
      "28               0.0     0.0       0.0     0.0     0.0     0.0  \n",
      "29               0.0     0.0       0.0     0.0     0.0     0.0  \n",
      "...              ...     ...       ...     ...     ...     ...  \n",
      "12030            0.0     0.0       0.0     0.0     0.0     0.0  \n",
      "12031            1.0     1.0       0.0     0.0     0.0     1.0  \n",
      "12032            0.0     0.0       0.0     0.0     0.0     0.0  \n",
      "12033            0.0     1.0       0.0     0.0     0.0     0.0  \n",
      "12034            0.0     0.0       0.0     0.0     0.0     0.0  \n",
      "12035            0.0     0.0       0.0     0.0     0.0     0.0  \n",
      "12036            0.0     0.0       0.0     0.0     0.0     0.0  \n",
      "12037            0.0     1.0       0.0     0.0     0.0     0.0  \n",
      "12038            0.0     0.0       0.0     0.0     0.0     0.0  \n",
      "12039            0.0     0.0       0.0     0.0     0.0     0.0  \n",
      "12040            0.0     0.0       1.0     0.0     0.0     1.0  \n",
      "12041            0.0     0.0       0.0     0.0     0.0     0.0  \n",
      "12042            0.0     0.0       0.0     0.0     1.0     0.0  \n",
      "12043            0.0     0.0       0.0     0.0     0.0     0.0  \n",
      "12044            1.0     1.0       0.0     0.0     0.0     0.0  \n",
      "12045            0.0     1.0       0.0     0.0     1.0     0.0  \n",
      "12046            0.0     0.0       0.0     0.0     0.0     0.0  \n",
      "12047            0.0     0.0       0.0     0.0     0.0     0.0  \n",
      "12048            0.0     1.0       0.0     0.0     0.0     1.0  \n",
      "12049            0.0     0.0       0.0     0.0     0.0     0.0  \n",
      "12050            0.0     1.0       0.0     0.0     1.0     0.0  \n",
      "12051            0.0     0.0       0.0     0.0     0.0     0.0  \n",
      "12052            0.0     0.0       0.0     0.0     0.0     0.0  \n",
      "12053            0.0     0.0       0.0     0.0     0.0     0.0  \n",
      "12054            0.0     0.0       0.0     0.0     0.0     1.0  \n",
      "12055            0.0     0.0       0.0     0.0     0.0     0.0  \n",
      "12056            0.0     0.0       0.0     0.0     0.0     0.0  \n",
      "12057            0.0     0.0       0.0     0.0     0.0     0.0  \n",
      "12058            0.0     0.0       0.0     0.0     0.0     0.0  \n",
      "12059            0.0     0.0       0.0     0.0     0.0     0.0  \n",
      "\n",
      "[12060 rows x 12 columns]\n",
      "     NR.AhR  NR.AR  NR.AR.LBD  NR.Aromatase  NR.ER  NR.ER.LBD  NR.PPAR.gamma  \\\n",
      "0       0.0    1.0        0.0           0.0    0.0        0.0            0.0   \n",
      "1       0.0    1.0        0.0           0.0    0.0        0.0            0.0   \n",
      "2       1.0    1.0        0.0           0.0    1.0        0.0            0.0   \n",
      "3       0.0    0.0        0.0           1.0    0.0        0.0            0.0   \n",
      "4       0.0    0.0        0.0           0.0    0.0        0.0            0.0   \n",
      "5       1.0    0.0        0.0           0.0    1.0        1.0            1.0   \n",
      "6       0.0    0.0        0.0           0.0    1.0        0.0            0.0   \n",
      "7       0.0    0.0        0.0           0.0    1.0        0.0            0.0   \n",
      "8       0.0    0.0        0.0           0.0    0.0        0.0            0.0   \n",
      "9       0.0    0.0        0.0           0.0    0.0        0.0            0.0   \n",
      "10      0.0    0.0        0.0           1.0    0.0        0.0            0.0   \n",
      "11      1.0    0.0        0.0           1.0    0.0        0.0            0.0   \n",
      "12      0.0    0.0        0.0           0.0    0.0        0.0            0.0   \n",
      "13      0.0    0.0        0.0           0.0    0.0        0.0            0.0   \n",
      "14      1.0    0.0        0.0           0.0    1.0        0.0            0.0   \n",
      "15      0.0    0.0        0.0           0.0    0.0        0.0            0.0   \n",
      "16      0.0    0.0        0.0           1.0    0.0        0.0            0.0   \n",
      "17      0.0    0.0        0.0           0.0    0.0        0.0            0.0   \n",
      "18      1.0    0.0        0.0           0.0    0.0        0.0            0.0   \n",
      "19      1.0    0.0        1.0           0.0    0.0        0.0            0.0   \n",
      "20      0.0    0.0        0.0           0.0    0.0        0.0            0.0   \n",
      "21      0.0    0.0        0.0           0.0    1.0        1.0            0.0   \n",
      "22      0.0    0.0        0.0           0.0    0.0        0.0            0.0   \n",
      "23      0.0    0.0        0.0           0.0    0.0        0.0            1.0   \n",
      "24      0.0    0.0        1.0           0.0    0.0        1.0            1.0   \n",
      "25      1.0    0.0        1.0           0.0    1.0        1.0            1.0   \n",
      "26      0.0    0.0        0.0           1.0    0.0        0.0            0.0   \n",
      "27      0.0    0.0        0.0           0.0    0.0        0.0            0.0   \n",
      "28      0.0    0.0        0.0           0.0    0.0        0.0            0.0   \n",
      "29      0.0    0.0        0.0           0.0    0.0        0.0            0.0   \n",
      "..      ...    ...        ...           ...    ...        ...            ...   \n",
      "617     0.0    0.0        0.0           0.0    0.0        0.0            0.0   \n",
      "618     0.0    0.0        0.0           0.0    0.0        0.0            0.0   \n",
      "619     0.0    0.0        0.0           0.0    0.0        0.0            0.0   \n",
      "620     0.0    0.0        0.0           1.0    0.0        0.0            0.0   \n",
      "621     0.0    0.0        0.0           1.0    0.0        0.0            0.0   \n",
      "622     0.0    0.0        0.0           0.0    0.0        0.0            0.0   \n",
      "623     0.0    0.0        0.0           0.0    0.0        0.0            0.0   \n",
      "624     0.0    0.0        0.0           0.0    0.0        0.0            1.0   \n",
      "625     1.0    0.0        0.0           0.0    1.0        1.0            0.0   \n",
      "626     0.0    0.0        0.0           0.0    0.0        0.0            0.0   \n",
      "627     0.0    0.0        0.0           0.0    0.0        0.0            0.0   \n",
      "628     0.0    0.0        0.0           0.0    1.0        0.0            0.0   \n",
      "629     1.0    0.0        1.0           0.0    0.0        1.0            1.0   \n",
      "630     0.0    0.0        0.0           0.0    0.0        0.0            0.0   \n",
      "631     0.0    0.0        0.0           0.0    0.0        0.0            0.0   \n",
      "632     0.0    0.0        0.0           0.0    0.0        0.0            0.0   \n",
      "633     0.0    0.0        0.0           0.0    0.0        0.0            0.0   \n",
      "634     0.0    0.0        0.0           0.0    0.0        0.0            0.0   \n",
      "635     0.0    0.0        0.0           0.0    0.0        0.0            0.0   \n",
      "636     1.0    1.0        0.0           0.0    1.0        0.0            0.0   \n",
      "637     0.0    0.0        0.0           0.0    0.0        0.0            0.0   \n",
      "638     0.0    0.0        0.0           0.0    0.0        0.0            0.0   \n",
      "639     1.0    0.0        0.0           0.0    0.0        0.0            1.0   \n",
      "640     0.0    0.0        0.0           0.0    0.0        0.0            0.0   \n",
      "641     0.0    0.0        0.0           0.0    0.0        0.0            0.0   \n",
      "642     0.0    0.0        0.0           0.0    0.0        0.0            0.0   \n",
      "643     0.0    0.0        0.0           0.0    0.0        0.0            0.0   \n",
      "644     0.0    0.0        0.0           0.0    0.0        0.0            0.0   \n",
      "645     0.0    0.0        0.0           0.0    0.0        0.0            0.0   \n",
      "646     0.0    0.0        0.0           0.0    0.0        0.0            0.0   \n",
      "\n",
      "     SR.ARE  SR.ATAD5  SR.HSE  SR.MMP  SR.p53  \n",
      "0       0.0       0.0     0.0     0.0     0.0  \n",
      "1       0.0       0.0     0.0     0.0     0.0  \n",
      "2       1.0       1.0     0.0     1.0     0.0  \n",
      "3       1.0       0.0     0.0     0.0     0.0  \n",
      "4       0.0       0.0     0.0     0.0     1.0  \n",
      "5       0.0       1.0     1.0     0.0     1.0  \n",
      "6       1.0       0.0     0.0     1.0     1.0  \n",
      "7       0.0       0.0     0.0     1.0     1.0  \n",
      "8       0.0       1.0     0.0     0.0     0.0  \n",
      "9       0.0       0.0     0.0     0.0     1.0  \n",
      "10      0.0       0.0     0.0     1.0     0.0  \n",
      "11      0.0       1.0     0.0     0.0     0.0  \n",
      "12      1.0       0.0     0.0     1.0     0.0  \n",
      "13      0.0       0.0     0.0     0.0     0.0  \n",
      "14      1.0       1.0     0.0     0.0     0.0  \n",
      "15      0.0       0.0     0.0     0.0     0.0  \n",
      "16      0.0       0.0     0.0     1.0     0.0  \n",
      "17      0.0       0.0     0.0     0.0     0.0  \n",
      "18      0.0       0.0     0.0     0.0     0.0  \n",
      "19      1.0       1.0     0.0     1.0     0.0  \n",
      "20      0.0       1.0     0.0     0.0     0.0  \n",
      "21      0.0       0.0     0.0     0.0     1.0  \n",
      "22      0.0       0.0     0.0     0.0     1.0  \n",
      "23      0.0       0.0     0.0     0.0     0.0  \n",
      "24      0.0       0.0     0.0     1.0     1.0  \n",
      "25      0.0       0.0     1.0     0.0     1.0  \n",
      "26      0.0       0.0     0.0     0.0     1.0  \n",
      "27      0.0       0.0     0.0     0.0     0.0  \n",
      "28      0.0       0.0     0.0     0.0     0.0  \n",
      "29      0.0       0.0     0.0     0.0     0.0  \n",
      "..      ...       ...     ...     ...     ...  \n",
      "617     1.0       0.0     0.0     0.0     0.0  \n",
      "618     0.0       0.0     0.0     0.0     0.0  \n",
      "619     0.0       0.0     0.0     0.0     0.0  \n",
      "620     0.0       0.0     0.0     0.0     0.0  \n",
      "621     0.0       0.0     0.0     1.0     0.0  \n",
      "622     0.0       0.0     0.0     0.0     0.0  \n",
      "623     0.0       0.0     0.0     0.0     0.0  \n",
      "624     0.0       0.0     0.0     0.0     0.0  \n",
      "625     1.0       0.0     0.0     0.0     0.0  \n",
      "626     1.0       0.0     0.0     1.0     0.0  \n",
      "627     1.0       0.0     0.0     0.0     0.0  \n",
      "628     0.0       0.0     0.0     0.0     0.0  \n",
      "629     1.0       0.0     1.0     1.0     1.0  \n",
      "630     0.0       0.0     0.0     0.0     0.0  \n",
      "631     0.0       0.0     0.0     0.0     0.0  \n",
      "632     0.0       0.0     0.0     0.0     0.0  \n",
      "633     0.0       0.0     0.0     0.0     1.0  \n",
      "634     0.0       0.0     0.0     0.0     0.0  \n",
      "635     0.0       0.0     0.0     0.0     0.0  \n",
      "636     1.0       1.0     0.0     1.0     0.0  \n",
      "637     0.0       0.0     0.0     0.0     0.0  \n",
      "638     0.0       0.0     0.0     0.0     0.0  \n",
      "639     1.0       0.0     1.0     1.0     0.0  \n",
      "640     0.0       0.0     0.0     0.0     0.0  \n",
      "641     0.0       0.0     0.0     0.0     0.0  \n",
      "642     1.0       0.0     0.0     0.0     0.0  \n",
      "643     0.0       0.0     0.0     0.0     0.0  \n",
      "644     0.0       0.0     0.0     0.0     0.0  \n",
      "645     1.0       0.0     0.0     0.0     1.0  \n",
      "646     1.0       0.0     0.0     0.0     1.0  \n",
      "\n",
      "[647 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "# Test print\n",
    "print(X_train)\n",
    "print(X_test)\n",
    "\n",
    "print(Y_train)\n",
    "print(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classification of the records**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# creazione della DNN cin keras\n",
    "    # esplorazione preliminare dei parametri (con tutte le features)\n",
    "# confronto con la medesima DNN delle 3 feature extraction\n",
    "# AutoML (maybe AutoKeras) su DNN o RandomForest\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "AML Project.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
