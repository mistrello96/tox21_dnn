{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X71Bwbwowszz"
   },
   "source": [
    "# Toxicity - Matteo Mistri and Daniele Papetti\n",
    "**Tox21 dataset analysis and prediction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please, do note that this notebook is thought to run on colab and using google drive as storage system, so you may need to change the directories of input and output in order to make the notebook run locally. If you have possible suggestions to fix this problem, you can write us. Any suggestion will be appreciated :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "15GuXYIXwsz5"
   },
   "source": [
    "**Dataset input and preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pddFR07XwytH"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nty7chPCwyYk"
   },
   "outputs": [],
   "source": [
    "cd drive/'My Drive'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e7xdczejJ_6k"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "X_train = pd.read_csv(\"./AML_project/tox21_dense_train.csv\")\n",
    "X_test = pd.read_csv(\"./AML_project/tox21_dense_test.csv\")\n",
    "Y_train = pd.read_csv(\"./AML_project/tox21_labels_train.csv\")\n",
    "Y_test = pd.read_csv(\"./AML_project/tox21_labels_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EBGbIq_Kws0I"
   },
   "source": [
    "We decide to drop the names of the samples because we are sure that it is an irrelevant feature for the further operations.\n",
    "We substitute the NaN values in the labels with 0 value since we assume that if the test was not performed, the doctors would have thought that the molecule would have not been involved in that biological pathway. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s6V_MPfGNFqp"
   },
   "outputs": [],
   "source": [
    "# drop first column that contains names\n",
    "X_train = X_train.drop(X_train.columns[[0]], axis = 1)\n",
    "X_test = X_test.drop(X_test.columns[[0]], axis = 1)\n",
    "\n",
    "Y_train = Y_train.drop(Y_train.columns[[0]], axis = 1)\n",
    "Y_test = Y_test.drop(Y_test.columns[[0]], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ce0pqt0fNMP2"
   },
   "outputs": [],
   "source": [
    "# transform NaN in 0 in the labels\n",
    "Y_train = Y_train.fillna(0)\n",
    "Y_test = Y_test.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MNUixAt7ws0a"
   },
   "outputs": [],
   "source": [
    "# distribuzione etichette\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "for l in Y_train.columns:\n",
    "  unique_values_count = Counter(Y_train[l])\n",
    "  print(\"{}: {}\".format(l, unique_values_count))\n",
    "\n",
    "  plt.figure(figsize = (8, 6))\n",
    "  sns.countplot(x = l, data = Y_train)\n",
    "  plt.xlabel(\"Values\", fontsize = 16)\n",
    "  plt.ylabel(\"Number of instances\", fontsize = 16)\n",
    "  plt.title(' '.join(l.split('.')), fontsize = 18)\n",
    "  plt.xticks(fontsize = 12)\n",
    "  plt.yticks(fontsize = 12)\n",
    "  plt.savefig(\"./AML_project/images/png/hist-{}.png\".format(''.join(l.split('.'))))\n",
    "  plt.savefig(\"./AML_project/images/pdf/hist-{}.pdf\".format(''.join(l.split('.'))))\n",
    "  plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V1_QcGE8ws0i"
   },
   "outputs": [],
   "source": [
    "# ricerca e gestione outliers (3 * Q3/4-q1/4)\n",
    "from collections import Counter\n",
    "\n",
    "# remove outliers\n",
    "outliers = list()\n",
    "# if we consider feature by feature, we drop all the dataset\n",
    "# we count how many time a record is considered outlier\n",
    "for column in X_train.columns:\n",
    "  mean = X_train[column].mean()\n",
    "  q1 = X_train[column].quantile(1 / 4)\n",
    "  q3 = X_train[column].quantile(3 / 4)\n",
    "  threshold = 3 * (q3 - q1)\n",
    "\n",
    "  # Indexes of outliers\n",
    "  outliers.extend(X_train[X_train[column] > mean + threshold].index.values.tolist())      \n",
    "  outliers.extend(X_train[X_train[column] < mean - threshold].index.values.tolist())\n",
    "\n",
    "# drop a record if it is considered to be an outlier in more that 1/4 of the features\n",
    "out_counter = Counter(outliers)\n",
    "toDrop = [k for k, v in zip(out_counter.keys(), out_counter.values()) if v > 200]\n",
    "\n",
    "# Delete these row indexes from dataFrame\n",
    "X_train.drop(toDrop, inplace = True)\n",
    "Y_train.drop(toDrop, inplace = True)\n",
    "X_train.reset_index(drop = True, inplace = True)\n",
    "Y_train.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fxjwvYhRws0q"
   },
   "outputs": [],
   "source": [
    "# correlation between labels\n",
    "# Create correlation matrix\n",
    "corr_matrix = Y_train.append(Y_test).corr().abs()\n",
    "\n",
    "plt.figure(figsize = (8, 6))\n",
    "'''\n",
    "issue with the labels cut off, know issue with matplotlib\n",
    "sns.heatmap(corr_matrix, fmt = '.2f', annot = True, vmin=0, vmax=1,\n",
    "            xticklabels = ['\\n'.join(x.split('.')) for x in corr_matrix],\n",
    "            yticklabels = ['\\n'.join(x.split('.')) for x in corr_matrix])\n",
    "'''\n",
    "sns.heatmap(corr_matrix, fmt = '.2f', annot = True, vmin = 0, vmax = 1,\n",
    "            xticklabels = range(12), yticklabels = range(12))\n",
    "plt.title(\"Labels correlation matrix heatmap\", fontsize = 18)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"./AML_project/images/png/labels_corr_matrix_heatmap.png\")\n",
    "plt.savefig(\"./AML_project/images/pdf/labels_corr_matrix_heatmap.pdf\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n0i6zTSNws0u"
   },
   "outputs": [],
   "source": [
    "# correlation between features and labels\n",
    "corr_matrix = X_train.append(X_test).merge(Y_train.append(Y_test),\n",
    "                                           right_index = True, left_index = True).corr().abs()\n",
    "# Print most correlated feature for each class\n",
    "s = set()\n",
    "for label in Y_train.columns:\n",
    "  # extract correlation column of the considered class\n",
    "  corr_col = corr_matrix[label]\n",
    "  # remove the elements whose indes is a label\n",
    "  cleaned_col = corr_col.drop(Y_train.columns, axis = 0)\n",
    "  # sort the result\n",
    "  sorted_col = cleaned_col.sort_values(ascending = False)\n",
    "  # extract the most correlated features for the considered class\n",
    "  s.add(sorted_col.index[0])\n",
    "  print(\"Class {} is mosty correlated with feature {} with a value of {}\".format(label, sorted_col.index[0], round(sorted_col[0], 3)))\n",
    "print(\"unique features: {}\".format(len(s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG THIS, why -1 values\n",
    "high_corr_features_count = {k: (sum(1 for x in corr_matrix[k] if x > 0.90) - 1) for k in corr_matrix.columns} # -1 because evry feature is correlated with itself with value 1\n",
    "print(high_corr_features_count)\n",
    "for k in high_corr_features_count.keys():\n",
    "  if high_corr_features_count[k] == -1:\n",
    "    print(k)\n",
    "c = Counter(high_corr_features_count.values())\n",
    "print(c)\n",
    "\n",
    "plt.Figure(figsize = (8, 6))\n",
    "plt.yscale('log')\n",
    "plt.ylim(bottom = min(c.values()) - 0.2, top = max(c.values()) + 50)\n",
    "for p, v in c.items():\n",
    "  plt.scatter(p, v)\n",
    "plt.xlabel(\"Number of highly correlated features/labels\", fontsize = 16)\n",
    "plt.ylabel(\"Number of features/labels\", fontsize = 16)\n",
    "plt.xticks(fontsize = 14)\n",
    "plt.yticks(fontsize = 14)\n",
    "plt.title(\"Distribution of correlated features/labels\", fontsize = 18)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"./AML_project/images/png/distribution_high_corr.png\")\n",
    "plt.savefig(\"./AML_project/images/pdf/distribution_high_corr.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BPetLISCws02"
   },
   "outputs": [],
   "source": [
    "# search for missing values in features\n",
    "print(X_train.isnull().any().any())\n",
    "print(X_test.isnull().any().any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zIb5irM2SGA_"
   },
   "outputs": [],
   "source": [
    "# check if all features are numeric\n",
    "set(X_train.dtypes.append(X_test.dtypes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "905LrB1NNboE"
   },
   "outputs": [],
   "source": [
    "# normalize features in 0 mean and 1 std\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train.append(X_test).values)\n",
    "X_train = pd.DataFrame(scaler.transform(X_train.values), columns = X_train.columns)\n",
    "X_test = pd.DataFrame(scaler.transform(X_test.values), columns = X_test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E5nXCDuISkyF"
   },
   "source": [
    "**Use only one of the following method to reduce number of features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XdLtq27eMeun",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# correlation analysis and drop correlated features\n",
    "# create correlation matrix\n",
    "import numpy as np\n",
    "corr_matrix = X_train.corr().abs()\n",
    "\n",
    "# select upper triangle of correlation matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k = 1).astype(np.bool))\n",
    "\n",
    "# find index of feature columns with correlation greater than 0.90\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.90)]\n",
    "\n",
    "# drop the features\n",
    "X_train.drop(X_train[to_drop], axis = 1)\n",
    "X_test.drop(X_test[to_drop], axis = 1)\n",
    "X_train.reset_index(drop = True, inplace = True)\n",
    "X_test.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nsX5l7J3P-12"
   },
   "outputs": [],
   "source": [
    "# PCA features reduction\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "features_limit = 100\n",
    "columns = ['col' + str(x) for x in range(features_limit)]\n",
    "PCA_transformer = PCA(n_components = features_limit)\n",
    "PCA_transformer.fit(X_train.append(X_test).values)\n",
    "X_train = pd.DataFrame(PCA_transformer.transform(X_train.values), columns = columns)\n",
    "X_test = pd.DataFrame(PCA_transformer.transform(X_test.values), columns = columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vfe4cnNtTABp",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "import keras.optimizers\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "# AUTOENCODER\n",
    "# Define early stopping\n",
    "es = EarlyStopping(monitor = 'val_loss', mode = 'min', \n",
    "                   verbose = 1, patience = 15, min_delta = 0.001,\n",
    "                   restore_best_weights = True)\n",
    "\n",
    "encoding_dim1 = int(X_train.shape[1] / 2)\n",
    "encoding_dim2 = int(encoding_dim1 / 2)\n",
    "encoding_dim3 = 100\n",
    "columns = ['col' + str(x) for x in range(encoding_dim3)]\n",
    "\n",
    "input_layer = Input(shape = (X_train.shape[1],))\n",
    "# \"encoded\" is the encoded representation of the input\n",
    "encoded1 = Dense(encoding_dim1, activation = 'relu')(input_layer)\n",
    "encoded2 = Dense(encoding_dim2, activation = 'relu')(encoded1)\n",
    "bottleneck = Dense(encoding_dim3, activation = 'relu')(encoded2)\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "decoded2 = Dense(encoding_dim2, activation = 'relu')(bottleneck)\n",
    "decoded1 = Dense(encoding_dim1, activation = 'relu')(decoded2)\n",
    "output_layer = Dense(X_train.shape[1], activation = 'linear')(decoded1)\n",
    "# this model maps an input to its reconstruction\n",
    "autoencoder = Model(input_layer, output_layer)\n",
    "# this model maps an input to its encoded representation\n",
    "encoder = Model(input_layer, bottleneck)\n",
    "\n",
    "# compile the model\n",
    "autoencoder.compile(optimizer = 'adam', loss = 'mse')\n",
    "autoencoder.summary()\n",
    "# fit the autoencoder\n",
    "autoencoder.fit(X_train.append(X_test).values, X_train.append(X_test).values, \n",
    "                validation_split = 0.1, epochs = 300, batch_size = 256, \n",
    "                verbose = True, callbacks = [es], use_multiprocessing = True)\n",
    "# extract representation\n",
    "X_train = pd.DataFrame(encoder.predict(X_train.values), columns = columns)\n",
    "X_test = pd.DataFrame(encoder.predict(X_test.values), columns = columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G4_TLzUNws1m"
   },
   "source": [
    "**Classification of the records**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SupYFKeGws1o"
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "# confronto con la medesima DNN delle 3 feature extraction\n",
    "# AutoML (maybe AutoKeras) su DNN o RandomForest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yts-GtRjws15"
   },
   "outputs": [],
   "source": [
    "# evaluate auc for a given model\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_performance(model, test_features, test_label):\n",
    "  test_predictions = model.predict(test_features)\n",
    "  test_pred_df = pd.DataFrame(data = test_predictions, columns = test_label.columns)\n",
    "  auc = dict()\n",
    "  for c_pred, c_true in zip(test_pred_df, test_label):\n",
    "    auc[c_true] = roc_auc_score(y_true = test_label[c_true], y_score = test_pred_df[c_pred])\n",
    "\n",
    "  for k in auc:\n",
    "    print(\"{}: {}\".format(k, auc[k]))\n",
    "\n",
    "  print(\"\\nmean: {}\".format(np.mean(list(auc.values()))))\n",
    "  return((auc, np.mean(list(auc.values()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UUqjc8BHws10"
   },
   "outputs": [],
   "source": [
    "# confusion matrix as a dictionary\n",
    "# Predict and evaluate performances\n",
    "def get_confusion_matrix(model, test_features, test_label):\n",
    "  predictions = model.predict(test_features)\n",
    "  predictions = [[round(x,1) for x in l] for l in predictions]\n",
    "  #print(predictions[:10])\n",
    "  #print(test_label.head)\n",
    "  d = {k: {'t1': 0, 't0': 0, 'f1': 0, 'f0': 0} for k in range (1, 13)}\n",
    "  for preds, trues in zip(predictions, test_label.itertuples()):\n",
    "    for p, t, k in zip(preds, trues[1:], range(1, 13)):\n",
    "      p = 1 if p > 0.5 else 0\n",
    "      if p == t and p == 0:\n",
    "          d[k]['t0'] = d[k]['t0'] + 1\n",
    "      if p == t and p == 1:\n",
    "          d[k]['t1'] = d[k]['t1'] + 1\n",
    "      if p != t and p == 0:\n",
    "          d[k]['f0'] = d[k]['f0'] + 1\n",
    "      if p != t and p == 0:\n",
    "          d[k]['f1'] = d[k]['f1'] + 1\n",
    "  for k in d:\n",
    "      print(d[k])\n",
    "  return d\n",
    "#tmp = [get_class(x) for x in y_val]\n",
    "#print(classification_report(tmp, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iJTcK3bqws1u"
   },
   "outputs": [],
   "source": [
    "### CLASSIC NN\n",
    "from keras.models import Sequential\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from keras.callbacks import EarlyStopping, Callback\n",
    "from keras.layers import Dense, BatchNormalization, Dropout, Activation\n",
    "from keras import regularizers\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras.backend.tensorflow_backend as tfb\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "POS_WEIGHT = 20\n",
    "\n",
    "# def custom loss(weighted_binary_crossentropy not defined in keras)\n",
    "def weighted_binary_crossentropy(target, output):\n",
    "  # transform back to logits\n",
    "  _epsilon = tfb._to_tensor(tfb.epsilon(), output.dtype.base_dtype)\n",
    "  output = tf.clip_by_value(output, _epsilon, 1 - _epsilon)\n",
    "  output = tf.log(output / (1 - output))\n",
    "  # compute weighted loss\n",
    "  loss = tf.nn.weighted_cross_entropy_with_logits(targets = target,\n",
    "                                                  logits = output,\n",
    "                                                  pos_weight = POS_WEIGHT)\n",
    "  return tf.reduce_mean(loss, axis = -1)\n",
    "\n",
    "# Define early stopping\n",
    "es = EarlyStopping(monitor = 'val_loss', mode = 'min', verbose = 1, patience = 10, min_delta = 0.001, restore_best_weights = True)\n",
    "\n",
    "class roc_callback(Callback):\n",
    "  def __init__(self,training_data,validation_data):\n",
    "    self.x = training_data[0]\n",
    "    self.y = training_data[1]\n",
    "    self.x_val = validation_data[0]\n",
    "    self.y_val = validation_data[1]\n",
    "\n",
    "  def on_train_begin(self, logs = dict()):\n",
    "    return\n",
    "\n",
    "  def on_train_end(self, logs = dict()):\n",
    "    return\n",
    "\n",
    "  def on_epoch_begin(self, epoch, logs = dict()):\n",
    "    return\n",
    "\n",
    "  def on_epoch_end(self, epoch, logs = dict()):\n",
    "    y_pred = pd.DataFrame(data = self.model.predict(self.x), columns = self.y.columns)\n",
    "    y_pred_val = pd.DataFrame(data = self.model.predict(self.x_val), columns = self.y.columns)\n",
    "    average_roc = 0.0\n",
    "    average_roc_val = 0.0\n",
    "    for i in self.y.columns:\n",
    "      average_roc += roc_auc_score(self.y[i], y_pred[i])\n",
    "      average_roc_val += roc_auc_score(self.y_val[i], y_pred_val[i])\n",
    "\n",
    "    average_roc = average_roc / 12\n",
    "    average_roc_val = average_roc_val / 12\n",
    "    print('Average-roc-auc: {}   Average-roc-auc_val: {}'.format(round(average_roc, 4), \n",
    "                                                                  round(average_roc_val,4)))\n",
    "    return\n",
    "\n",
    "    def on_batch_begin(self, batch, logs = dict()):\n",
    "      return\n",
    "\n",
    "    def on_batch_end(self, batch, logs = dict()):\n",
    "      return\n",
    "\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "average_roc = 0.0\n",
    "for t, v in kfold.split(X_train, Y_train):\n",
    "  # NN\n",
    "  NN = Sequential()\n",
    "  NN.add(Dense(512, input_shape=(X_train.shape[1],)))\n",
    "  NN.add(BatchNormalization())\n",
    "  NN.add(Activation('relu'))\n",
    "  NN.add(Dropout(rate=0.5))\n",
    "  NN.add(Dense(256))\n",
    "  NN.add(BatchNormalization())\n",
    "  NN.add(Activation('relu'))\n",
    "  NN.add(Dropout(rate=0.5))\n",
    "  NN.add(Dense(128))\n",
    "  NN.add(BatchNormalization())\n",
    "  NN.add(Activation('relu'))\n",
    "  NN.add(Dropout(rate=0.5))\n",
    "  NN.add(Dense(12, activation = 'sigmoid'))\n",
    "\n",
    "  # Compile model\n",
    "  NN.compile(optimizer = 'adam', loss = weighted_binary_crossentropy)\n",
    "  # Fit tne network\n",
    "  learning_process_NN = NN.fit(X_train.iloc[t], Y_train.iloc[t], validation_data = (X_train.iloc[v], Y_train.iloc[v]), \n",
    "                              callbacks = [roc_callback(training_data = (X_train.iloc[t], Y_train.iloc[t]), \n",
    "                                                        validation_data = (X_train.iloc[v], Y_train.iloc[v])), es], \n",
    "                              epochs = 300, batch_size = 128, verbose = True, use_multiprocessing = True)\n",
    "  average_roc += evaluate_performance(NN, X_test, Y_test)[1]\n",
    "\n",
    "print(\"\\n\\n\\n Mean penformance on 10 fold cross validation of the baseline model is {}\".format(average_roc/10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fQRzL_VLD6r_"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_history(network_history):\n",
    "    plt.figure()\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.plot(network_history.history['loss'])\n",
    "    plt.plot(network_history.history['val_loss'])\n",
    "    plt.legend(['Training', 'Validation'])\n",
    "\n",
    "plot_history(learning_process_NN)\n",
    "plt.savefig(\"learning_process_baseline.pdf\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "AML_Project.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
